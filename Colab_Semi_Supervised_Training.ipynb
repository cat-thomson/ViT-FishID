{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65590a95",
   "metadata": {},
   "source": [
    "# ðŸŸ ViT Fish Classification - Semi-Supervised Learning with EMA Teacher-Student\n",
    "\n",
    "This notebook trains a Vision Transformer (ViT) using **semi-supervised learning** with an **EMA Teacher-Student framework** for fish species classification.\n",
    "\n",
    "**Key Features:**\n",
    "- âœ… **Fixed Consistency Loss** - Now properly uses unlabeled data\n",
    "- âœ… **Optimized Pseudo-label Threshold** (0.7 instead of 0.95)\n",
    "- âœ… **Temperature Scaling** for better probability calibration\n",
    "- âœ… **Exponential Moving Average (EMA) Teacher**\n",
    "- âœ… **Consistency Regularization** between student and teacher\n",
    "- âœ… **Auto-GPU Detection** and optimization\n",
    "\n",
    "**Expected Performance:**\n",
    "- **Without Semi-Supervised**: ~60% accuracy (your current result)\n",
    "- **With Semi-Supervised**: 65-75% accuracy (leveraging 13,908 unlabeled images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e862068",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 1: Environment Setup\n",
    "\n",
    "Install required packages and check GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision timm transformers\n",
    "!pip install -q albumentations opencv-python\n",
    "!pip install -q wandb tqdm scikit-learn\n",
    "!pip install -q Pillow numpy pandas matplotlib seaborn\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"ðŸ”¥ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ðŸš€ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸŽ¯ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU available - training will be slow!\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸŽ® Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b909b30",
   "metadata": {},
   "source": [
    "## ðŸ“ Step 2: Mount Google Drive & Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0510ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Clone your GitHub repository\n",
    "!git clone https://github.com/cat-thomson/ViT-FishID.git\n",
    "%cd ViT-FishID\n",
    "\n",
    "# List files to confirm\n",
    "!ls -la\n",
    "\n",
    "print(\"\\nâœ… Repository cloned successfully!\")\n",
    "print(\"ðŸ“‚ Available files:\")\n",
    "!ls *.py | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e6873",
   "metadata": {},
   "source": [
    "## ðŸ—‚ï¸ Step 3: Setup Fish Dataset\n",
    "\n",
    "**Important**: Make sure your fish images are uploaded to Google Drive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration - UPDATE THESE PATHS\n",
    "DRIVE_DATA_PATH = \"/content/drive/MyDrive/fish_dataset.zip\"  # ðŸ‘ˆ UPDATE THIS\n",
    "# Alternative if your data is in a folder:\n",
    "# DRIVE_DATA_PATH = \"/content/drive/MyDrive/fish_images/\"  # ðŸ‘ˆ UPDATE THIS\n",
    "\n",
    "# Extract dataset if it's a zip file\n",
    "if DRIVE_DATA_PATH.endswith('.zip'):\n",
    "    print(f\"ðŸ“¦ Extracting {DRIVE_DATA_PATH}...\")\n",
    "    with zipfile.ZipFile(DRIVE_DATA_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content/')\n",
    "    \n",
    "    # Find the extracted folder\n",
    "    extracted_folders = [f for f in os.listdir('/content/') if os.path.isdir(f'/content/{f}') and 'fish' in f.lower()]\n",
    "    if extracted_folders:\n",
    "        data_dir = f'/content/{extracted_folders[0]}'\n",
    "        print(f\"âœ… Dataset extracted to: {data_dir}\")\n",
    "    else:\n",
    "        print(\"âŒ Could not find fish dataset folder\")\n",
    "        data_dir = '/content/fish_images'  # fallback\n",
    "else:\n",
    "    # Copy folder from Drive\n",
    "    data_dir = '/content/fish_images'\n",
    "    !cp -r {DRIVE_DATA_PATH} {data_dir}\n",
    "    print(f\"âœ… Dataset copied to: {data_dir}\")\n",
    "\n",
    "# Check dataset structure\n",
    "print(f\"\\nðŸ“Š Dataset contents:\")\n",
    "if os.path.exists(data_dir):\n",
    "    print(f\"ðŸ“ Total files: {len(list(Path(data_dir).rglob('*.*')))}\")\n",
    "    print(f\"ðŸ“ Directories: {len([d for d in Path(data_dir).iterdir() if d.is_dir()])}\")\n",
    "    \n",
    "    # Show first few directories\n",
    "    dirs = [d.name for d in Path(data_dir).iterdir() if d.is_dir()][:5]\n",
    "    print(f\"ðŸ“‚ Sample directories: {dirs}\")\n",
    "else:\n",
    "    print(f\"âŒ Dataset not found at {data_dir}\")\n",
    "    print(\"Please update DRIVE_DATA_PATH in the cell above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0317e70",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Step 4: Organize Dataset for Semi-Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize the dataset for semi-supervised learning\n",
    "organize_command = f\"python organize_fish_data.py --input_dir {data_dir} --output_dir /content/organized_fish_dataset --train_ratio 0.7 --val_ratio 0.2 --test_ratio 0.1 --seed 42\"\n",
    "\n",
    "print(f\"ðŸ”„ Organizing dataset...\")\n",
    "print(f\"Command: {organize_command}\")\n",
    "\n",
    "!{organize_command}\n",
    "\n",
    "# Verify organization\n",
    "print(\"\\nâœ… Dataset organization complete!\")\n",
    "print(\"ðŸ“Š Final structure:\")\n",
    "!ls -la /content/organized_fish_dataset/\n",
    "\n",
    "# Count files in each split\n",
    "labeled_dir = \"/content/organized_fish_dataset/labeled\"\n",
    "unlabeled_dir = \"/content/organized_fish_dataset/unlabeled\"\n",
    "val_dir = \"/content/organized_fish_dataset/val\"\n",
    "\n",
    "if os.path.exists(labeled_dir):\n",
    "    labeled_count = len(list(Path(labeled_dir).rglob('*.jpg'))) + len(list(Path(labeled_dir).rglob('*.png')))\n",
    "    print(f\"ðŸ“ˆ Labeled images: {labeled_count:,}\")\n",
    "\n",
    "if os.path.exists(unlabeled_dir):\n",
    "    unlabeled_count = len(list(Path(unlabeled_dir).rglob('*.jpg'))) + len(list(Path(unlabeled_dir).rglob('*.png')))\n",
    "    print(f\"ðŸ“Š Unlabeled images: {unlabeled_count:,}\")\n",
    "\n",
    "if os.path.exists(val_dir):\n",
    "    val_count = len(list(Path(val_dir).rglob('*.jpg'))) + len(list(Path(val_dir).rglob('*.png')))\n",
    "    print(f\"ðŸ“‹ Validation images: {val_count:,}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Semi-supervised ratio: {unlabeled_count/labeled_count:.1f}x unlabeled data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8494f7e7",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 5: Setup Weights & Biases (Optional)\n",
    "\n",
    "Track your training progress with W&B!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03133fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Login to W&B (optional - comment out if you don't want to use it)\n",
    "try:\n",
    "    wandb.login()\n",
    "    use_wandb = True\n",
    "    print(\"âœ… W&B login successful!\")\n",
    "except:\n",
    "    use_wandb = False\n",
    "    print(\"âš ï¸ W&B login skipped - training will run without logging\")\n",
    "\n",
    "# Alternatively, you can skip W&B entirely\n",
    "# use_wandb = False\n",
    "# print(\"ðŸ“Š Training without W&B logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaaaab2",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 6: Start Semi-Supervised Training (IMPROVED)\n",
    "\n",
    "**ðŸ”§ Fixed Issues:**\n",
    "- âœ… **Consistency Loss**: Now properly computed on ALL unlabeled data\n",
    "- âœ… **Pseudo-label Threshold**: Lowered to 0.7 (from 0.95) for better utilization\n",
    "- âœ… **Temperature Scaling**: Added for better probability calibration\n",
    "- âœ… **MSE Consistency Loss**: More stable than KL divergence\n",
    "\n",
    "**Expected Results:**\n",
    "- **Consistency Loss**: Should be > 0.001 (not 0.0000)\n",
    "- **High-conf Pseudo**: Should be 10-30% (not 0.0%)\n",
    "- **Validation Accuracy**: 65-75% (improved from 60%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f0c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved training command with optimized parameters\n",
    "training_command = f\"\"\"\n",
    "python main_semi_supervised.py \\\n",
    "    --data_dir /content/organized_fish_dataset \\\n",
    "    --epochs 50 \\\n",
    "    --batch_size 32 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --weight_decay 0.05 \\\n",
    "    --warmup_epochs 5 \\\n",
    "    --ramp_up_epochs 15 \\\n",
    "    --ema_momentum 0.999 \\\n",
    "    --consistency_loss mse \\\n",
    "    --consistency_weight 1.0 \\\n",
    "    --pseudo_label_threshold 0.7 \\\n",
    "    --temperature 4.0 \\\n",
    "    --unlabeled_ratio 2.0 \\\n",
    "    --save_frequency 10 \\\n",
    "    --model_name vit_base_patch16_224 \\\n",
    "    --pretrained \\\n",
    "    {'--use_wandb' if use_wandb else ''} \\\n",
    "    {'--wandb_project vit-fish-semi-supervised-colab' if use_wandb else ''}\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"ðŸš€ Starting IMPROVED Semi-Supervised Training...\")\n",
    "print(\"\\nðŸ”§ Key Improvements:\")\n",
    "print(\"  âœ… Pseudo-label threshold: 0.7 (was 0.95)\")\n",
    "print(\"  âœ… Fixed consistency loss computation\")\n",
    "print(\"  âœ… Temperature scaling: 4.0\")\n",
    "print(\"  âœ… MSE consistency loss (more stable)\")\n",
    "print(\"  âœ… Shorter warmup: 5 epochs (was 10)\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Command: {training_command}\")\n",
    "print(\"\\nâ° Training will take 2-4 hours depending on your GPU...\")\n",
    "print(\"\\nðŸŽ¯ Watch for:\")\n",
    "print(\"  - Cons Loss > 0.001 (should NOT be 0.0000)\")\n",
    "print(\"  - High-conf Pseudo: 10-30% (should NOT be 0.0%)\")\n",
    "print(\"  - Validation Acc: 65-75% (improvement from 60%)\")\n",
    "\n",
    "# Execute training\n",
    "!{training_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdac673",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Step 7: Monitor Training Progress\n",
    "\n",
    "Check if the semi-supervised learning is working correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c19c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training is producing the right outputs\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"ðŸ” Checking Semi-Supervised Training Health...\")\n",
    "\n",
    "# Check if checkpoints are being saved\n",
    "checkpoint_dir = \"/content/ViT-FishID/semi_supervised_checkpoints\"\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
    "    print(f\"âœ… Checkpoints saved: {len(checkpoints)}\")\n",
    "    if checkpoints:\n",
    "        print(f\"ðŸ“‚ Latest: {sorted(checkpoints)[-1]}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No checkpoints found yet\")\n",
    "\n",
    "# Tips for monitoring\n",
    "print(\"\\nðŸŽ¯ What to Look For:\")\n",
    "print(\"\")\n",
    "âœ… **Good Signs:**\n",
    "- Cons Loss: 0.001-0.1 (NOT 0.0000)\n",
    "- High-conf Pseudo: 10-50%\n",
    "- Validation accuracy increasing\n",
    "- Loss decreasing steadily\n",
    "\n",
    "âŒ **Bad Signs:**\n",
    "- Cons Loss: 0.0000 (semi-supervised not working)\n",
    "- High-conf Pseudo: 0.0% (threshold too high)\n",
    "- Validation accuracy stuck\n",
    "- Training loss not decreasing\n",
    "\n",
    "ðŸ”§ **If Problems:**\n",
    "1. Lower pseudo_label_threshold to 0.5\n",
    "2. Increase temperature to 5.0\n",
    "3. Check consistency_weight ramping\n",
    "\"\"\")\n",
    "\n",
    "# If using W&B, show link\n",
    "if use_wandb:\n",
    "    print(f\"\\nðŸ“Š Monitor training at: https://wandb.ai/\")\n",
    "    print(\"ðŸ”— Look for project: vit-fish-semi-supervised-colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6762e",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 8: Analyze Results\n",
    "\n",
    "Compare supervised vs semi-supervised performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffcfc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze the best model\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ðŸ“Š Analyzing Training Results...\")\n",
    "\n",
    "# Check if best model exists\n",
    "best_model_path = \"/content/ViT-FishID/semi_supervised_checkpoints/model_best.pth\"\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"âœ… Best model found: {best_model_path}\")\n",
    "    \n",
    "    # Load checkpoint info\n",
    "    try:\n",
    "        checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ Best Results:\")\n",
    "        print(f\"  ðŸ“ˆ Best Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "        print(f\"  ðŸŽ¯ Best Accuracy: {checkpoint.get('best_accuracy', 'N/A'):.2f}%\")\n",
    "        print(f\"  ðŸ“‰ Final Loss: {checkpoint.get('loss', 'N/A')}\")\n",
    "        \n",
    "        # Model info\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model_size = sum(p.numel() for p in checkpoint['model_state_dict'].values())\n",
    "            print(f\"  ðŸ§  Model Parameters: {model_size:,}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not load checkpoint details: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ Best model not found - training may still be running\")\n",
    "\n",
    "# Performance comparison\n",
    "print(\"\\nðŸ“ˆ Expected Performance Comparison:\")\n",
    "print(\"\")\n",
    "**Supervised Learning Only** (your previous result):\n",
    "- Validation Accuracy: ~60%\n",
    "- Uses only: 3,273 labeled images\n",
    "- Consistency Loss: 0.0000\n",
    "\n",
    "**Semi-Supervised Learning** (this improved version):\n",
    "- Validation Accuracy: 65-75% ðŸ“ˆ\n",
    "- Uses: 3,273 labeled + 13,908 unlabeled images\n",
    "- Consistency Loss: >0.001\n",
    "- High-conf Pseudo: 10-30%\n",
    "\n",
    "ðŸš€ **Expected Improvement: +5-15% accuracy**\n",
    "\"\"\")\n",
    "\n",
    "# Show training artifacts\n",
    "checkpoint_dir = \"/content/ViT-FishID/semi_supervised_checkpoints\"\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    files = os.listdir(checkpoint_dir)\n",
    "    print(f\"\\nðŸ“ Training Artifacts ({len(files)} files):\")\n",
    "    for f in sorted(files)[:10]:  # Show first 10\n",
    "        print(f\"  ðŸ“„ {f}\")\n",
    "    if len(files) > 10:\n",
    "        print(f\"  ... and {len(files)-10} more files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d44334",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 9: Download Results\n",
    "\n",
    "Package and download your trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cbcb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a results package\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ðŸ“¦ Packaging results for download...\")\n",
    "\n",
    "# Create timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_zip = f\"/content/vit_fish_semi_supervised_results_{timestamp}.zip\"\n",
    "\n",
    "with zipfile.ZipFile(results_zip, 'w') as zipf:\n",
    "    # Add checkpoints\n",
    "    checkpoint_dir = \"/content/ViT-FishID/semi_supervised_checkpoints\"\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        for file in os.listdir(checkpoint_dir):\n",
    "            if file.endswith('.pth'):\n",
    "                zipf.write(os.path.join(checkpoint_dir, file), f\"checkpoints/{file}\")\n",
    "    \n",
    "    # Add code files\n",
    "    code_files = [\n",
    "        \"main_semi_supervised.py\",\n",
    "        \"semi_supervised_trainer.py\", \n",
    "        \"vit_model.py\",\n",
    "        \"semi_supervised_data.py\"\n",
    "    ]\n",
    "    \n",
    "    for file in code_files:\n",
    "        if os.path.exists(f\"/content/ViT-FishID/{file}\"):\n",
    "            zipf.write(f\"/content/ViT-FishID/{file}\", f\"code/{file}\")\n",
    "    \n",
    "    # Add this notebook\n",
    "    notebook_files = [f for f in os.listdir(\"/content/ViT-FishID\") if f.endswith('.ipynb')]\n",
    "    for nb in notebook_files:\n",
    "        zipf.write(f\"/content/ViT-FishID/{nb}\", f\"notebooks/{nb}\")\n",
    "\n",
    "print(f\"âœ… Results packaged: {results_zip}\")\n",
    "print(f\"ðŸ“¦ Size: {os.path.getsize(results_zip) / 1e6:.1f} MB\")\n",
    "\n",
    "# Download the results\n",
    "from google.colab import files\n",
    "print(\"â¬‡ï¸ Starting download...\")\n",
    "files.download(results_zip)\n",
    "\n",
    "print(\"\\nðŸŽ‰ Training Complete!\")\n",
    "print(\"ðŸ“ Your package contains:\")\n",
    "print(\"  - ðŸ† Best trained model (model_best.pth)\")\n",
    "print(\"  - ðŸ’¾ Training checkpoints\")\n",
    "print(\"  - ðŸ“ All source code\")\n",
    "print(\"  - ðŸ““ This notebook\")\n",
    "\n",
    "# Copy to Google Drive as backup\n",
    "drive_backup = f\"/content/drive/MyDrive/vit_fish_results_{timestamp}.zip\"\n",
    "!cp {results_zip} {drive_backup}\n",
    "print(f\"\\nâ˜ï¸ Backup saved to Google Drive: {drive_backup}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de1fb4",
   "metadata": {},
   "source": [
    "## ðŸš€ Next Steps\n",
    "\n",
    "### ðŸŽ¯ **Key Improvements Made:**\n",
    "1. **Fixed Consistency Loss** - Now properly computed on ALL unlabeled data\n",
    "2. **Optimized Threshold** - Lowered from 0.95 to 0.7 for better utilization\n",
    "3. **Added Temperature Scaling** - Better probability calibration\n",
    "4. **Improved Loss Function** - MSE consistency loss for stability\n",
    "\n",
    "### ðŸ“Š **Expected Results:**\n",
    "- **Consistency Loss**: Should be > 0.001 (not 0.0000)\n",
    "- **High-confidence Pseudo-labels**: 10-30% (not 0.0%)\n",
    "- **Validation Accuracy**: 65-75% (improvement from 60%)\n",
    "\n",
    "### ðŸ”§ **If Results Aren't Good:**\n",
    "1. **Lower threshold further**: Try 0.5 or 0.6\n",
    "2. **Adjust temperature**: Try 5.0 or 6.0\n",
    "3. **Different consistency loss**: Try 'kl' instead of 'mse'\n",
    "4. **More unlabeled data**: Increase unlabeled_ratio to 3.0\n",
    "\n",
    "### ðŸ“ˆ **For Better Performance:**\n",
    "1. **Longer training**: Increase epochs to 100\n",
    "2. **Learning rate scheduling**: Add cosine annealing\n",
    "3. **Data augmentation**: More aggressive augmentations\n",
    "4. **Model size**: Try ViT-Large for better capacity\n",
    "\n",
    "### ðŸ† **Success Metrics:**\n",
    "- âœ… Cons Loss > 0.001\n",
    "- âœ… High-conf Pseudo > 10%\n",
    "- âœ… Validation Accuracy > 65%\n",
    "- âœ… Steady improvement over epochs\n",
    "\n",
    "**ðŸŽ‰ Happy Training!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
