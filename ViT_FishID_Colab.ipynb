{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b2847bf",
   "metadata": {},
   "source": [
    "# üêü ViT-FishID: Semi-Supervised Fish Classification in Google Colab\n",
    "\n",
    "This notebook adapts the ViT-FishID project to run in Google Colab, providing a complete end-to-end pipeline for semi-supervised fish classification using Vision Transformers and EMA teacher-student framework.\n",
    "\n",
    "## üöÄ What This Notebook Does:\n",
    "- Sets up the complete environment in Google Colab\n",
    "- Handles data upload and organization from Google Drive\n",
    "- Implements ViT-Base with EMA teacher-student semi-supervised learning\n",
    "- Provides interactive training with progress monitoring\n",
    "- Saves results and checkpoints to Google Drive\n",
    "\n",
    "## üìã Before You Start:\n",
    "1. **Enable GPU**: Go to Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better recommended)\n",
    "2. **Prepare Your Data**: Upload your fish images to Google Drive\n",
    "3. **Get W&B API Key**: Optional but recommended for experiment tracking\n",
    "\n",
    "Let's get started! üé£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c206a85",
   "metadata": {},
   "source": [
    "## üì¶ Section 1: Install Required Dependencies\n",
    "\n",
    "First, let's install all the required packages that aren't pre-installed in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fcfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q timm transformers wandb pillow opencv-python scikit-learn tqdm\n",
    "\n",
    "# Install PyTorch with CUDA support (if not already installed)\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Verify installations\n",
    "try:\n",
    "    import timm\n",
    "    import transformers\n",
    "    import wandb\n",
    "    from PIL import Image\n",
    "    import cv2\n",
    "    import sklearn\n",
    "    print(\"‚úÖ All packages installed successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing package: {e}\")\n",
    "\n",
    "# Set up for deterministic training\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "print(\"üéØ Random seed set to 42 for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcb4c94",
   "metadata": {},
   "source": [
    "## üíæ Section 2: Mount Google Drive\n",
    "\n",
    "Mount Google Drive to access your datasets and save training outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set up project directories in Google Drive\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
    "PROJECT_DIR = f\"{DRIVE_ROOT}/ViT-FishID\"\n",
    "DATA_DIR = f\"{PROJECT_DIR}/data\"\n",
    "CHECKPOINTS_DIR = f\"{PROJECT_DIR}/checkpoints\"\n",
    "RESULTS_DIR = f\"{PROJECT_DIR}/results\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"üìÇ Google Drive mounted successfully!\")\n",
    "print(f\"Project directory: {PROJECT_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Checkpoints directory: {CHECKPOINTS_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "\n",
    "# List contents of project directory\n",
    "if os.path.exists(PROJECT_DIR):\n",
    "    print(f\"\\nüìã Contents of {PROJECT_DIR}:\")\n",
    "    for item in os.listdir(PROJECT_DIR):\n",
    "        item_path = os.path.join(PROJECT_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  üìÅ {item}/\")\n",
    "        else:\n",
    "            print(f\"  üìÑ {item}\")\n",
    "else:\n",
    "    print(f\"üÜï {PROJECT_DIR} is empty - ready for your data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea45b2f",
   "metadata": {},
   "source": [
    "## üì§ Section 3: Upload Dataset to Google Drive\n",
    "\n",
    "Choose one of the following methods to get your fish dataset into Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad482fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Upload files directly to Colab (for small datasets)\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "def upload_and_extract_zip():\n",
    "    \"\"\"Upload a zip file and extract it to the data directory\"\"\"\n",
    "    print(\"üì§ Upload your fish dataset as a ZIP file:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    for filename in uploaded.keys():\n",
    "        if filename.endswith('.zip'):\n",
    "            print(f\"üì¶ Extracting {filename}...\")\n",
    "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(DATA_DIR)\n",
    "            print(f\"‚úÖ Extracted to {DATA_DIR}\")\n",
    "            os.remove(filename)  # Clean up\n",
    "        else:\n",
    "            # Move non-zip files to data directory\n",
    "            shutil.move(filename, os.path.join(DATA_DIR, filename))\n",
    "\n",
    "# Method 2: Use existing data from Google Drive\n",
    "def check_existing_data():\n",
    "    \"\"\"Check if data already exists in Google Drive\"\"\"\n",
    "    if os.path.exists(f\"{DATA_DIR}/organized_fish_dataset\"):\n",
    "        print(\"‚úÖ Found existing organized dataset!\")\n",
    "        return True\n",
    "    elif os.path.exists(f\"{DATA_DIR}/Images\"):\n",
    "        print(\"‚úÖ Found raw images directory!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå No existing data found in Google Drive\")\n",
    "        return False\n",
    "\n",
    "# Method 3: Download from URL (if you have a dataset URL)\n",
    "def download_from_url(url, filename):\n",
    "    \"\"\"Download dataset from URL\"\"\"\n",
    "    import urllib.request\n",
    "    print(f\"üì• Downloading {filename} from {url}...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    \n",
    "    if filename.endswith('.zip'):\n",
    "        print(\"üì¶ Extracting...\")\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall(DATA_DIR)\n",
    "        os.remove(filename)\n",
    "    print(\"‚úÖ Download complete!\")\n",
    "\n",
    "# Check what we have\n",
    "print(\"üîç Checking for existing data...\")\n",
    "has_data = check_existing_data()\n",
    "\n",
    "if not has_data:\n",
    "    print(\"\\nüéØ Choose your upload method:\")\n",
    "    print(\"1. Run upload_and_extract_zip() to upload a ZIP file\")\n",
    "    print(\"2. Manually upload to Google Drive and restart this cell\")\n",
    "    print(\"3. Use download_from_url(url, filename) if you have a URL\")\n",
    "    print(\"\\nExample: upload_and_extract_zip()\")\n",
    "else:\n",
    "    print(\"üìä Listing data directory contents:\")\n",
    "    for root, dirs, files in os.walk(DATA_DIR):\n",
    "        level = root.replace(DATA_DIR, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}üìÅ {os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files[:5]:  # Show first 5 files\n",
    "            print(f\"{subindent}üìÑ {file}\")\n",
    "        if len(files) > 5:\n",
    "            print(f\"{subindent}... and {len(files) - 5} more files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb4dd08",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Section 4: Set Up Directory Structure\n",
    "\n",
    "Create the necessary directory structure and organize data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe486279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "def organize_fish_data(input_dir, output_dir, labeled_species=None):\n",
    "    \"\"\"\n",
    "    Organize fish images into labeled and unlabeled directories\n",
    "    \"\"\"\n",
    "    if labeled_species is None:\n",
    "        # Common fish species - you can modify this list\n",
    "        labeled_species = ['bass', 'trout', 'salmon', 'tuna', 'cod', 'mackerel']\n",
    "    \n",
    "    # Create output structure\n",
    "    labeled_dir = os.path.join(output_dir, 'labeled')\n",
    "    unlabeled_dir = os.path.join(output_dir, 'unlabeled')\n",
    "    \n",
    "    os.makedirs(labeled_dir, exist_ok=True)\n",
    "    os.makedirs(unlabeled_dir, exist_ok=True)\n",
    "    \n",
    "    # Create species directories\n",
    "    for species in labeled_species:\n",
    "        os.makedirs(os.path.join(labeled_dir, species), exist_ok=True)\n",
    "    \n",
    "    # Find all image files\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.gif']\n",
    "    all_images = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        all_images.extend(glob.glob(os.path.join(input_dir, '**', ext), recursive=True))\n",
    "    \n",
    "    print(f\"üîç Found {len(all_images)} images\")\n",
    "    \n",
    "    # Organize images\n",
    "    species_counts = defaultdict(int)\n",
    "    unlabeled_count = 0\n",
    "    \n",
    "    for img_path in all_images:\n",
    "        filename = os.path.basename(img_path).lower()\n",
    "        assigned = False\n",
    "        \n",
    "        # Check if filename contains any labeled species\n",
    "        for species in labeled_species:\n",
    "            if species.lower() in filename:\n",
    "                dest_dir = os.path.join(labeled_dir, species)\n",
    "                dest_path = os.path.join(dest_dir, os.path.basename(img_path))\n",
    "                \n",
    "                # Copy file if not already there\n",
    "                if not os.path.exists(dest_path):\n",
    "                    shutil.copy2(img_path, dest_path)\n",
    "                \n",
    "                species_counts[species] += 1\n",
    "                assigned = True\n",
    "                break\n",
    "        \n",
    "        if not assigned:\n",
    "            # Move to unlabeled\n",
    "            dest_path = os.path.join(unlabeled_dir, os.path.basename(img_path))\n",
    "            if not os.path.exists(dest_path):\n",
    "                shutil.copy2(img_path, dest_path)\n",
    "            unlabeled_count += 1\n",
    "    \n",
    "    # Create dataset info\n",
    "    dataset_info = {\n",
    "        'total_images': len(all_images),\n",
    "        'labeled_species': dict(species_counts),\n",
    "        'unlabeled_count': unlabeled_count,\n",
    "        'species_list': labeled_species\n",
    "    }\n",
    "    \n",
    "    # Save dataset info\n",
    "    with open(os.path.join(output_dir, 'dataset_info.json'), 'w') as f:\n",
    "        json.dump(dataset_info, f, indent=2)\n",
    "    \n",
    "    print(\"\\nüìä Dataset Organization Complete!\")\n",
    "    print(f\"Total images: {len(all_images)}\")\n",
    "    print(f\"Labeled images: {sum(species_counts.values())}\")\n",
    "    print(f\"Unlabeled images: {unlabeled_count}\")\n",
    "    print(\"\\nSpecies breakdown:\")\n",
    "    for species, count in species_counts.items():\n",
    "        print(f\"  üêü {species}: {count} images\")\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "# Set up the organized dataset directory\n",
    "ORGANIZED_DATA_DIR = f\"{DATA_DIR}/organized_fish_dataset\"\n",
    "\n",
    "# Check if we need to organize data\n",
    "if os.path.exists(ORGANIZED_DATA_DIR) and os.path.exists(f\"{ORGANIZED_DATA_DIR}/dataset_info.json\"):\n",
    "    print(\"‚úÖ Found existing organized dataset!\")\n",
    "    \n",
    "    # Load and display dataset info\n",
    "    with open(f\"{ORGANIZED_DATA_DIR}/dataset_info.json\", 'r') as f:\n",
    "        dataset_info = json.load(f)\n",
    "    \n",
    "    print(f\"üìä Dataset Summary:\")\n",
    "    print(f\"  Total images: {dataset_info['total_images']}\")\n",
    "    print(f\"  Labeled images: {sum(dataset_info['labeled_species'].values())}\")\n",
    "    print(f\"  Unlabeled images: {dataset_info['unlabeled_count']}\")\n",
    "    print(f\"  Species: {', '.join(dataset_info['species_list'])}\")\n",
    "    \n",
    "else:\n",
    "    print(\"üîß Need to organize data...\")\n",
    "    \n",
    "    # Look for raw images\n",
    "    raw_images_dir = None\n",
    "    possible_dirs = [f\"{DATA_DIR}/Images\", f\"{DATA_DIR}/images\", f\"{DATA_DIR}/fish_images\"]\n",
    "    \n",
    "    for pdir in possible_dirs:\n",
    "        if os.path.exists(pdir):\n",
    "            raw_images_dir = pdir\n",
    "            break\n",
    "    \n",
    "    if raw_images_dir:\n",
    "        print(f\"üìÅ Found raw images in: {raw_images_dir}\")\n",
    "        print(\"üîÑ Organizing data...\")\n",
    "        \n",
    "        # You can customize these species based on your dataset\n",
    "        my_species = ['bass', 'trout', 'salmon', 'tuna', 'cod', 'mackerel', 'snapper', 'grouper']\n",
    "        \n",
    "        organize_fish_data(raw_images_dir, ORGANIZED_DATA_DIR, my_species)\n",
    "    else:\n",
    "        print(\"‚ùå No raw images found. Please upload your data first!\")\n",
    "        print(\"Expected directories: Images/, images/, or fish_images/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daef04d",
   "metadata": {},
   "source": [
    "## üß† Section 5: Model Definitions\n",
    "\n",
    "Now let's define our Vision Transformer and EMA teacher-student framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1703be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from copy import deepcopy\n",
    "\n",
    "class ViTFishClassifier(nn.Module):\n",
    "    \"\"\"Vision Transformer for Fish Classification\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, model_name='vit_base_patch16_224', pretrained=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Create ViT model - IMPORTANT: use global_pool='token' for classification\n",
    "        self.model = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=num_classes,\n",
    "            global_pool='token'  # This ensures we get [batch_size, num_classes] output\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Created ViT model: {model_name}\")\n",
    "        print(f\"   Parameters: {sum(p.numel() for p in self.model.parameters()) / 1e6:.1f}M\")\n",
    "        print(f\"   Classes: {num_classes}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        \"\"\"Extract features before the classification head\"\"\"\n",
    "        # Get features from the model\n",
    "        features = self.model.forward_features(x)\n",
    "        \n",
    "        # Global pool if needed\n",
    "        if hasattr(self.model, 'global_pool') and self.model.global_pool == 'token':\n",
    "            features = features[:, 0]  # Take CLS token\n",
    "        elif len(features.shape) > 2:\n",
    "            features = features.mean(dim=1)  # Global average pooling\n",
    "            \n",
    "        return features\n",
    "\n",
    "class EMATeacher:\n",
    "    \"\"\"Exponential Moving Average Teacher\"\"\"\n",
    "    \n",
    "    def __init__(self, student_model, momentum=0.999):\n",
    "        self.momentum = momentum\n",
    "        self.teacher_model = deepcopy(student_model)\n",
    "        \n",
    "        # Initialize teacher with student weights\n",
    "        for teacher_param, student_param in zip(\n",
    "            self.teacher_model.parameters(), \n",
    "            student_model.parameters()\n",
    "        ):\n",
    "            teacher_param.data.copy_(student_param.data)\n",
    "            teacher_param.requires_grad = False\n",
    "        \n",
    "        self.teacher_model.eval()\n",
    "        print(f\"‚úÖ EMA Teacher initialized with momentum: {momentum}\")\n",
    "    \n",
    "    def update(self, student_model):\n",
    "        \"\"\"Update teacher model using EMA\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for teacher_param, student_param in zip(\n",
    "                self.teacher_model.parameters(),\n",
    "                student_model.parameters()\n",
    "            ):\n",
    "                teacher_param.data = (\n",
    "                    self.momentum * teacher_param.data +\n",
    "                    (1.0 - self.momentum) * student_param.data\n",
    "                )\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Forward pass through teacher model\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.teacher_model(x)\n",
    "\n",
    "# Test the models\n",
    "def test_models():\n",
    "    \"\"\"Test model creation and forward pass\"\"\"\n",
    "    # Create a dummy batch\n",
    "    batch_size = 4\n",
    "    dummy_input = torch.randn(batch_size, 3, 224, 224)\n",
    "    \n",
    "    # Test with different number of classes\n",
    "    num_classes = 10  # Will be updated based on actual data\n",
    "    \n",
    "    print(\"üß™ Testing model creation...\")\n",
    "    \n",
    "    # Create student model\n",
    "    student = ViTFishClassifier(num_classes=num_classes)\n",
    "    \n",
    "    # Test forward pass\n",
    "    with torch.no_grad():\n",
    "        output = student(dummy_input)\n",
    "        print(f\"‚úÖ Student output shape: {output.shape}\")\n",
    "        assert output.shape == (batch_size, num_classes), f\"Expected {(batch_size, num_classes)}, got {output.shape}\"\n",
    "        \n",
    "        # Test feature extraction\n",
    "        features = student.get_features(dummy_input)\n",
    "        print(f\"‚úÖ Feature shape: {features.shape}\")\n",
    "    \n",
    "    # Create teacher model\n",
    "    teacher = EMATeacher(student)\n",
    "    \n",
    "    # Test teacher forward pass\n",
    "    with torch.no_grad():\n",
    "        teacher_output = teacher(dummy_input)\n",
    "        print(f\"‚úÖ Teacher output shape: {teacher_output.shape}\")\n",
    "        assert teacher_output.shape == (batch_size, num_classes)\n",
    "    \n",
    "    print(\"üéâ All model tests passed!\")\n",
    "    \n",
    "    return student, teacher\n",
    "\n",
    "# Run tests\n",
    "student_model, teacher_model = test_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ac84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "class FishDataset(Dataset):\n",
    "    \"\"\"Dataset for fish images\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels=None, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.is_labeled = labels is not None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.is_labeled:\n",
    "            return image, self.labels[idx]\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "def create_transforms(image_size=224, is_training=True):\n",
    "    \"\"\"Create data transforms\"\"\"\n",
    "    if is_training:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "def prepare_data_loaders(data_dir, batch_size=32, val_split=0.2):\n",
    "    \"\"\"Prepare data loaders for semi-supervised learning\"\"\"\n",
    "    \n",
    "    # Load dataset info\n",
    "    with open(os.path.join(data_dir, 'dataset_info.json'), 'r') as f:\n",
    "        dataset_info = json.load(f)\n",
    "    \n",
    "    # Create species to ID mapping\n",
    "    species_list = list(dataset_info['labeled_species'].keys())\n",
    "    species_to_id = {species: idx for idx, species in enumerate(species_list)}\n",
    "    id_to_species = {idx: species for species, idx in species_to_id.items()}\n",
    "    num_classes = len(species_list)\n",
    "    \n",
    "    print(f\"üìä Found {num_classes} species: {species_list}\")\n",
    "    \n",
    "    # Collect labeled data\n",
    "    labeled_paths = []\n",
    "    labeled_labels = []\n",
    "    \n",
    "    labeled_dir = os.path.join(data_dir, 'labeled')\n",
    "    for species, species_id in species_to_id.items():\n",
    "        species_dir = os.path.join(labeled_dir, species)\n",
    "        if os.path.exists(species_dir):\n",
    "            species_paths = glob.glob(os.path.join(species_dir, '*.*'))\n",
    "            labeled_paths.extend(species_paths)\n",
    "            labeled_labels.extend([species_id] * len(species_paths))\n",
    "    \n",
    "    print(f\"üìö Labeled data: {len(labeled_paths)} images\")\n",
    "    \n",
    "    # Collect unlabeled data\n",
    "    unlabeled_dir = os.path.join(data_dir, 'unlabeled')\n",
    "    unlabeled_paths = []\n",
    "    if os.path.exists(unlabeled_dir):\n",
    "        unlabeled_paths = glob.glob(os.path.join(unlabeled_dir, '*.*'))\n",
    "    \n",
    "    print(f\"üîÑ Unlabeled data: {len(unlabeled_paths)} images\")\n",
    "    \n",
    "    # Split labeled data into train/val\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    if len(labeled_paths) > 0:\n",
    "        train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "            labeled_paths, labeled_labels, \n",
    "            test_size=val_split, \n",
    "            stratify=labeled_labels,\n",
    "            random_state=42\n",
    "        )\n",
    "    else:\n",
    "        train_paths, val_paths, train_labels, val_labels = [], [], [], []\n",
    "    \n",
    "    print(f\"üèãÔ∏è Training labeled: {len(train_paths)} images\")\n",
    "    print(f\"üéØ Validation: {len(val_paths)} images\")\n",
    "    \n",
    "    # Create transforms\n",
    "    train_transform = create_transforms(is_training=True)\n",
    "    val_transform = create_transforms(is_training=False)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_labeled_dataset = FishDataset(train_paths, train_labels, train_transform)\n",
    "    val_dataset = FishDataset(val_paths, val_labels, val_transform)\n",
    "    unlabeled_dataset = FishDataset(unlabeled_paths, None, train_transform)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_labeled_loader = DataLoader(\n",
    "        train_labeled_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    unlabeled_loader = DataLoader(\n",
    "        unlabeled_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    ) if len(unlabeled_paths) > 0 else None\n",
    "    \n",
    "    return {\n",
    "        'train_labeled': train_labeled_loader,\n",
    "        'validation': val_loader,\n",
    "        'unlabeled': unlabeled_loader,\n",
    "        'num_classes': num_classes,\n",
    "        'species_to_id': species_to_id,\n",
    "        'id_to_species': id_to_species,\n",
    "        'dataset_info': dataset_info\n",
    "    }\n",
    "\n",
    "# Test data loading\n",
    "if os.path.exists(ORGANIZED_DATA_DIR):\n",
    "    print(\"üîç Testing data loading...\")\n",
    "    data_info = prepare_data_loaders(ORGANIZED_DATA_DIR, batch_size=8)\n",
    "    print(f\"‚úÖ Data loaders created successfully!\")\n",
    "    print(f\"   Number of classes: {data_info['num_classes']}\")\n",
    "    print(f\"   Species: {list(data_info['species_to_id'].keys())}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No organized data found. Please run the data organization step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7330a87",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Section 6: Training Pipeline\n",
    "\n",
    "Now let's implement the semi-supervised training loop with EMA teacher-student framework."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
