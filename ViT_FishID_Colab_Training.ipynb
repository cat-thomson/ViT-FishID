{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796c6e33",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/cat-thomson/ViT-FishID/blob/main/ViT_FishID_Colab_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2f60df",
   "metadata": {},
   "source": [
    "# 🐟 ViT-FishID: Semi-Supervised Training in Google Colab\n",
    "\n",
    "This notebook demonstrates how to train the ViT-FishID model using Google Colab's free GPU resources.\n",
    "\n",
    "**Expected Performance:**\n",
    "- Training Time: 2-3 hours for 50 epochs on Tesla T4\n",
    "- Memory Usage: ~6-8GB GPU memory\n",
    "- Accuracy: ~75-85% validation accuracy\n",
    "\n",
    "**Requirements:**\n",
    "- Google account with Google Drive access\n",
    "- Fish dataset uploaded to Google Drive\n",
    "- GPU runtime enabled in Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bcb2a3",
   "metadata": {},
   "source": [
    "## 🚀 Step 1: Setup and GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3540b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"🔍 System Information:\")\n",
    "print(f\"Python version: {os.sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(\"✅ GPU is ready for training!\")\n",
    "else:\n",
    "    print(\"❌ No GPU detected. Please enable GPU runtime:\")\n",
    "    print(\"   Runtime → Change runtime type → Hardware accelerator → GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149f671b",
   "metadata": {},
   "source": [
    "## 📁 Step 2: Mount Google Drive\n",
    "\n",
    "This will give us access to your fish dataset stored in Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# List contents to verify mount\n",
    "print(\"\\n📂 Google Drive contents:\")\n",
    "drive_path = '/content/drive/MyDrive'\n",
    "if os.path.exists(drive_path):\n",
    "    items = os.listdir(drive_path)[:10]  # Show first 10 items\n",
    "    for item in items:\n",
    "        print(f\"  - {item}\")\n",
    "    if len(os.listdir(drive_path)) > 10:\n",
    "        print(f\"  ... and {len(os.listdir(drive_path)) - 10} more items\")\n",
    "    print(\"\\n✅ Google Drive mounted successfully!\")\n",
    "else:\n",
    "    print(\"❌ Failed to mount Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b6273",
   "metadata": {},
   "source": [
    "## 📦 Step 3: Install Dependencies\n",
    "\n",
    "Installing all required packages for ViT-FishID training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c724abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"📦 Installing dependencies...\")\n",
    "\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q timm transformers\n",
    "!pip install -q albumentations\n",
    "!pip install -q wandb\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q matplotlib seaborn\n",
    "!pip install -q tqdm\n",
    "\n",
    "print(\"✅ All dependencies installed successfully!\")\n",
    "\n",
    "# Verify installations\n",
    "import torch\n",
    "import torchvision\n",
    "import timm\n",
    "import albumentations\n",
    "import cv2\n",
    "import sklearn\n",
    "\n",
    "print(\"\\n📋 Package versions:\")\n",
    "print(f\"  - torch: {torch.__version__}\")\n",
    "print(f\"  - torchvision: {torchvision.__version__}\")\n",
    "print(f\"  - timm: {timm.__version__}\")\n",
    "print(f\"  - albumentations: {albumentations.__version__}\")\n",
    "print(f\"  - opencv: {cv2.__version__}\")\n",
    "print(f\"  - sklearn: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b795fc",
   "metadata": {},
   "source": [
    "## 🔄 Step 4: Clone ViT-FishID Repository\n",
    "\n",
    "Getting the latest code from your GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "import os\n",
    "\n",
    "# Remove existing directory if it exists\n",
    "if os.path.exists('/content/ViT-FishID'):\n",
    "    !rm -rf /content/ViT-FishID\n",
    "\n",
    "# Clone the repository\n",
    "print(\"📥 Cloning ViT-FishID repository...\")\n",
    "!git clone https://github.com/cat-thomson/ViT-FishID.git /content/ViT-FishID\n",
    "\n",
    "# Change to project directory\n",
    "%cd /content/ViT-FishID\n",
    "\n",
    "# List project files\n",
    "print(\"\\n📂 Project structure:\")\n",
    "!ls -la\n",
    "\n",
    "print(\"\\n✅ Repository cloned successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155c400",
   "metadata": {},
   "source": [
    "## 🗂️ Step 5: Setup Data Path and Extraction\n",
    "\n",
    "**IMPORTANT:** Specify the path to your fish dataset ZIP file in Google Drive.\n",
    "\n",
    "This step will:\n",
    "1. Locate your `fish_cutouts.zip` file in Google Drive\n",
    "2. Extract it to Colab's local storage for faster access\n",
    "3. Validate the data structure\n",
    "\n",
    "Expected structure after extraction:\n",
    "```\n",
    "fish_cutouts/\n",
    "├── labeled/\n",
    "│   ├── species_1/\n",
    "│   │   ├── fish_001.jpg\n",
    "│   │   └── fish_002.jpg\n",
    "│   └── species_2/\n",
    "│       └── ...\n",
    "└── unlabeled/\n",
    "    ├── fish_003.jpg\n",
    "    └── fish_004.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4008678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY THIS PATH to point to your fish_cutouts.zip file in Google Drive\n",
    "ZIP_FILE_PATH = '/content/drive/MyDrive/fish_cutouts.zip'  # 👈 Your ZIP file path\n",
    "\n",
    "# Alternative common paths (uncomment the one that matches your setup):\n",
    "# ZIP_FILE_PATH = '/content/drive/MyDrive/ViT-FishID/fish_cutouts.zip'\n",
    "# ZIP_FILE_PATH = '/content/drive/MyDrive/datasets/fish_cutouts.zip'\n",
    "# ZIP_FILE_PATH = '/content/drive/MyDrive/data/fish_cutouts.zip'\n",
    "\n",
    "# Local extraction directory (will be created in Colab)\n",
    "DATA_DIR = '/content/fish_cutouts'\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "print(f\"🎯 ZIP file location: {ZIP_FILE_PATH}\")\n",
    "print(f\"📁 Extraction target: {DATA_DIR}\")\n",
    "\n",
    "# Check if ZIP file exists\n",
    "if os.path.exists(ZIP_FILE_PATH):\n",
    "    print(\"✅ ZIP file found!\")\n",
    "\n",
    "    # Get ZIP file size\n",
    "    zip_size_mb = os.path.getsize(ZIP_FILE_PATH) / (1024 * 1024)\n",
    "    print(f\"📦 ZIP file size: {zip_size_mb:.1f} MB\")\n",
    "\n",
    "    # Remove existing extracted data if present\n",
    "    if os.path.exists(DATA_DIR):\n",
    "        print(\"🧹 Removing existing extracted data...\")\n",
    "        shutil.rmtree(DATA_DIR)\n",
    "\n",
    "    # Extract ZIP file\n",
    "    print(\"📤 Extracting ZIP file to local storage...\")\n",
    "    print(\"⏳ This may take a few minutes for large datasets...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(ZIP_FILE_PATH, 'r') as zip_ref:\n",
    "            # Extract to temporary location first\n",
    "            temp_extract_dir = '/content/temp_extract'\n",
    "            if os.path.exists(temp_extract_dir):\n",
    "                shutil.rmtree(temp_extract_dir)\n",
    "            zip_ref.extractall(temp_extract_dir)\n",
    "\n",
    "            # DEBUG: Show what was extracted\n",
    "            extracted_items = os.listdir(temp_extract_dir)\n",
    "            print(f\"🔍 DEBUG - Extracted items: {extracted_items}\")\n",
    "            \n",
    "            # Look for the actual dataset (skip macOS artifacts)\n",
    "            data_candidates = [item for item in extracted_items \n",
    "                             if not item.startswith('.') and not item.startswith('__MACOSX')]\n",
    "            print(f\"🔍 DEBUG - Data candidates: {data_candidates}\")\n",
    "\n",
    "            # Find the directory that contains labeled/ and unlabeled/\n",
    "            dataset_found = False\n",
    "            \n",
    "            for candidate in data_candidates:\n",
    "                candidate_path = os.path.join(temp_extract_dir, candidate)\n",
    "                if os.path.isdir(candidate_path):\n",
    "                    print(f\"🔍 DEBUG - Checking {candidate}: {os.listdir(candidate_path)}\")\n",
    "                    \n",
    "                    # Check if this directory has labeled/ and unlabeled/\n",
    "                    if 'labeled' in os.listdir(candidate_path) and 'unlabeled' in os.listdir(candidate_path):\n",
    "                        print(f\"✅ Found dataset in: {candidate}\")\n",
    "                        shutil.move(candidate_path, DATA_DIR)\n",
    "                        dataset_found = True\n",
    "                        break\n",
    "                    \n",
    "                    # Check one level deeper\n",
    "                    subdirs = [d for d in os.listdir(candidate_path) \n",
    "                             if os.path.isdir(os.path.join(candidate_path, d)) and not d.startswith('.') and not d.startswith('__')]\n",
    "                    \n",
    "                    for subdir in subdirs:\n",
    "                        subdir_path = os.path.join(candidate_path, subdir)\n",
    "                        print(f\"🔍 DEBUG - Checking {candidate}/{subdir}: {os.listdir(subdir_path)}\")\n",
    "                        \n",
    "                        if 'labeled' in os.listdir(subdir_path) and 'unlabeled' in os.listdir(subdir_path):\n",
    "                            print(f\"✅ Found dataset one level deeper in: {candidate}/{subdir}\")\n",
    "                            shutil.move(subdir_path, DATA_DIR)\n",
    "                            dataset_found = True\n",
    "                            break\n",
    "                    \n",
    "                    if dataset_found:\n",
    "                        break\n",
    "            \n",
    "            if not dataset_found:\n",
    "                print(\"⚠️  Could not find labeled/ and unlabeled/ folders, moving first candidate\")\n",
    "                if data_candidates:\n",
    "                    shutil.move(os.path.join(temp_extract_dir, data_candidates[0]), DATA_DIR)\n",
    "                else:\n",
    "                    shutil.move(temp_extract_dir, DATA_DIR)\n",
    "\n",
    "            # Clean up temp directory if it still exists\n",
    "            if os.path.exists(temp_extract_dir):\n",
    "                shutil.rmtree(temp_extract_dir)\n",
    "\n",
    "        extraction_time = time.time() - start_time\n",
    "        print(f\"✅ Extraction completed in {extraction_time:.1f} seconds!\")\n",
    "\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"❌ Error: Invalid ZIP file format\")\n",
    "        print(\"Please check that your file is a valid ZIP archive\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during extraction: {str(e)}\")\n",
    "        print(\"Please check the ZIP file path and try again\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ ZIP file not found!\")\n",
    "    print(\"\\n🔧 To fix this:\")\n",
    "    print(\"1. Upload your fish_cutouts.zip file to Google Drive\")\n",
    "    print(\"2. Update the ZIP_FILE_PATH variable above with the correct path\")\n",
    "    print(\"3. Make sure the file name is exactly 'fish_cutouts.zip'\")\n",
    "    print(\"\\n💡 Common locations to check:\")\n",
    "    print(\"   - /content/drive/MyDrive/fish_cutouts.zip\")\n",
    "    print(\"   - /content/drive/MyDrive/ViT-FishID/fish_cutouts.zip\")\n",
    "    print(\"   - /content/drive/MyDrive/datasets/fish_cutouts.zip\")\n",
    "\n",
    "# Now validate the extracted data\n",
    "print(f\"\\n📊 Validating extracted dataset...\")\n",
    "\n",
    "# Check if data directory exists\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(\"✅ Data directory found!\")\n",
    "    \n",
    "    # DEBUG: Show the actual structure\n",
    "    print(\"🔍 DEBUG - Final directory structure:\")\n",
    "    for item in os.listdir(DATA_DIR):\n",
    "        item_path = os.path.join(DATA_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  📂 {item}/\")\n",
    "            try:\n",
    "                subitems = os.listdir(item_path)[:10]  # Show first 10 items\n",
    "                for subitem in subitems:\n",
    "                    subitem_path = os.path.join(item_path, subitem)\n",
    "                    if os.path.isdir(subitem_path):\n",
    "                        print(f\"    📂 {subitem}/\")\n",
    "                    else:\n",
    "                        print(f\"    📄 {subitem}\")\n",
    "                if len(os.listdir(item_path)) > 10:\n",
    "                    print(f\"    ... and {len(os.listdir(item_path)) - 10} more items\")\n",
    "            except (PermissionError, OSError):\n",
    "                print(\"    (cannot read contents)\")\n",
    "        else:\n",
    "            print(f\"  📄 {item}\")\n",
    "\n",
    "    # Show directory size\n",
    "    def get_dir_size(path):\n",
    "        total = 0\n",
    "        for dirpath, dirnames, filenames in os.walk(path):\n",
    "            for filename in filenames:\n",
    "                total += os.path.getsize(os.path.join(dirpath, filename))\n",
    "        return total / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "    dir_size_mb = get_dir_size(DATA_DIR)\n",
    "    print(f\"📏 Extracted dataset size: {dir_size_mb:.1f} MB\")\n",
    "\n",
    "    # Check for labeled and unlabeled subdirectories\n",
    "    labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
    "    unlabeled_dir = os.path.join(DATA_DIR, 'unlabeled')\n",
    "\n",
    "    if os.path.exists(labeled_dir) and os.path.exists(unlabeled_dir):\n",
    "        print(\"✅ Semi-supervised structure detected (labeled/ and unlabeled/ folders)\")\n",
    "\n",
    "        # Count classes and samples\n",
    "        classes = [d for d in os.listdir(labeled_dir) if os.path.isdir(os.path.join(labeled_dir, d))]\n",
    "        print(f\"📊 Found {len(classes)} species classes\")\n",
    "\n",
    "        # Count labeled samples\n",
    "        labeled_count = 0\n",
    "        sample_classes = classes[:5]  # Show first 5 classes\n",
    "        for class_dir in sample_classes:\n",
    "            class_path = os.path.join(labeled_dir, class_dir)\n",
    "            if os.path.isdir(class_path):\n",
    "                class_samples = len([f for f in os.listdir(class_path)\n",
    "                                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "                labeled_count += class_samples\n",
    "                print(f\"  - {class_dir}: {class_samples} samples\")\n",
    "\n",
    "        if len(classes) > 5:\n",
    "            # Count remaining classes\n",
    "            remaining_count = 0\n",
    "            for class_dir in classes[5:]:\n",
    "                class_path = os.path.join(labeled_dir, class_dir)\n",
    "                if os.path.isdir(class_path):\n",
    "                    class_samples = len([f for f in os.listdir(class_path)\n",
    "                                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "                    remaining_count += class_samples\n",
    "            labeled_count += remaining_count\n",
    "            print(f\"  ... and {len(classes) - 5} more classes with {remaining_count} samples\")\n",
    "\n",
    "        print(f\"📊 Total labeled samples: {labeled_count:,}\")\n",
    "\n",
    "        # Count unlabeled samples\n",
    "        unlabeled_files = [f for f in os.listdir(unlabeled_dir)\n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        print(f\"📊 Unlabeled samples: {len(unlabeled_files):,}\")\n",
    "\n",
    "        print(f\"\\n🎯 Dataset ready for training!\")\n",
    "        print(f\"   Total samples: {labeled_count + len(unlabeled_files):,}\")\n",
    "        print(f\"   Labeled: {labeled_count:,} ({labeled_count/(labeled_count + len(unlabeled_files))*100:.1f}%)\")\n",
    "        print(f\"   Unlabeled: {len(unlabeled_files):,} ({len(unlabeled_files)/(labeled_count + len(unlabeled_files))*100:.1f}%)\")\n",
    "\n",
    "    elif any(os.path.isdir(os.path.join(DATA_DIR, d)) for d in os.listdir(DATA_DIR)):\n",
    "        print(\"ℹ️  Supervised structure detected (species folders directly in data dir)\")\n",
    "        classes = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
    "        print(f\"📊 Found {len(classes)} species classes\")\n",
    "        print(\"⚠️  Note: For semi-supervised training, organize data into labeled/ and unlabeled/ folders\")\n",
    "        \n",
    "        # Let's see what these 2 classes actually are\n",
    "        print(\"\udd0d DEBUG - The 2 'classes' found are:\")\n",
    "        for class_dir in classes:\n",
    "            class_path = os.path.join(DATA_DIR, class_dir)\n",
    "            print(f\"  📂 {class_dir}/\")\n",
    "            try:\n",
    "                items = os.listdir(class_path)[:5]\n",
    "                for item in items:\n",
    "                    print(f\"    - {item}\")\n",
    "                if len(os.listdir(class_path)) > 5:\n",
    "                    print(f\"    ... and {len(os.listdir(class_path)) - 5} more items\")\n",
    "            except:\n",
    "                print(\"    (cannot read contents)\")\n",
    "    else:\n",
    "        print(\"❌ No valid data structure found in extracted files\")\n",
    "        print(\"Expected: labeled/ and unlabeled/ subdirectories\")\n",
    "        print(\"\\n\udcc1 Current structure:\")\n",
    "        for item in os.listdir(DATA_DIR):\n",
    "            print(f\"  - {item}\")\n",
    "else:\n",
    "    print(\"❌ Data extraction failed or directory not found!\")\n",
    "    print(\"Please check the ZIP file and extraction process above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0fe32",
   "metadata": {},
   "source": [
    "## 📊 Step 6: Setup Weights & Biases (Optional)\n",
    "\n",
    "W&B provides excellent training visualization and experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Weights & Biases for experiment tracking\n",
    "import wandb\n",
    "\n",
    "# Login to W&B (you'll need to create a free account at wandb.ai)\n",
    "print(\"🔐 Setting up Weights & Biases...\")\n",
    "print(\"\\nTo use W&B:\")\n",
    "print(\"1. Go to https://wandb.ai and create a free account\")\n",
    "print(\"2. Get your API key from https://wandb.ai/authorize\")\n",
    "print(\"3. Run the cell below and paste your API key when prompted\")\n",
    "print(\"\\nOr skip W&B by setting USE_WANDB = False below\")\n",
    "\n",
    "# Set this to True if you want to use W&B, False to skip\n",
    "USE_WANDB = True  # 👈 Set to False if you don't want to use W&B\n",
    "\n",
    "if USE_WANDB:\n",
    "    try:\n",
    "        # Try to login (will prompt for API key if not already logged in)\n",
    "        wandb.login()\n",
    "        print(\"✅ W&B login successful!\")\n",
    "    except:\n",
    "        print(\"⚠️  W&B login failed. Training will continue without W&B logging.\")\n",
    "        USE_WANDB = False\n",
    "else:\n",
    "    print(\"ℹ️  Skipping W&B setup. Training will run without experiment tracking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe6af6d",
   "metadata": {},
   "source": [
    "## ⚙️ Step 7: Configure Training Parameters\n",
    "\n",
    "Adjust these parameters based on your needs and available GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e959fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    # Basic settings\n",
    "    'mode': 'semi_supervised',  # 'supervised' or 'semi_supervised'\n",
    "    'epochs': 50,               # Reduced for Colab (normally 100)\n",
    "    'batch_size': 16,           # Reduced for GPU memory (normally 32)\n",
    "    'image_size': 224,\n",
    "    'num_workers': 2,           # Reduced for Colab\n",
    "    \n",
    "    # Data splitting\n",
    "    'val_split': 0.2,          # 20% for validation\n",
    "    'test_split': 0.2,         # 20% for test\n",
    "    \n",
    "    # Model settings\n",
    "    'model_name': 'vit_base_patch16_224',\n",
    "    'pretrained': True,\n",
    "    'dropout_rate': 0.1,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.05,\n",
    "    'warmup_epochs': 5,        # Reduced for shorter training\n",
    "    \n",
    "    # Semi-supervised settings\n",
    "    'consistency_weight': 2.0,\n",
    "    'pseudo_label_threshold': 0.7,\n",
    "    'temperature': 4.0,\n",
    "    'unlabeled_ratio': 2.0,\n",
    "    'ramp_up_epochs': 10,      # Reduced for shorter training\n",
    "    \n",
    "    # EMA settings\n",
    "    'ema_momentum': 0.999,\n",
    "    \n",
    "    # Logging\n",
    "    'use_wandb': USE_WANDB,\n",
    "    'wandb_project': 'vit-fish-colab',\n",
    "    'save_frequency': 10,      # Save every 10 epochs\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(\"🎛️  Training Configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n💡 Tips for Colab:\")\n",
    "print(f\"  - Batch size {TRAINING_CONFIG['batch_size']} should work on most Colab GPUs\")\n",
    "print(f\"  - {TRAINING_CONFIG['epochs']} epochs will take ~2-3 hours\")\n",
    "print(\"  - If you get GPU memory errors, reduce batch_size to 8\")\n",
    "print(\"  - Training will automatically save checkpoints every 10 epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9cbd50",
   "metadata": {},
   "source": [
    "## 🚀 Step 8: Start Training!\n",
    "\n",
    "This cell will start the semi-supervised training process. It may take 2-3 hours to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace87e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training command\n",
    "training_cmd = f\"\"\"python train.py \\\n",
    "    --data_dir \"{DATA_DIR}\" \\\n",
    "    --mode {TRAINING_CONFIG['mode']} \\\n",
    "    --epochs {TRAINING_CONFIG['epochs']} \\\n",
    "    --batch_size {TRAINING_CONFIG['batch_size']} \\\n",
    "    --image_size {TRAINING_CONFIG['image_size']} \\\n",
    "    --num_workers {TRAINING_CONFIG['num_workers']} \\\n",
    "    --val_split {TRAINING_CONFIG['val_split']} \\\n",
    "    --test_split {TRAINING_CONFIG['test_split']} \\\n",
    "    --model_name {TRAINING_CONFIG['model_name']} \\\n",
    "    --learning_rate {TRAINING_CONFIG['learning_rate']} \\\n",
    "    --weight_decay {TRAINING_CONFIG['weight_decay']} \\\n",
    "    --warmup_epochs {TRAINING_CONFIG['warmup_epochs']} \\\n",
    "    --consistency_weight {TRAINING_CONFIG['consistency_weight']} \\\n",
    "    --pseudo_label_threshold {TRAINING_CONFIG['pseudo_label_threshold']} \\\n",
    "    --temperature {TRAINING_CONFIG['temperature']} \\\n",
    "    --unlabeled_ratio {TRAINING_CONFIG['unlabeled_ratio']} \\\n",
    "    --ramp_up_epochs {TRAINING_CONFIG['ramp_up_epochs']} \\\n",
    "    --ema_momentum {TRAINING_CONFIG['ema_momentum']} \\\n",
    "    --save_frequency {TRAINING_CONFIG['save_frequency']} \\\n",
    "    --seed {TRAINING_CONFIG['seed']}\"\"\"\n",
    "\n",
    "if TRAINING_CONFIG['use_wandb']:\n",
    "    training_cmd += f\" --use_wandb --wandb_project {TRAINING_CONFIG['wandb_project']}\"\n",
    "\n",
    "if TRAINING_CONFIG['pretrained']:\n",
    "    training_cmd += \" --pretrained\"\n",
    "\n",
    "print(\"🚀 Starting ViT-FishID training...\")\n",
    "print(\"\\n📋 Training command:\")\n",
    "print(training_cmd.replace('\\\\', '').strip())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Execute training\n",
    "!{training_cmd}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af5177",
   "metadata": {},
   "source": [
    "## 📊 Step 9: Check Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea96e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for saved checkpoints\n",
    "import os\n",
    "import glob\n",
    "\n",
    "checkpoint_dir = '/content/ViT-FishID/checkpoints'\n",
    "print(f\"📁 Checking for checkpoints in: {checkpoint_dir}\")\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = glob.glob(os.path.join(checkpoint_dir, '*.pth'))\n",
    "    if checkpoints:\n",
    "        print(f\"\\n✅ Found {len(checkpoints)} checkpoint(s):\")\n",
    "        for checkpoint in sorted(checkpoints):\n",
    "            file_size = os.path.getsize(checkpoint) / (1024**2)  # MB\n",
    "            print(f\"  - {os.path.basename(checkpoint)} ({file_size:.1f} MB)\")\n",
    "        \n",
    "        # Check if best model exists\n",
    "        best_model = os.path.join(checkpoint_dir, 'model_best.pth')\n",
    "        if os.path.exists(best_model):\n",
    "            print(f\"\\n🏆 Best model saved: model_best.pth\")\n",
    "    else:\n",
    "        print(\"❌ No checkpoints found\")\n",
    "else:\n",
    "    print(\"❌ Checkpoint directory not found\")\n",
    "\n",
    "# Show training logs summary\n",
    "print(\"\\n📊 Training Summary:\")\n",
    "print(\"Check the training output above for final accuracy metrics\")\n",
    "\n",
    "if TRAINING_CONFIG['use_wandb']:\n",
    "    print(\"\\n📈 For detailed metrics and visualizations, check your W&B dashboard:\")\n",
    "    print(f\"https://wandb.ai/your-username/{TRAINING_CONFIG['wandb_project']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff698a72",
   "metadata": {},
   "source": [
    "## 💾 Step 10: Download Model and Results\n",
    "\n",
    "Save your trained model to Google Drive for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89513455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy trained model to Google Drive\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a timestamped folder in Google Drive\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_dir = f'/content/drive/MyDrive/ViT-FishID_Training_{timestamp}'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"💾 Saving results to Google Drive: {save_dir}\")\n",
    "\n",
    "# Copy checkpoints\n",
    "checkpoint_dir = '/content/ViT-FishID/checkpoints'\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    drive_checkpoint_dir = os.path.join(save_dir, 'checkpoints')\n",
    "    shutil.copytree(checkpoint_dir, drive_checkpoint_dir)\n",
    "    print(f\"✅ Checkpoints saved to: {drive_checkpoint_dir}\")\n",
    "\n",
    "# Save training configuration\n",
    "import json\n",
    "config_file = os.path.join(save_dir, 'training_config.json')\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(TRAINING_CONFIG, f, indent=2)\n",
    "print(f\"✅ Training config saved to: {config_file}\")\n",
    "\n",
    "# Create a summary file\n",
    "summary_file = os.path.join(save_dir, 'training_summary.txt')\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(f\"ViT-FishID Training Summary\\n\")\n",
    "    f.write(f\"========================\\n\\n\")\n",
    "    f.write(f\"Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Mode: {TRAINING_CONFIG['mode']}\\n\")\n",
    "    f.write(f\"Epochs: {TRAINING_CONFIG['epochs']}\\n\")\n",
    "    f.write(f\"Batch Size: {TRAINING_CONFIG['batch_size']}\\n\")\n",
    "    f.write(f\"Data Directory: {DATA_DIR}\\n\")\n",
    "    f.write(f\"\\nModel Architecture: {TRAINING_CONFIG['model_name']}\\n\")\n",
    "    f.write(f\"Learning Rate: {TRAINING_CONFIG['learning_rate']}\\n\")\n",
    "    f.write(f\"Consistency Weight: {TRAINING_CONFIG['consistency_weight']}\\n\")\n",
    "    f.write(f\"\\nCheckpoints saved in: checkpoints/\\n\")\n",
    "    f.write(f\"Best model: checkpoints/model_best.pth\\n\")\n",
    "\n",
    "print(f\"✅ Training summary saved to: {summary_file}\")\n",
    "\n",
    "print(f\"\\n🎉 All results saved to Google Drive!\")\n",
    "print(f\"📁 Location: {save_dir}\")\n",
    "print(f\"\\n💡 You can now:\")\n",
    "print(f\"   1. Download the checkpoints folder for local use\")\n",
    "print(f\"   2. Use model_best.pth for inference\")\n",
    "print(f\"   3. Continue training from any checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc6396",
   "metadata": {},
   "source": [
    "## 🧪 Step 11: Quick Model Evaluation (Optional)\n",
    "\n",
    "Test your trained model on a few sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c1e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation of the trained model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if best model exists\n",
    "best_model_path = '/content/ViT-FishID/checkpoints/model_best.pth'\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(\"🧪 Loading trained model for quick evaluation...\")\n",
    "    \n",
    "    # Load model checkpoint info\n",
    "    checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "    \n",
    "    print(f\"📊 Model training info:\")\n",
    "    if 'epoch' in checkpoint:\n",
    "        print(f\"  - Best epoch: {checkpoint['epoch']}\")\n",
    "    if 'best_acc' in checkpoint:\n",
    "        print(f\"  - Best accuracy: {checkpoint['best_acc']:.2f}%\")\n",
    "    if 'teacher_acc' in checkpoint:\n",
    "        print(f\"  - Teacher accuracy: {checkpoint['teacher_acc']:.2f}%\")\n",
    "    \n",
    "    # Get class names if available\n",
    "    if 'class_names' in checkpoint:\n",
    "        class_names = checkpoint['class_names']\n",
    "        print(f\"  - Number of classes: {len(class_names)}\")\n",
    "        print(f\"  - Sample classes: {class_names[:5]}...\")\n",
    "    \n",
    "    print(\"\\n✅ Model evaluation completed! Check the metrics above.\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No trained model found. Make sure training completed successfully.\")\n",
    "\n",
    "print(\"\\n💡 For comprehensive evaluation:\")\n",
    "print(\"   Use the evaluate.py script with your test dataset\")\n",
    "print(\"   The test set was automatically created during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf6b4d",
   "metadata": {},
   "source": [
    "## 🔧 Troubleshooting\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "**1. GPU Memory Error (CUDA out of memory)**\n",
    "- Reduce batch_size to 8 or 4\n",
    "- Restart runtime and try again\n",
    "\n",
    "**2. Data Not Found**\n",
    "- Check that DATA_DIR path is correct\n",
    "- Ensure data is uploaded to Google Drive\n",
    "- Verify folder structure (labeled/ and unlabeled/)\n",
    "\n",
    "**3. Training Stops Unexpectedly**\n",
    "- Colab sessions timeout after 12 hours\n",
    "- Use runtime management to prevent disconnection\n",
    "- Checkpoints are saved every 10 epochs for resuming\n",
    "\n",
    "**4. Low Accuracy**\n",
    "- Increase epochs (try 75-100)\n",
    "- Adjust consistency_weight (try 1.0-3.0)\n",
    "- Lower pseudo_label_threshold (try 0.5-0.6)\n",
    "\n",
    "**5. Consistency Loss is 0.0000**\n",
    "- Lower pseudo_label_threshold to 0.5\n",
    "- Check that you have unlabeled data\n",
    "- Ensure semi_supervised mode is selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d21afb",
   "metadata": {},
   "source": [
    "## 🚀 Next Steps\n",
    "\n",
    "After training is complete, you can:\n",
    "\n",
    "1. **Download your model**: The trained model is saved in Google Drive\n",
    "2. **Continue training**: Resume from checkpoints for more epochs\n",
    "3. **Evaluate performance**: Use the test set for final evaluation\n",
    "4. **Deploy model**: Use the trained model for fish classification\n",
    "5. **Experiment**: Try different hyperparameters or architectures\n",
    "\n",
    "### Model Files Saved:\n",
    "- `model_best.pth`: Best performing model (use this for inference)\n",
    "- `model_latest.pth`: Most recent checkpoint\n",
    "- `model_epoch_XX.pth`: Periodic checkpoints\n",
    "\n",
    "### Performance Expectations:\n",
    "- **50 epochs**: ~70-80% accuracy\n",
    "- **100 epochs**: ~75-85% accuracy\n",
    "- **Semi-supervised**: Should outperform supervised training\n",
    "\n",
    "**Happy fish classification! 🐟🎉**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
