{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0af9a0",
   "metadata": {},
   "source": [
    "# üöÄ ViT-FishID: Resume Training from Epoch 19\n",
    "\n",
    "**COLAB PRO EXTENDED TRAINING**\n",
    "- Resume from: Epoch 19 checkpoint\n",
    "- Target epochs: 100 total epochs (81 remaining)\n",
    "- Expected training time: 6-8 hours with Colab Pro\n",
    "- GPU: Tesla T4/V100/A100 (depending on availability)\n",
    "\n",
    "This notebook will:\n",
    "1. ‚úÖ Resume training from your saved checkpoint at epoch 19\n",
    "2. ‚úÖ Train for 100 total epochs (81 more epochs)\n",
    "3. ‚úÖ Save checkpoints to Google Drive every 10 epochs\n",
    "4. ‚úÖ Use semi-supervised learning with your fish dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c6e33",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/cat-thomson/ViT-FishID/blob/main/ViT_FishID_Colab_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2f60df",
   "metadata": {},
   "source": [
    "# üêü ViT-FishID: Extended Training Session\n",
    "\n",
    "**RESUME FROM EPOCH 19 - COLAB PRO**\n",
    "\n",
    "This notebook resumes training from your saved checkpoint and runs for 100 total epochs.\n",
    "\n",
    "**Current Status:**\n",
    "- ‚úÖ Previous training: 19 epochs completed\n",
    "- üéØ Target: 100 total epochs (81 remaining)\n",
    "- ‚è±Ô∏è Expected time: 6-8 hours with Colab Pro\n",
    "- üíæ Auto-save every 10 epochs to Google Drive\n",
    "\n",
    "**Performance Target:**\n",
    "- Previous: ~78% validation accuracy at epoch 19\n",
    "- Expected: 85-90% accuracy after 100 epochs\n",
    "- Memory: ~8-12GB GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bcb2a3",
   "metadata": {},
   "source": [
    "## üöÄ Step 1: Setup and GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3540b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"üîç System Information:\")\n",
    "print(f\"Python version: {os.sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(\"‚úÖ GPU is ready for training!\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected. Please enable GPU runtime:\")\n",
    "    print(\"   Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149f671b",
   "metadata": {},
   "source": [
    "## üìÅ Step 2: Mount Google Drive\n",
    "\n",
    "This will give us access to your fish dataset stored in Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# List contents to verify mount\n",
    "print(\"\\nüìÇ Google Drive contents:\")\n",
    "drive_path = '/content/drive/MyDrive'\n",
    "if os.path.exists(drive_path):\n",
    "    items = os.listdir(drive_path)[:10]  # Show first 10 items\n",
    "    for item in items:\n",
    "        print(f\"  - {item}\")\n",
    "    if len(os.listdir(drive_path)) > 10:\n",
    "        print(f\"  ... and {len(os.listdir(drive_path)) - 10} more items\")\n",
    "    print(\"\\n‚úÖ Google Drive mounted successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to mount Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b6273",
   "metadata": {},
   "source": [
    "## üì¶ Step 3: Install Dependencies\n",
    "\n",
    "Installing all required packages for ViT-FishID training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c724abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q timm transformers\n",
    "!pip install -q albumentations\n",
    "!pip install -q wandb\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q matplotlib seaborn\n",
    "!pip install -q tqdm\n",
    "\n",
    "print(\"‚úÖ All dependencies installed successfully!\")\n",
    "\n",
    "# Verify installations\n",
    "import torch\n",
    "import torchvision\n",
    "import timm\n",
    "import albumentations\n",
    "import cv2\n",
    "import sklearn\n",
    "\n",
    "print(\"\\nüìã Package versions:\")\n",
    "print(f\"  - torch: {torch.__version__}\")\n",
    "print(f\"  - torchvision: {torchvision.__version__}\")\n",
    "print(f\"  - timm: {timm.__version__}\")\n",
    "print(f\"  - albumentations: {albumentations.__version__}\")\n",
    "print(f\"  - opencv: {cv2.__version__}\")\n",
    "print(f\"  - sklearn: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b795fc",
   "metadata": {},
   "source": [
    "## üîÑ Step 4: Clone ViT-FishID Repository\n",
    "\n",
    "Getting the latest code from your GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "import os\n",
    "\n",
    "# Remove existing directory if it exists\n",
    "if os.path.exists('/content/ViT-FishID'):\n",
    "    !rm -rf /content/ViT-FishID\n",
    "\n",
    "# Clone the repository\n",
    "print(\"üì• Cloning ViT-FishID repository...\")\n",
    "!git clone https://github.com/cat-thomson/ViT-FishID.git /content/ViT-FishID\n",
    "\n",
    "# Change to project directory\n",
    "%cd /content/ViT-FishID\n",
    "\n",
    "# List project files\n",
    "print(\"\\nüìÇ Project structure:\")\n",
    "!ls -la\n",
    "\n",
    "print(\"\\n‚úÖ Repository cloned successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155c400",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Step 5: Setup Data Path and Extraction\n",
    "\n",
    "**IMPORTANT:** Specify the path to your fish dataset ZIP file in Google Drive.\n",
    "\n",
    "This step will:\n",
    "1. Locate your `fish_cutouts.zip` file in Google Drive\n",
    "2. Extract it to Colab's local storage for faster access\n",
    "3. Validate the data structure\n",
    "\n",
    "Expected structure after extraction:\n",
    "```\n",
    "fish_cutouts/\n",
    "‚îú‚îÄ‚îÄ labeled/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ species_1/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fish_001.jpg\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fish_002.jpg\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ species_2/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ unlabeled/\n",
    "    ‚îú‚îÄ‚îÄ fish_003.jpg\n",
    "    ‚îî‚îÄ‚îÄ fish_004.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4008678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a136cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae46fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data path and extraction - PROPER ZIP HANDLING\n",
    "import zipfile\n",
    "import shutil\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"üóÇÔ∏è SETTING UP FISH DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Configuration - Update these paths as needed\n",
    "ZIP_FILE_PATH = '/content/drive/MyDrive/fish_cutouts.zip'  # Main location\n",
    "BACKUP_ZIP_PATH = '/content/drive/MyDrive/ViT-FishID/checkpoints/fish_cutouts.zip'  # Backup location\n",
    "DATA_DIR = '/content/fish_cutouts'\n",
    "\n",
    "print(f\"üéØ Target data directory: {DATA_DIR}\")\n",
    "\n",
    "# Check if data already exists locally (from previous session)\n",
    "if os.path.exists(DATA_DIR) and os.path.exists(os.path.join(DATA_DIR, 'labeled')):\n",
    "    print(\"‚úÖ Data already available locally from previous session!\")\n",
    "    \n",
    "    # Quick validation\n",
    "    labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
    "    unlabeled_dir = os.path.join(DATA_DIR, 'unlabeled')\n",
    "    \n",
    "    if os.path.exists(labeled_dir):\n",
    "        labeled_species = [d for d in os.listdir(labeled_dir) \n",
    "                          if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')]\n",
    "        print(f\"üêü Found {len(labeled_species)} labeled species\")\n",
    "        \n",
    "    if os.path.exists(unlabeled_dir):\n",
    "        unlabeled_files = [f for f in os.listdir(unlabeled_dir) \n",
    "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        print(f\"üìä Found {len(unlabeled_files)} unlabeled images\")\n",
    "        \n",
    "    print(\"‚úÖ Data validation passed - ready for training!\")\n",
    "\n",
    "else:\n",
    "    print(\"üì• Data not found locally, extracting from Google Drive...\")\n",
    "    \n",
    "    # Try to find the ZIP file\n",
    "    zip_file_to_use = None\n",
    "    if os.path.exists(ZIP_FILE_PATH):\n",
    "        zip_file_to_use = ZIP_FILE_PATH\n",
    "        print(f\"‚úÖ Found ZIP file at: {ZIP_FILE_PATH}\")\n",
    "    elif os.path.exists(BACKUP_ZIP_PATH):\n",
    "        zip_file_to_use = BACKUP_ZIP_PATH\n",
    "        print(f\"‚úÖ Found ZIP file at backup location: {BACKUP_ZIP_PATH}\")\n",
    "    else:\n",
    "        print(\"‚ùå ZIP file not found at either location!\")\n",
    "        print(f\"   Tried: {ZIP_FILE_PATH}\")\n",
    "        print(f\"   Tried: {BACKUP_ZIP_PATH}\")\n",
    "        print(\"üìù Please ensure fish_cutouts.zip is uploaded to Google Drive\")\n",
    "    \n",
    "    if zip_file_to_use:\n",
    "        print(f\"\\nüì¶ Extracting {os.path.basename(zip_file_to_use)}...\")\n",
    "        print(f\"üìè ZIP file size: {os.path.getsize(zip_file_to_use) / (1024**2):.1f} MB\")\n",
    "        \n",
    "        # Clean extraction\n",
    "        temp_extract_dir = '/content/temp_fish_extract'\n",
    "        if os.path.exists(temp_extract_dir):\n",
    "            shutil.rmtree(temp_extract_dir)\n",
    "        \n",
    "        try:\n",
    "            # Extract ZIP file\n",
    "            with zipfile.ZipFile(zip_file_to_use, 'r') as zip_ref:\n",
    "                zip_ref.extractall(temp_extract_dir)\n",
    "            \n",
    "            print(\"‚úÖ ZIP extraction completed\")\n",
    "            \n",
    "            # Handle nested folder structure: fish_cutouts.zip contains fish_cutouts/ folder\n",
    "            extracted_items = os.listdir(temp_extract_dir)\n",
    "            print(f\"üìÅ Found in ZIP: {extracted_items}\")\n",
    "            \n",
    "            # Look for the fish_cutouts folder inside the extraction\n",
    "            fish_cutouts_source = None\n",
    "            for item in extracted_items:\n",
    "                item_path = os.path.join(temp_extract_dir, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    # Check if this folder contains 'labeled' and 'unlabeled' subdirectories\n",
    "                    sub_items = os.listdir(item_path)\n",
    "                    if 'labeled' in sub_items or any('label' in sub.lower() for sub in sub_items):\n",
    "                        fish_cutouts_source = item_path\n",
    "                        print(f\"‚úÖ Found fish data in: {item}\")\n",
    "                        break\n",
    "            \n",
    "            # If we found the nested fish_cutouts folder, move it to the target location\n",
    "            if fish_cutouts_source:\n",
    "                # Remove existing target if it exists\n",
    "                if os.path.exists(DATA_DIR):\n",
    "                    shutil.rmtree(DATA_DIR)\n",
    "                \n",
    "                # Move the fish_cutouts folder to the correct location\n",
    "                shutil.move(fish_cutouts_source, DATA_DIR)\n",
    "                print(f\"‚úÖ Data moved to: {DATA_DIR}\")\n",
    "                \n",
    "                # Verify the structure\n",
    "                if os.path.exists(os.path.join(DATA_DIR, 'labeled')):\n",
    "                    labeled_species = [d for d in os.listdir(os.path.join(DATA_DIR, 'labeled')) \n",
    "                                     if os.path.isdir(os.path.join(DATA_DIR, 'labeled', d)) and not d.startswith('.')]\n",
    "                    print(f\"üêü Verified: {len(labeled_species)} species in labeled data\")\n",
    "                \n",
    "                if os.path.exists(os.path.join(DATA_DIR, 'unlabeled')):\n",
    "                    unlabeled_count = len([f for f in os.listdir(os.path.join(DATA_DIR, 'unlabeled'))\n",
    "                                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "                    print(f\"üìä Verified: {unlabeled_count} images in unlabeled data\")\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ùå Could not find fish_cutouts folder structure in ZIP\")\n",
    "                print(\"üìÅ Available items:\", extracted_items)\n",
    "            \n",
    "            # Cleanup temporary extraction\n",
    "            if os.path.exists(temp_extract_dir):\n",
    "                shutil.rmtree(temp_extract_dir)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during extraction: {e}\")\n",
    "            if os.path.exists(temp_extract_dir):\n",
    "                shutil.rmtree(temp_extract_dir)\n",
    "\n",
    "# Final verification\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(f\"\\n‚úÖ DATASET READY\")\n",
    "    print(f\"üìÅ Location: {DATA_DIR}\")\n",
    "    \n",
    "    # Show structure\n",
    "    for subdir in ['labeled', 'unlabeled']:\n",
    "        subdir_path = os.path.join(DATA_DIR, subdir)\n",
    "        if os.path.exists(subdir_path):\n",
    "            if subdir == 'labeled':\n",
    "                species_count = len([d for d in os.listdir(subdir_path) \n",
    "                                   if os.path.isdir(os.path.join(subdir_path, d)) and not d.startswith('.')])\n",
    "                print(f\"  üìÇ {subdir}/: {species_count} species folders\")\n",
    "            else:\n",
    "                file_count = len([f for f in os.listdir(subdir_path) \n",
    "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "                print(f\"  üìÇ {subdir}/: {file_count} images\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {subdir}/ not found\")\n",
    "    \n",
    "    print(\"\ude80 Ready to proceed with training!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå DATASET SETUP FAILED\")\n",
    "    print(f\"\udcdd Please check that fish_cutouts.zip contains the proper folder structure:\")\n",
    "    print(f\"   fish_cutouts.zip\")\n",
    "    print(f\"   ‚îî‚îÄ‚îÄ fish_cutouts/\")\n",
    "    print(f\"       ‚îú‚îÄ‚îÄ labeled/\")\n",
    "    print(f\"       ‚îÇ   ‚îú‚îÄ‚îÄ species1/\")\n",
    "    print(f\"       ‚îÇ   ‚îî‚îÄ‚îÄ species2/\")\n",
    "    print(f\"       ‚îî‚îÄ‚îÄ unlabeled/\")\n",
    "    print(f\"           ‚îú‚îÄ‚îÄ image1.jpg\")\n",
    "    print(f\"           ‚îî‚îÄ‚îÄ image2.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0fe32",
   "metadata": {},
   "source": [
    "## üìä Step 6: Setup Weights & Biases (Optional)\n",
    "\n",
    "W&B provides excellent training visualization and experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Weights & Biases for experiment tracking\n",
    "import wandb\n",
    "\n",
    "# Login to W&B (you'll need to create a free account at wandb.ai)\n",
    "print(\"üîê Setting up Weights & Biases...\")\n",
    "print(\"\\nTo use W&B:\")\n",
    "print(\"1. Go to https://wandb.ai and create a free account\")\n",
    "print(\"2. Get your API key from https://wandb.ai/authorize\")\n",
    "print(\"3. Run the cell below and paste your API key when prompted\")\n",
    "print(\"\\nOr skip W&B by setting USE_WANDB = False below\")\n",
    "\n",
    "# Set this to True if you want to use W&B, False to skip\n",
    "USE_WANDB = True  # üëà Set to False if you don't want to use W&B\n",
    "\n",
    "if USE_WANDB:\n",
    "    try:\n",
    "        # Try to login (will prompt for API key if not already logged in)\n",
    "        wandb.login()\n",
    "        print(\"‚úÖ W&B login successful!\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  W&B login failed. Training will continue without W&B logging.\")\n",
    "        USE_WANDB = False\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Skipping W&B setup. Training will run without experiment tracking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5190f01",
   "metadata": {},
   "source": [
    "## üîÑ Step 6: Locate Checkpoint from Epoch 19\n",
    "\n",
    "Finding your saved checkpoint to resume training from where you left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b35ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate checkpoint from epoch 19\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "print(\"üîç Looking for checkpoint from epoch 19...\")\n",
    "\n",
    "# Possible checkpoint locations\n",
    "checkpoint_locations = [\n",
    "    '/content/drive/MyDrive/ViT-FishID/checkpoints',\n",
    "    '/content/drive/MyDrive/ViT-FishID_Training_*/checkpoints',\n",
    "    '/content/drive/MyDrive/checkpoints',\n",
    "    '/content/ViT-FishID/checkpoints'\n",
    "]\n",
    "\n",
    "checkpoint_path = None\n",
    "checkpoint_info = None\n",
    "\n",
    "# Search for epoch 19 checkpoint\n",
    "for location_pattern in checkpoint_locations:\n",
    "    for location in glob.glob(location_pattern):\n",
    "        if os.path.exists(location):\n",
    "            print(f\"üìÅ Checking: {location}\")\n",
    "            \n",
    "            # Look for epoch 19 specifically\n",
    "            epoch_19_files = glob.glob(os.path.join(location, '*epoch_19*'))\n",
    "            manual_files = glob.glob(os.path.join(location, '*manual*epoch*19*'))\n",
    "            emergency_files = glob.glob(os.path.join(location, '*emergency*epoch*19*'))\n",
    "            \n",
    "            all_candidates = epoch_19_files + manual_files + emergency_files\n",
    "            \n",
    "            for candidate in all_candidates:\n",
    "                if candidate.endswith('.pth'):\n",
    "                    print(f\"üéØ Found candidate: {os.path.basename(candidate)}\")\n",
    "                    try:\n",
    "                        # Verify checkpoint can be loaded\n",
    "                        test_checkpoint = torch.load(candidate, map_location='cpu')\n",
    "                        epoch = test_checkpoint.get('epoch', 'unknown')\n",
    "                        \n",
    "                        if epoch == 19 or '19' in os.path.basename(candidate):\n",
    "                            checkpoint_path = candidate\n",
    "                            checkpoint_info = test_checkpoint\n",
    "                            print(f\"‚úÖ FOUND EPOCH 19 CHECKPOINT!\")\n",
    "                            print(f\"üìÅ Location: {checkpoint_path}\")\n",
    "                            print(f\"üìä Epoch: {epoch}\")\n",
    "                            \n",
    "                            if 'best_accuracy' in test_checkpoint:\n",
    "                                print(f\"üìä Best accuracy so far: {test_checkpoint['best_accuracy']:.2f}%\")\n",
    "                            elif 'best_acc' in test_checkpoint:\n",
    "                                print(f\"üìä Best accuracy so far: {test_checkpoint['best_acc']:.2f}%\")\n",
    "                                \n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Could not load {candidate}: {e}\")\n",
    "            \n",
    "            if checkpoint_path:\n",
    "                break\n",
    "        \n",
    "        if checkpoint_path:\n",
    "            break\n",
    "\n",
    "if checkpoint_path:\n",
    "    print(f\"\\nüéâ Checkpoint ready for resuming training!\")\n",
    "    print(f\"üìÑ File: {os.path.basename(checkpoint_path)}\")\n",
    "    print(f\"üìè Size: {os.path.getsize(checkpoint_path) / (1024*1024):.1f} MB\")\n",
    "    \n",
    "    # Set up checkpoint directory for new saves\n",
    "    checkpoint_save_dir = '/content/drive/MyDrive/ViT-FishID/checkpoints_extended'\n",
    "    os.makedirs(checkpoint_save_dir, exist_ok=True)\n",
    "    print(f\"üíæ New checkpoints will be saved to: {checkpoint_save_dir}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No checkpoint found for epoch 19!\")\n",
    "    print(\"\\nüîß Troubleshooting:\")\n",
    "    print(\"1. Check that you have a checkpoint saved from previous training\")\n",
    "    print(\"2. Ensure the checkpoint is uploaded to Google Drive\")\n",
    "    print(\"3. Look for files named like: checkpoint_epoch_19.pth, emergency_checkpoint_epoch_19.pth\")\n",
    "    print(\"\\nüìÅ Checked locations:\")\n",
    "    for location in checkpoint_locations:\n",
    "        print(f\"  - {location}\")\n",
    "    \n",
    "    # Fallback: look for any checkpoints\n",
    "    print(\"\\nüîç All available checkpoints:\")\n",
    "    for location_pattern in checkpoint_locations:\n",
    "        for location in glob.glob(location_pattern):\n",
    "            if os.path.exists(location):\n",
    "                all_checkpoints = glob.glob(os.path.join(location, '*.pth'))\n",
    "                for cp in all_checkpoints:\n",
    "                    print(f\"  - {os.path.basename(cp)}\")\n",
    "\n",
    "# Store checkpoint path for later use\n",
    "RESUME_CHECKPOINT = checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup: Copy local checkpoint to Google Drive if not found there\n",
    "import shutil\n",
    "\n",
    "if not checkpoint_path:\n",
    "    print(\"üîç Checkpoint not found in Google Drive, checking local copy...\")\n",
    "    \n",
    "    # Check if there's a local checkpoint file we uploaded\n",
    "    local_checkpoint = '/content/ViT-FishID/checkpoint_epoch_19.pth'\n",
    "    \n",
    "    if os.path.exists(local_checkpoint):\n",
    "        print(\"‚úÖ Found local checkpoint file!\")\n",
    "        \n",
    "        # Copy to Google Drive\n",
    "        drive_backup_dir = '/content/drive/MyDrive/ViT-FishID/checkpoints'\n",
    "        os.makedirs(drive_backup_dir, exist_ok=True)\n",
    "        \n",
    "        drive_checkpoint_path = os.path.join(drive_backup_dir, 'checkpoint_epoch_19.pth')\n",
    "        shutil.copy2(local_checkpoint, drive_checkpoint_path)\n",
    "        \n",
    "        print(f\"üíæ Copied checkpoint to Google Drive: {drive_checkpoint_path}\")\n",
    "        \n",
    "        # Verify the copied checkpoint\n",
    "        try:\n",
    "            test_checkpoint = torch.load(drive_checkpoint_path, map_location='cpu')\n",
    "            epoch = test_checkpoint.get('epoch', 'unknown')\n",
    "            if 'best_accuracy' in test_checkpoint:\n",
    "                accuracy = test_checkpoint['best_accuracy']\n",
    "                print(f\"‚úÖ Verification passed - Epoch {epoch}, Accuracy: {accuracy:.2f}%\")\n",
    "            \n",
    "            # Update our variables\n",
    "            checkpoint_path = drive_checkpoint_path\n",
    "            checkpoint_info = test_checkpoint\n",
    "            RESUME_CHECKPOINT = checkpoint_path\n",
    "            \n",
    "            print(\"üéâ Checkpoint ready for resuming training!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error verifying copied checkpoint: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå No local checkpoint found either\")\n",
    "        print(\"üìù Please ensure you have the checkpoint_epoch_19.pth file\")\n",
    "        print(\"   You can upload it to Colab or place it in Google Drive\")\n",
    "\n",
    "# Final check\n",
    "if checkpoint_path:\n",
    "    print(f\"\\n‚úÖ FINAL CHECKPOINT STATUS:\")\n",
    "    print(f\"üìÅ Using checkpoint: {checkpoint_path}\")\n",
    "    print(f\"üìä Ready to resume from epoch 19\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå NO CHECKPOINT AVAILABLE\")\n",
    "    print(f\"üîÑ Training will start from epoch 1 instead\")\n",
    "    print(f\"‚ö†Ô∏è  This will take much longer than resuming!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe6af6d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 7: Configure Training Parameters\n",
    "\n",
    "Adjust these parameters based on your needs and available GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e959fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration - RESUME FROM EPOCH 19 FOR 100 TOTAL EPOCHS\n",
    "TRAINING_CONFIG = {\n",
    "    # RESUME SETTINGS\n",
    "    'resume_from_checkpoint': RESUME_CHECKPOINT,\n",
    "    'start_epoch': 20,  # Next epoch after 19\n",
    "    'total_epochs': 100,  # Target total epochs\n",
    "    'remaining_epochs': 81,  # 100 - 19 = 81 epochs left\n",
    "    \n",
    "    # CORE SETTINGS\n",
    "    'mode': 'semi_supervised',  # semi_supervised or supervised\n",
    "    'data_dir': DATA_DIR,\n",
    "    'batch_size': 16,  # Increased for Colab Pro\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.05,\n",
    "    \n",
    "    # MODEL SETTINGS\n",
    "    'model_name': 'vit_base_patch16_224',\n",
    "    'num_classes': 37,  # Will be auto-detected\n",
    "    'pretrained': True,\n",
    "    \n",
    "    # SEMI-SUPERVISED SETTINGS - FIXED CONSISTENCY LOSS\n",
    "    'consistency_weight': 2.0,\n",
    "    'pseudo_label_threshold': 0.7,\n",
    "    'temperature': 4.0,\n",
    "    'warmup_epochs': 5,  # Reduced since we're resuming\n",
    "    'ramp_up_epochs': 15,  # Reduced since we're resuming\n",
    "    \n",
    "    # CHECKPOINT SETTINGS - SAVE EVERY EPOCH\n",
    "    'save_frequency': 1,  # Save EVERY epoch (changed from 10)\n",
    "    'checkpoint_dir': '/content/drive/MyDrive/ViT-FishID/checkpoints_extended',\n",
    "    'backup_dir': '/content/drive/MyDrive/ViT-FishID/checkpoints_backup',\n",
    "    \n",
    "    # LOGGING\n",
    "    'use_wandb': True,\n",
    "    'wandb_project': 'vit-fishid-extended',\n",
    "    'wandb_run_name': f'resume_epoch19_to_100_fixed'\n",
    "}\n",
    "\n",
    "print(\"üéØ EXTENDED TRAINING CONFIGURATION - FIXED VERSION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üìä Resume from: Epoch {TRAINING_CONFIG['start_epoch'] - 1}\")\n",
    "print(f\"üìä Target epochs: {TRAINING_CONFIG['total_epochs']}\")\n",
    "print(f\"üìä Remaining epochs: {TRAINING_CONFIG['remaining_epochs']}\")\n",
    "print(f\"üìä Expected time: {TRAINING_CONFIG['remaining_epochs'] * 4:.0f}-{TRAINING_CONFIG['remaining_epochs'] * 6:.0f} minutes\")\n",
    "print(f\"üìä Batch size: {TRAINING_CONFIG['batch_size']} (optimized for Colab Pro)\")\n",
    "print(f\"\udcbe Checkpoint saves: EVERY epoch (enhanced backup)\")\n",
    "print(f\"üìä Mode: {TRAINING_CONFIG['mode']} with FIXED consistency loss\")\n",
    "\n",
    "# Create checkpoint directories\n",
    "os.makedirs(TRAINING_CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "os.makedirs(TRAINING_CONFIG['backup_dir'], exist_ok=True)\n",
    "print(f\"üìÅ Primary saves: {TRAINING_CONFIG['checkpoint_dir']}\")\n",
    "print(f\"\udcc1 Backup saves: {TRAINING_CONFIG['backup_dir']}\")\n",
    "\n",
    "if TRAINING_CONFIG['resume_from_checkpoint']:\n",
    "    print(f\"‚úÖ Will resume from: {os.path.basename(TRAINING_CONFIG['resume_from_checkpoint'])}\")\n",
    "else:\n",
    "    print(\"‚ùå No checkpoint found - will start fresh training\")\n",
    "    TRAINING_CONFIG['start_epoch'] = 1\n",
    "    TRAINING_CONFIG['remaining_epochs'] = TRAINING_CONFIG['total_epochs']\n",
    "\n",
    "print(f\"\\nüîß FIXES APPLIED:\")\n",
    "print(f\"  ‚úÖ Consistency loss: Fixed tensor initialization\")\n",
    "print(f\"  ‚úÖ Checkpoint saving: Every epoch + Google Drive backup\")\n",
    "print(f\"  ‚úÖ Error handling: Enhanced for robustness\")\n",
    "print(f\"  ‚úÖ State dict keys: Fixed ema_teacher key naming\")\n",
    "\n",
    "print(\"\\nüí° With Colab Pro, this training should complete without timeout!\")\n",
    "print(\"üíæ Every epoch will be saved with Google Drive backup every 5 epochs\")\n",
    "\n",
    "# Verify data directory\n",
    "if os.path.exists(TRAINING_CONFIG['data_dir']):\n",
    "    labeled_dir = os.path.join(TRAINING_CONFIG['data_dir'], 'labeled')\n",
    "    if os.path.exists(labeled_dir):\n",
    "        species_count = len([d for d in os.listdir(labeled_dir) \n",
    "                           if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')])\n",
    "        TRAINING_CONFIG['num_classes'] = species_count\n",
    "        print(f\"üìä Detected {species_count} fish species\")\n",
    "    \n",
    "print(f\"\\nüöÄ Ready to resume training for {TRAINING_CONFIG['remaining_epochs']} more epochs!\")\n",
    "print(f\"üîß All consistency loss and checkpoint issues have been resolved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9cbd50",
   "metadata": {},
   "source": [
    "## üöÄ Step 8: Start Training!\n",
    "\n",
    "This cell will start the semi-supervised training process. It may take 2-3 hours to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace87e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Extended Training - Resume from Epoch 19\n",
    "import os\n",
    "\n",
    "print(\"üöÄ STARTING EXTENDED TRAINING SESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create checkpoint save directory\n",
    "os.makedirs(TRAINING_CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "# Build training command for resuming\n",
    "training_cmd = f\"\"\"python train.py \\\\\n",
    "    --mode {TRAINING_CONFIG['mode']} \\\\\n",
    "    --data_dir {TRAINING_CONFIG['data_dir']} \\\\\n",
    "    --epochs {TRAINING_CONFIG['total_epochs']} \\\\\n",
    "    --batch_size {TRAINING_CONFIG['batch_size']} \\\\\n",
    "    --learning_rate {TRAINING_CONFIG['learning_rate']} \\\\\n",
    "    --weight_decay {TRAINING_CONFIG['weight_decay']} \\\\\n",
    "    --model_name {TRAINING_CONFIG['model_name']} \\\\\n",
    "    --num_classes {TRAINING_CONFIG['num_classes']} \\\\\n",
    "    --consistency_weight {TRAINING_CONFIG['consistency_weight']} \\\\\n",
    "    --pseudo_label_threshold {TRAINING_CONFIG['pseudo_label_threshold']} \\\\\n",
    "    --temperature {TRAINING_CONFIG['temperature']} \\\\\n",
    "    --warmup_epochs {TRAINING_CONFIG['warmup_epochs']} \\\\\n",
    "    --ramp_up_epochs {TRAINING_CONFIG['ramp_up_epochs']} \\\\\n",
    "    --save_dir {TRAINING_CONFIG['checkpoint_dir']} \\\\\n",
    "    --save_frequency {TRAINING_CONFIG['save_frequency']}\"\"\"\n",
    "\n",
    "# Add resume checkpoint if available\n",
    "if TRAINING_CONFIG['resume_from_checkpoint']:\n",
    "    training_cmd += f\" \\\\\\n    --resume_from {TRAINING_CONFIG['resume_from_checkpoint']}\"\n",
    "    print(f\"üìÇ Resuming from: {os.path.basename(TRAINING_CONFIG['resume_from_checkpoint'])}\")\n",
    "\n",
    "# Add W&B logging\n",
    "if TRAINING_CONFIG['use_wandb']:\n",
    "    training_cmd += f\" \\\\\\n    --use_wandb --wandb_project {TRAINING_CONFIG['wandb_project']} --wandb_run_name {TRAINING_CONFIG['wandb_run_name']}\"\n",
    "\n",
    "# Add pretrained flag\n",
    "if TRAINING_CONFIG['pretrained']:\n",
    "    training_cmd += \" \\\\\\n    --pretrained\"\n",
    "\n",
    "print(f\"üìä Training for {TRAINING_CONFIG['remaining_epochs']} more epochs...\")\n",
    "print(f\"üéØ Target: {TRAINING_CONFIG['total_epochs']} total epochs\")\n",
    "print(f\"‚è±Ô∏è Estimated time: {TRAINING_CONFIG['remaining_epochs'] * 4:.0f}-{TRAINING_CONFIG['remaining_epochs'] * 6:.0f} minutes\")\n",
    "print(f\"üíæ Checkpoints saved to: {TRAINING_CONFIG['checkpoint_dir']}\")\n",
    "\n",
    "print(\"\\nüìã Extended Training Command:\")\n",
    "print(training_cmd.replace('\\\\', '').strip())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Execute training\n",
    "print(\"üé¨ TRAINING STARTED - EPOCH 20 TO 100\")\n",
    "print(\"‚è∞ Started at:\", __import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "!{training_cmd}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ EXTENDED TRAINING COMPLETED!\")\n",
    "print(\"‚è∞ Finished at:\", __import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print(f\"üèÜ Total epochs completed: {TRAINING_CONFIG['total_epochs']}\")\n",
    "print(f\"üíæ All checkpoints saved to Google Drive\")\n",
    "\n",
    "# Quick summary of final results\n",
    "final_checkpoint = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'model_best.pth')\n",
    "if os.path.exists(final_checkpoint):\n",
    "    try:\n",
    "        import torch\n",
    "        final_results = torch.load(final_checkpoint, map_location='cpu')\n",
    "        if 'best_accuracy' in final_results:\n",
    "            print(f\"üéØ Final best accuracy: {final_results['best_accuracy']:.2f}%\")\n",
    "        if 'epoch' in final_results:\n",
    "            print(f\"üìä Best model from epoch: {final_results['epoch']}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\n‚úÖ Your model is ready for evaluation and deployment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781620bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Pre-Training Verification - Test Fixes\n",
    "print(\"üîß TESTING FIXES BEFORE TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test 1: Verify checkpoint loading\n",
    "if TRAINING_CONFIG['resume_from_checkpoint']:\n",
    "    print(\"‚úÖ Test 1: Checkpoint Loading\")\n",
    "    try:\n",
    "        test_checkpoint = torch.load(TRAINING_CONFIG['resume_from_checkpoint'], map_location='cpu')\n",
    "        print(f\"   üìä Checkpoint epoch: {test_checkpoint.get('epoch', 'Unknown')}\")\n",
    "        print(f\"   üìä Best accuracy: {test_checkpoint.get('best_accuracy', 'Unknown')}\")\n",
    "        print(f\"   üîë Available keys: {list(test_checkpoint.keys())}\")\n",
    "        \n",
    "        # Check for correct key names\n",
    "        required_keys = ['student_state_dict', 'epoch']\n",
    "        missing_keys = [key for key in required_keys if key not in test_checkpoint]\n",
    "        if missing_keys:\n",
    "            print(f\"   ‚ö†Ô∏è Missing keys: {missing_keys}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ All required keys present\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Checkpoint test failed: {e}\")\n",
    "\n",
    "# Test 2: Verify consistency loss function\n",
    "print(f\"\\n‚úÖ Test 2: Consistency Loss Function\")\n",
    "try:\n",
    "    from model import ConsistencyLoss\n",
    "    consistency_loss_fn = ConsistencyLoss(temperature=4.0)\n",
    "    \n",
    "    # Create dummy tensors\n",
    "    dummy_student = torch.randn(4, 37)  # batch_size=4, num_classes=37\n",
    "    dummy_teacher = torch.randn(4, 37)\n",
    "    \n",
    "    loss = consistency_loss_fn(dummy_student, dummy_teacher)\n",
    "    print(f\"   üìä Test loss value: {loss.item():.6f}\")\n",
    "    print(f\"   üìä Loss tensor shape: {loss.shape}\")\n",
    "    print(f\"   üìä Loss requires grad: {loss.requires_grad}\")\n",
    "    \n",
    "    if loss.requires_grad and loss.item() > 0:\n",
    "        print(f\"   ‚úÖ Consistency loss function working correctly\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Potential issue with consistency loss\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Consistency loss test failed: {e}\")\n",
    "\n",
    "# Test 3: Verify Google Drive paths\n",
    "print(f\"\\n‚úÖ Test 3: Google Drive Paths\")\n",
    "for path_name, path in [\n",
    "    (\"Primary checkpoint dir\", TRAINING_CONFIG['checkpoint_dir']),\n",
    "    (\"Backup checkpoint dir\", TRAINING_CONFIG['backup_dir'])\n",
    "]:\n",
    "    try:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        test_file = os.path.join(path, 'test_write.txt')\n",
    "        with open(test_file, 'w') as f:\n",
    "            f.write('test')\n",
    "        os.remove(test_file)\n",
    "        print(f\"   ‚úÖ {path_name}: {path} (writable)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå {path_name}: {path} (error: {e})\")\n",
    "\n",
    "# Test 4: Verify training configuration\n",
    "print(f\"\\n‚úÖ Test 4: Training Configuration\")\n",
    "config_checks = [\n",
    "    (\"Save frequency\", TRAINING_CONFIG['save_frequency'] == 1, \"Every epoch\"),\n",
    "    (\"Consistency weight\", TRAINING_CONFIG['consistency_weight'] > 0, f\"{TRAINING_CONFIG['consistency_weight']}\"),\n",
    "    (\"Batch size\", TRAINING_CONFIG['batch_size'] >= 8, f\"{TRAINING_CONFIG['batch_size']}\"),\n",
    "    (\"Learning rate\", 0 < TRAINING_CONFIG['learning_rate'] < 1, f\"{TRAINING_CONFIG['learning_rate']}\"),\n",
    "]\n",
    "\n",
    "for check_name, condition, value in config_checks:\n",
    "    status = \"‚úÖ\" if condition else \"‚ùå\"\n",
    "    print(f\"   {status} {check_name}: {value}\")\n",
    "\n",
    "print(f\"\\nüéØ PRE-TRAINING VERIFICATION COMPLETE\")\n",
    "print(f\"üìä Configuration looks {'‚úÖ GOOD' if all([check[1] for check in config_checks]) else '‚ö†Ô∏è NEEDS ATTENTION'}\")\n",
    "print(f\"üöÄ Ready to start training with all fixes applied!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af5177",
   "metadata": {},
   "source": [
    "## üìä Step 9: Check Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea96e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Extended Training Results (Epoch 19 ‚Üí 100)\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "checkpoint_dir = TRAINING_CONFIG['checkpoint_dir']\n",
    "print(f\"üìÅ Checking results in: {checkpoint_dir}\")\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = glob.glob(os.path.join(checkpoint_dir, '*.pth'))\n",
    "    if checkpoints:\n",
    "        print(f\"\\n‚úÖ Found {len(checkpoints)} checkpoint(s) from extended training:\")\n",
    "        \n",
    "        # Sort checkpoints by epoch number\n",
    "        epoch_checkpoints = []\n",
    "        other_checkpoints = []\n",
    "        \n",
    "        for cp in checkpoints:\n",
    "            basename = os.path.basename(cp)\n",
    "            if 'epoch_' in basename:\n",
    "                try:\n",
    "                    epoch_num = int(basename.split('epoch_')[1].split('.')[0])\n",
    "                    epoch_checkpoints.append((epoch_num, cp))\n",
    "                except:\n",
    "                    other_checkpoints.append(cp)\n",
    "            else:\n",
    "                other_checkpoints.append(cp)\n",
    "        \n",
    "        # Show epoch checkpoints in order\n",
    "        epoch_checkpoints.sort(key=lambda x: x[0])\n",
    "        for epoch, cp in epoch_checkpoints:\n",
    "            file_size = os.path.getsize(cp) / (1024**2)\n",
    "            print(f\"  üìä Epoch {epoch}: {os.path.basename(cp)} ({file_size:.1f} MB)\")\n",
    "        \n",
    "        # Show other checkpoints\n",
    "        for cp in other_checkpoints:\n",
    "            file_size = os.path.getsize(cp) / (1024**2)\n",
    "            print(f\"  üèÜ {os.path.basename(cp)} ({file_size:.1f} MB)\")\n",
    "        \n",
    "        # Analyze best model\n",
    "        best_model = os.path.join(checkpoint_dir, 'model_best.pth')\n",
    "        if os.path.exists(best_model):\n",
    "            print(f\"\\nüèÜ BEST MODEL ANALYSIS:\")\n",
    "            try:\n",
    "                best_checkpoint = torch.load(best_model, map_location='cpu')\n",
    "                \n",
    "                best_epoch = best_checkpoint.get('epoch', 'Unknown')\n",
    "                best_acc = best_checkpoint.get('best_accuracy', best_checkpoint.get('best_acc', 'Unknown'))\n",
    "                \n",
    "                print(f\"  üìä Best epoch: {best_epoch}\")\n",
    "                print(f\"  üìä Best accuracy: {best_acc:.2f}%\" if isinstance(best_acc, (int, float)) else f\"  üìä Best accuracy: {best_acc}\")\n",
    "                \n",
    "                # Show training progression\n",
    "                if epoch_checkpoints:\n",
    "                    print(f\"\\nüìà TRAINING PROGRESSION:\")\n",
    "                    print(f\"  üèÅ Started: Epoch 19 (resumed)\")\n",
    "                    print(f\"  üéØ Completed: Epoch {max(epoch_checkpoints, key=lambda x: x[0])[0]}\")\n",
    "                    print(f\"  üèÜ Best: Epoch {best_epoch}\")\n",
    "                    print(f\"  üìä Total training: {19 + len([e for e, _ in epoch_checkpoints if e > 19])} epochs\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Could not analyze best model: {e}\")\n",
    "        \n",
    "        # Training duration estimate\n",
    "        if epoch_checkpoints:\n",
    "            epochs_completed = len([e for e, _ in epoch_checkpoints if e > 19])\n",
    "            print(f\"\\n‚è±Ô∏è EXTENDED TRAINING SUMMARY:\")\n",
    "            print(f\"  üìä Additional epochs completed: {epochs_completed}\")\n",
    "            print(f\"  üéØ Target was: 81 additional epochs (to reach 100 total)\")\n",
    "            \n",
    "            if epochs_completed >= 81:\n",
    "                print(f\"  ‚úÖ TRAINING GOAL ACHIEVED! Completed all {epochs_completed} additional epochs\")\n",
    "            else:\n",
    "                print(f\"  ‚è≥ Training partially complete: {epochs_completed}/81 additional epochs\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå No checkpoints found in extended training directory\")\n",
    "        \n",
    "        # Check if training is still using old directory\n",
    "        old_checkpoint_dir = '/content/ViT-FishID/checkpoints'\n",
    "        if os.path.exists(old_checkpoint_dir):\n",
    "            old_checkpoints = glob.glob(os.path.join(old_checkpoint_dir, '*.pth'))\n",
    "            if old_checkpoints:\n",
    "                print(f\"\\nüí° Found {len(old_checkpoints)} checkpoints in old directory:\")\n",
    "                print(f\"   {old_checkpoint_dir}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Extended training checkpoint directory not found\")\n",
    "\n",
    "# W&B link\n",
    "if TRAINING_CONFIG['use_wandb']:\n",
    "    print(f\"\\nüìà View detailed training metrics:\")\n",
    "    print(f\"   https://wandb.ai/your-username/{TRAINING_CONFIG['wandb_project']}\")\n",
    "    print(f\"   Run: {TRAINING_CONFIG['wandb_run_name']}\")\n",
    "\n",
    "print(f\"\\nüéâ Extended training session complete!\")\n",
    "print(f\"üöÄ Your model trained from epoch 19 to 100!\")\n",
    "print(f\"üíæ All results saved to Google Drive: {checkpoint_dir}\")\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\nüìä PERFORMANCE COMPARISON:\")\n",
    "print(f\"  üîÑ Previous (Epoch 19): ~78% accuracy\")\n",
    "print(f\"  üéØ Extended (Epoch 100): Check best_accuracy above\")\n",
    "print(f\"  üìà Expected improvement: 5-10% accuracy gain\")\n",
    "print(f\"  üèÜ Your model should now be ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff698a72",
   "metadata": {},
   "source": [
    "## üíæ Step 10: Download Model and Results\n",
    "\n",
    "Save your trained model to Google Drive for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89513455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy trained model to Google Drive\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a timestamped folder in Google Drive\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_dir = f'/content/drive/MyDrive/ViT-FishID_Training_{timestamp}'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üíæ Saving results to Google Drive: {save_dir}\")\n",
    "\n",
    "# Copy checkpoints\n",
    "checkpoint_dir = '/content/ViT-FishID/checkpoints'\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    drive_checkpoint_dir = os.path.join(save_dir, 'checkpoints')\n",
    "    shutil.copytree(checkpoint_dir, drive_checkpoint_dir)\n",
    "    print(f\"‚úÖ Checkpoints saved to: {drive_checkpoint_dir}\")\n",
    "\n",
    "# Save training configuration\n",
    "import json\n",
    "config_file = os.path.join(save_dir, 'training_config.json')\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(TRAINING_CONFIG, f, indent=2)\n",
    "print(f\"‚úÖ Training config saved to: {config_file}\")\n",
    "\n",
    "# Create a summary file\n",
    "summary_file = os.path.join(save_dir, 'training_summary.txt')\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(f\"ViT-FishID Training Summary\\n\")\n",
    "    f.write(f\"========================\\n\\n\")\n",
    "    f.write(f\"Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Mode: {TRAINING_CONFIG['mode']}\\n\")\n",
    "    f.write(f\"Epochs: {TRAINING_CONFIG['epochs']}\\n\")\n",
    "    f.write(f\"Batch Size: {TRAINING_CONFIG['batch_size']}\\n\")\n",
    "    f.write(f\"Data Directory: {DATA_DIR}\\n\")\n",
    "    f.write(f\"\\nModel Architecture: {TRAINING_CONFIG['model_name']}\\n\")\n",
    "    f.write(f\"Learning Rate: {TRAINING_CONFIG['learning_rate']}\\n\")\n",
    "    f.write(f\"Consistency Weight: {TRAINING_CONFIG['consistency_weight']}\\n\")\n",
    "    f.write(f\"\\nCheckpoints saved in: checkpoints/\\n\")\n",
    "    f.write(f\"Best model: checkpoints/model_best.pth\\n\")\n",
    "\n",
    "print(f\"‚úÖ Training summary saved to: {summary_file}\")\n",
    "\n",
    "print(f\"\\nüéâ All results saved to Google Drive!\")\n",
    "print(f\"üìÅ Location: {save_dir}\")\n",
    "print(f\"\\nüí° You can now:\")\n",
    "print(f\"   1. Download the checkpoints folder for local use\")\n",
    "print(f\"   2. Use model_best.pth for inference\")\n",
    "print(f\"   3. Continue training from any checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc6396",
   "metadata": {},
   "source": [
    "## üß™ Step 11: Quick Model Evaluation (Optional)\n",
    "\n",
    "Test your trained model on a few sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c1e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation of the trained model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if best model exists\n",
    "best_model_path = '/content/ViT-FishID/checkpoints/model_best.pth'\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(\"üß™ Loading trained model for quick evaluation...\")\n",
    "    \n",
    "    # Load model checkpoint info\n",
    "    checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "    \n",
    "    print(f\"üìä Model training info:\")\n",
    "    if 'epoch' in checkpoint:\n",
    "        print(f\"  - Best epoch: {checkpoint['epoch']}\")\n",
    "    if 'best_acc' in checkpoint:\n",
    "        print(f\"  - Best accuracy: {checkpoint['best_acc']:.2f}%\")\n",
    "    if 'teacher_acc' in checkpoint:\n",
    "        print(f\"  - Teacher accuracy: {checkpoint['teacher_acc']:.2f}%\")\n",
    "    \n",
    "    # Get class names if available\n",
    "    if 'class_names' in checkpoint:\n",
    "        class_names = checkpoint['class_names']\n",
    "        print(f\"  - Number of classes: {len(class_names)}\")\n",
    "        print(f\"  - Sample classes: {class_names[:5]}...\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Model evaluation completed! Check the metrics above.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No trained model found. Make sure training completed successfully.\")\n",
    "\n",
    "print(\"\\nüí° For comprehensive evaluation:\")\n",
    "print(\"   Use the evaluate.py script with your test dataset\")\n",
    "print(\"   The test set was automatically created during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf6b4d",
   "metadata": {},
   "source": [
    "## üîß Troubleshooting\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "**1. GPU Memory Error (CUDA out of memory)**\n",
    "- Reduce batch_size to 8 or 4\n",
    "- Restart runtime and try again\n",
    "\n",
    "**2. Data Not Found**\n",
    "- Check that DATA_DIR path is correct\n",
    "- Ensure data is uploaded to Google Drive\n",
    "- Verify folder structure (labeled/ and unlabeled/)\n",
    "\n",
    "**3. Training Stops Unexpectedly**\n",
    "- Colab sessions timeout after 12 hours\n",
    "- Use runtime management to prevent disconnection\n",
    "- Checkpoints are saved every 10 epochs for resuming\n",
    "\n",
    "**4. Low Accuracy**\n",
    "- Increase epochs (try 75-100)\n",
    "- Adjust consistency_weight (try 1.0-3.0)\n",
    "- Lower pseudo_label_threshold (try 0.5-0.6)\n",
    "\n",
    "**5. Consistency Loss is 0.0000**\n",
    "- Lower pseudo_label_threshold to 0.5\n",
    "- Check that you have unlabeled data\n",
    "- Ensure semi_supervised mode is selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d21afb",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "After training is complete, you can:\n",
    "\n",
    "1. **Download your model**: The trained model is saved in Google Drive\n",
    "2. **Continue training**: Resume from checkpoints for more epochs\n",
    "3. **Evaluate performance**: Use the test set for final evaluation\n",
    "4. **Deploy model**: Use the trained model for fish classification\n",
    "5. **Experiment**: Try different hyperparameters or architectures\n",
    "\n",
    "### Model Files Saved:\n",
    "- `model_best.pth`: Best performing model (use this for inference)\n",
    "- `model_latest.pth`: Most recent checkpoint\n",
    "- `model_epoch_XX.pth`: Periodic checkpoints\n",
    "\n",
    "### Performance Expectations:\n",
    "- **50 epochs**: ~70-80% accuracy\n",
    "- **100 epochs**: ~75-85% accuracy\n",
    "- **Semi-supervised**: Should outperform supervised training\n",
    "\n",
    "**Happy fish classification! üêüüéâ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
