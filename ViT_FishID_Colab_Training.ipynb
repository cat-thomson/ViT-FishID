{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "796c6e33",
      "metadata": {
        "id": "796c6e33"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cat-thomson/ViT-FishID/blob/main/ViT_FishID_Colab_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f2f60df",
      "metadata": {
        "id": "1f2f60df"
      },
      "source": [
        "# 🐟 ViT-FishID: Semi-Supervised Training in Google Colab\n",
        "\n",
        "This notebook demonstrates how to train the ViT-FishID model using Google Colab's free GPU resources.\n",
        "\n",
        "**Expected Performance:**\n",
        "- Training Time: 2-3 hours for 50 epochs on Tesla T4\n",
        "- Memory Usage: ~6-8GB GPU memory\n",
        "- Accuracy: ~75-85% validation accuracy\n",
        "\n",
        "**Requirements:**\n",
        "- Google account with Google Drive access\n",
        "- Fish dataset uploaded to Google Drive\n",
        "- GPU runtime enabled in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26bcb2a3",
      "metadata": {
        "id": "26bcb2a3"
      },
      "source": [
        "## 🚀 Step 1: Setup and GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f3540b19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3540b19",
        "outputId": "d59ef30c-5fd8-4fda-c1bd-0cece4e1ce56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 System Information:\n",
            "Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "GPU Device: Tesla T4\n",
            "GPU Memory: 14.7 GB\n",
            "✅ GPU is ready for training!\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"🔍 System Information:\")\n",
        "print(f\"Python version: {os.sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    print(\"✅ GPU is ready for training!\")\n",
        "else:\n",
        "    print(\"❌ No GPU detected. Please enable GPU runtime:\")\n",
        "    print(\"   Runtime → Change runtime type → Hardware accelerator → GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149f671b",
      "metadata": {
        "id": "149f671b"
      },
      "source": [
        "## 📁 Step 2: Mount Google Drive\n",
        "\n",
        "This will give us access to your fish dataset stored in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4abb3ffd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4abb3ffd",
        "outputId": "0f8041e3-6ffd-4937-c221-a0c547fc2723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "📂 Google Drive contents:\n",
            "  - Mock Matric\n",
            "  - Photos\n",
            "  - Admin\n",
            "  - Uni\n",
            "  - Fish_Training_Output\n",
            "  - fish_cutouts.zip\n",
            "  - Colab Notebooks\n",
            "\n",
            "✅ Google Drive mounted successfully!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# List contents to verify mount\n",
        "print(\"\\n📂 Google Drive contents:\")\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "if os.path.exists(drive_path):\n",
        "    items = os.listdir(drive_path)[:10]  # Show first 10 items\n",
        "    for item in items:\n",
        "        print(f\"  - {item}\")\n",
        "    if len(os.listdir(drive_path)) > 10:\n",
        "        print(f\"  ... and {len(os.listdir(drive_path)) - 10} more items\")\n",
        "    print(\"\\n✅ Google Drive mounted successfully!\")\n",
        "else:\n",
        "    print(\"❌ Failed to mount Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be8b6273",
      "metadata": {
        "id": "be8b6273"
      },
      "source": [
        "## 📦 Step 3: Install Dependencies\n",
        "\n",
        "Installing all required packages for ViT-FishID training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8c724abc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c724abc",
        "outputId": "99497c0d-a3f3-4ae2-878a-421e877ce582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Installing dependencies...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ All dependencies installed successfully!\n",
            "\n",
            "📋 Package versions:\n",
            "  - torch: 2.6.0+cu124\n",
            "  - torchvision: 0.21.0+cu124\n",
            "  - timm: 1.0.19\n",
            "  - albumentations: 2.0.8\n",
            "  - opencv: 4.12.0\n",
            "  - sklearn: 1.6.1\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "print(\"📦 Installing dependencies...\")\n",
        "\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q timm transformers\n",
        "!pip install -q albumentations\n",
        "!pip install -q wandb\n",
        "!pip install -q opencv-python-headless\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q matplotlib seaborn\n",
        "!pip install -q tqdm\n",
        "\n",
        "print(\"✅ All dependencies installed successfully!\")\n",
        "\n",
        "# Verify installations\n",
        "import torch\n",
        "import torchvision\n",
        "import timm\n",
        "import albumentations\n",
        "import cv2\n",
        "import sklearn\n",
        "\n",
        "print(\"\\n📋 Package versions:\")\n",
        "print(f\"  - torch: {torch.__version__}\")\n",
        "print(f\"  - torchvision: {torchvision.__version__}\")\n",
        "print(f\"  - timm: {timm.__version__}\")\n",
        "print(f\"  - albumentations: {albumentations.__version__}\")\n",
        "print(f\"  - opencv: {cv2.__version__}\")\n",
        "print(f\"  - sklearn: {sklearn.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b795fc",
      "metadata": {
        "id": "12b795fc"
      },
      "source": [
        "## 🔄 Step 4: Clone ViT-FishID Repository\n",
        "\n",
        "Getting the latest code from your GitHub repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c4e4cd45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4e4cd45",
        "outputId": "5cca3cc5-d197-44a9-b88e-20897021b0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Cloning ViT-FishID repository...\n",
            "Cloning into '/content/ViT-FishID'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 88 (delta 25), reused 79 (delta 16), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (88/88), 147.82 KiB | 2.69 MiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n",
            "/content/ViT-FishID\n",
            "\n",
            "📂 Project structure:\n",
            "total 264\n",
            "drwxr-xr-x 6 root root  4096 Aug 13 11:21 .\n",
            "drwxr-xr-x 1 root root  4096 Aug 13 11:21 ..\n",
            "drwxr-xr-x 2 root root  4096 Aug 13 11:21 backup_old_files\n",
            "-rw-r--r-- 1 root root  6938 Aug 13 11:21 COLAB_GUIDE.md\n",
            "-rw-r--r-- 1 root root 11892 Aug 13 11:21 colab_setup.py\n",
            "-rw-r--r-- 1 root root  5452 Aug 13 11:21 COLAB_SETUP_SUMMARY.md\n",
            "-rw-r--r-- 1 root root 19713 Aug 13 11:21 data.py\n",
            "-rw-r--r-- 1 root root 11572 Aug 13 11:21 evaluate.py\n",
            "drwxr-xr-x 2 root root  4096 Aug 13 11:21 fish_cutouts\n",
            "drwxr-xr-x 2 root root  4096 Aug 13 11:21 Frames\n",
            "drwxr-xr-x 8 root root  4096 Aug 13 11:21 .git\n",
            "-rw-r--r-- 1 root root    66 Aug 13 11:21 .gitattributes\n",
            "-rw-r--r-- 1 root root   646 Aug 13 11:21 .gitignore\n",
            "-rw-r--r-- 1 root root  9155 Aug 13 11:21 model.py\n",
            "-rw-r--r-- 1 root root 16771 Aug 13 11:21 pipeline.py\n",
            "-rw-r--r-- 1 root root  5429 Aug 13 11:21 PROJECT_CLEANUP_SUMMARY.md\n",
            "-rw-r--r-- 1 root root  5373 Aug 13 11:21 quickstart.py\n",
            "-rw-r--r-- 1 root root 16566 Aug 13 11:21 README.md\n",
            "-rw-r--r-- 1 root root   202 Aug 13 11:21 requirements.txt\n",
            "-rw-r--r-- 1 root root  5134 Aug 13 11:21 species_mapping.txt\n",
            "-rw-r--r-- 1 root root  1616 Aug 13 11:21 test_args.py\n",
            "-rw-r--r-- 1 root root 22998 Aug 13 11:21 trainer.py\n",
            "-rw-r--r-- 1 root root 13773 Aug 13 11:21 train.py\n",
            "-rw-r--r-- 1 root root  8818 Aug 13 11:21 utils.py\n",
            "-rw-r--r-- 1 root root 36248 Aug 13 11:21 ViT_FishID_Colab_Training.ipynb\n",
            "\n",
            "✅ Repository cloned successfully!\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "import os\n",
        "\n",
        "# Remove existing directory if it exists\n",
        "if os.path.exists('/content/ViT-FishID'):\n",
        "    !rm -rf /content/ViT-FishID\n",
        "\n",
        "# Clone the repository\n",
        "print(\"📥 Cloning ViT-FishID repository...\")\n",
        "!git clone https://github.com/cat-thomson/ViT-FishID.git /content/ViT-FishID\n",
        "\n",
        "# Change to project directory\n",
        "%cd /content/ViT-FishID\n",
        "\n",
        "# List project files\n",
        "print(\"\\n📂 Project structure:\")\n",
        "!ls -la\n",
        "\n",
        "print(\"\\n✅ Repository cloned successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8155c400",
      "metadata": {
        "id": "8155c400"
      },
      "source": [
        "## 🗂️ Step 5: Setup Data Path and Extraction\n",
        "\n",
        "**IMPORTANT:** Specify the path to your fish dataset ZIP file in Google Drive.\n",
        "\n",
        "This step will:\n",
        "1. Locate your `fish_cutouts.zip` file in Google Drive\n",
        "2. Extract it to Colab's local storage for faster access\n",
        "3. Validate the data structure\n",
        "\n",
        "Expected structure after extraction:\n",
        "```\n",
        "fish_cutouts/\n",
        "├── labeled/\n",
        "│   ├── species_1/\n",
        "│   │   ├── fish_001.jpg\n",
        "│   │   └── fish_002.jpg\n",
        "│   └── species_2/\n",
        "│       └── ...\n",
        "└── unlabeled/\n",
        "    ├── fish_003.jpg\n",
        "    └── fish_004.jpg\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODIFY THIS PATH to point to your fish_cutouts.zip file in Google Drive\n",
        "ZIP_FILE_PATH = '/content/drive/MyDrive/fish_cutouts.zip'  # 👈 Your ZIP file path\n",
        "\n",
        "# Alternative common paths (uncomment the one that matches your setup):\n",
        "# ZIP_FILE_PATH = '/content/drive/MyDrive/ViT-FishID/fish_cutouts.zip'\n",
        "# ZIP_FILE_PATH = '/content/drive/MyDrive/datasets/fish_cutouts.zip'\n",
        "# ZIP_FILE_PATH = '/content/drive/MyDrive/data/fish_cutouts.zip'\n",
        "\n",
        "# Local extraction directory (will be created in Colab)\n",
        "DATA_DIR = '/content/fish_cutouts'\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "print(f\"🎯 ZIP file location: {ZIP_FILE_PATH}\")\n",
        "print(f\"📁 Extraction target: {DATA_DIR}\")\n",
        "\n",
        "# Check if ZIP file exists\n",
        "if os.path.exists(ZIP_FILE_PATH):\n",
        "    print(\"✅ ZIP file found!\")\n",
        "\n",
        "    # Get ZIP file size\n",
        "    zip_size_mb = os.path.getsize(ZIP_FILE_PATH) / (1024 * 1024)\n",
        "    print(f\"📦 ZIP file size: {zip_size_mb:.1f} MB\")\n",
        "\n",
        "    # Remove existing extracted data if present\n",
        "    if os.path.exists(DATA_DIR):\n",
        "        print(\"🧹 Removing existing extracted data...\")\n",
        "        shutil.rmtree(DATA_DIR)\n",
        "\n",
        "    # Extract ZIP file\n",
        "    print(\"📤 Extracting ZIP file to local storage...\")\n",
        "    print(\"⏳ This may take a few minutes for large datasets...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(ZIP_FILE_PATH, 'r') as zip_ref:\n",
        "            # Extract to temporary location first\n",
        "            temp_extract_dir = '/content/temp_extract'\n",
        "            if os.path.exists(temp_extract_dir):\n",
        "                shutil.rmtree(temp_extract_dir)\n",
        "            zip_ref.extractall(temp_extract_dir)\n",
        "\n",
        "            # DEBUG: Show what was extracted\n",
        "            extracted_items = os.listdir(temp_extract_dir)\n",
        "            print(f\"🔍 DEBUG - Extracted items: {extracted_items}\")\n",
        "\n",
        "            # Look for the actual dataset (skip macOS artifacts)\n",
        "            data_candidates = [item for item in extracted_items\n",
        "                             if not item.startswith('.') and not item.startswith('__MACOSX')]\n",
        "            print(f\"🔍 DEBUG - Data candidates: {data_candidates}\")\n",
        "\n",
        "            # Find the directory that contains labeled/ and unlabeled/\n",
        "            dataset_found = False\n",
        "\n",
        "            for candidate in data_candidates:\n",
        "                candidate_path = os.path.join(temp_extract_dir, candidate)\n",
        "                if os.path.isdir(candidate_path):\n",
        "                    print(f\"🔍 DEBUG - Checking {candidate}: {os.listdir(candidate_path)}\")\n",
        "\n",
        "                    # Check if this directory has labeled/ and unlabeled/\n",
        "                    if 'labeled' in os.listdir(candidate_path) and 'unlabeled' in os.listdir(candidate_path):\n",
        "                        print(f\"✅ Found dataset in: {candidate}\")\n",
        "                        shutil.move(candidate_path, DATA_DIR)\n",
        "                        dataset_found = True\n",
        "                        break\n",
        "\n",
        "                    # Check one level deeper\n",
        "                    subdirs = [d for d in os.listdir(candidate_path)\n",
        "                             if os.path.isdir(os.path.join(candidate_path, d)) and not d.startswith('.') and not d.startswith('__')]\n",
        "\n",
        "                    for subdir in subdirs:\n",
        "                        subdir_path = os.path.join(candidate_path, subdir)\n",
        "                        print(f\"🔍 DEBUG - Checking {candidate}/{subdir}: {os.listdir(subdir_path)}\")\n",
        "\n",
        "                        if 'labeled' in os.listdir(subdir_path) and 'unlabeled' in os.listdir(subdir_path):\n",
        "                            print(f\"✅ Found dataset one level deeper in: {candidate}/{subdir}\")\n",
        "                            shutil.move(subdir_path, DATA_DIR)\n",
        "                            dataset_found = True\n",
        "                            break\n",
        "\n",
        "                    if dataset_found:\n",
        "                        break\n",
        "\n",
        "            if not dataset_found:\n",
        "                print(\"⚠️  Could not find labeled/ and unlabeled/ folders, moving first candidate\")\n",
        "                if data_candidates:\n",
        "                    shutil.move(os.path.join(temp_extract_dir, data_candidates[0]), DATA_DIR)\n",
        "                else:\n",
        "                    shutil.move(temp_extract_dir, DATA_DIR)\n",
        "\n",
        "            # Clean up temp directory if it still exists\n",
        "            if os.path.exists(temp_extract_dir):\n",
        "                shutil.rmtree(temp_extract_dir)\n",
        "\n",
        "        extraction_time = time.time() - start_time\n",
        "        print(f\"✅ Extraction completed in {extraction_time:.1f} seconds!\")\n",
        "\n",
        "    except zipfile.BadZipFile:\n",
        "        print(\"❌ Error: Invalid ZIP file format\")\n",
        "        print(\"Please check that your file is a valid ZIP archive\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during extraction: {str(e)}\")\n",
        "        print(\"Please check the ZIP file path and try again\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ ZIP file not found!\")\n",
        "    print(\"\\n🔧 To fix this:\")\n",
        "    print(\"1. Upload your fish_cutouts.zip file to Google Drive\")\n",
        "    print(\"2. Update the ZIP_FILE_PATH variable above with the correct path\")\n",
        "    print(\"3. Make sure the file name is exactly 'fish_cutouts.zip'\")\n",
        "    print(\"\\n💡 Common locations to check:\")\n",
        "    print(\"   - /content/drive/MyDrive/fish_cutouts.zip\")\n",
        "    print(\"   - /content/drive/MyDrive/ViT-FishID/fish_cutouts.zip\")\n",
        "    print(\"   - /content/drive/MyDrive/datasets/fish_cutouts.zip\")\n",
        "\n",
        "# Now validate the extracted data\n",
        "print(f\"\\n📊 Validating extracted dataset...\")\n",
        "\n",
        "# Check if data directory exists\n",
        "if os.path.exists(DATA_DIR):\n",
        "    print(\"✅ Data directory found!\")\n",
        "\n",
        "    # DEBUG: Show the actual structure\n",
        "    print(\"🔍 DEBUG - Final directory structure:\")\n",
        "    for item in os.listdir(DATA_DIR):\n",
        "        item_path = os.path.join(DATA_DIR, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"  📂 {item}/\")\n",
        "            try:\n",
        "                subitems = os.listdir(item_path)[:10]  # Show first 10 items\n",
        "                for subitem in subitems:\n",
        "                    subitem_path = os.path.join(item_path, subitem)\n",
        "                    if os.path.isdir(subitem_path):\n",
        "                        print(f\"    📂 {subitem}/\")\n",
        "                    else:\n",
        "                        print(f\"    📄 {subitem}\")\n",
        "                if len(os.listdir(item_path)) > 10:\n",
        "                    print(f\"    ... and {len(os.listdir(item_path)) - 10} more items\")\n",
        "            except (PermissionError, OSError):\n",
        "                print(\"    (cannot read contents)\")\n",
        "        else:\n",
        "            print(f\"  📄 {item}\")\n",
        "\n",
        "    # Show directory size\n",
        "    def get_dir_size(path):\n",
        "        total = 0\n",
        "        for dirpath, dirnames, filenames in os.walk(path):\n",
        "            for filename in filenames:\n",
        "                total += os.path.getsize(os.path.join(dirpath, filename))\n",
        "        return total / (1024 * 1024)  # Convert to MB\n",
        "\n",
        "    dir_size_mb = get_dir_size(DATA_DIR)\n",
        "    print(f\"📏 Extracted dataset size: {dir_size_mb:.1f} MB\")\n",
        "\n",
        "    # Check for labeled and unlabeled subdirectories\n",
        "    labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
        "    unlabeled_dir = os.path.join(DATA_DIR, 'unlabeled')\n",
        "\n",
        "    if os.path.exists(labeled_dir) and os.path.exists(unlabeled_dir):\n",
        "        print(\"✅ Semi-supervised structure detected (labeled/ and unlabeled/ folders)\")\n",
        "\n",
        "        # Count classes and samples\n",
        "        classes = [d for d in os.listdir(labeled_dir) if os.path.isdir(os.path.join(labeled_dir, d))]\n",
        "        print(f\"📊 Found {len(classes)} species classes\")\n",
        "\n",
        "        # Count labeled samples\n",
        "        labeled_count = 0\n",
        "        sample_classes = classes[:5]  # Show first 5 classes\n",
        "        for class_dir in sample_classes:\n",
        "            class_path = os.path.join(labeled_dir, class_dir)\n",
        "            if os.path.isdir(class_path):\n",
        "                class_samples = len([f for f in os.listdir(class_path)\n",
        "                                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                labeled_count += class_samples\n",
        "                print(f\"  - {class_dir}: {class_samples} samples\")\n",
        "\n",
        "        if len(classes) > 5:\n",
        "            # Count remaining classes\n",
        "            remaining_count = 0\n",
        "            for class_dir in classes[5:]:\n",
        "                class_path = os.path.join(labeled_dir, class_dir)\n",
        "                if os.path.isdir(class_path):\n",
        "                    class_samples = len([f for f in os.listdir(class_path)\n",
        "                                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                    remaining_count += class_samples\n",
        "            labeled_count += remaining_count\n",
        "            print(f\"  ... and {len(classes) - 5} more classes with {remaining_count} samples\")\n",
        "\n",
        "        print(f\"📊 Total labeled samples: {labeled_count:,}\")\n",
        "\n",
        "        # Count unlabeled samples\n",
        "        unlabeled_files = [f for f in os.listdir(unlabeled_dir)\n",
        "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        print(f\"📊 Unlabeled samples: {len(unlabeled_files):,}\")\n",
        "\n",
        "        print(f\"\\n🎯 Dataset ready for training!\")\n",
        "        print(f\"   Total samples: {labeled_count + len(unlabeled_files):,}\")\n",
        "        print(f\"   Labeled: {labeled_count:,} ({labeled_count/(labeled_count + len(unlabeled_files))*100:.1f}%)\")\n",
        "        print(f\"   Unlabeled: {len(unlabeled_files):,} ({len(unlabeled_files)/(labeled_count + len(unlabeled_files))*100:.1f}%)\")\n",
        "\n",
        "    elif any(os.path.isdir(os.path.join(DATA_DIR, d)) for d in os.listdir(DATA_DIR)):\n",
        "        print(\"ℹ️  Supervised structure detected (species folders directly in data dir)\")\n",
        "        classes = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
        "        print(f\"📊 Found {len(classes)} species classes\")\n",
        "        print(\"⚠️  Note: For semi-supervised training, organize data into labeled/ and unlabeled/ folders\")\n",
        "\n",
        "        # Let's see what these 2 classes actually are\n",
        "        print(\"� DEBUG - The 2 'classes' found are:\")\n",
        "        for class_dir in classes:\n",
        "            class_path = os.path.join(DATA_DIR, class_dir)\n",
        "            print(f\"  📂 {class_dir}/\")\n",
        "            try:\n",
        "                items = os.listdir(class_path)[:5]\n",
        "                for item in items:\n",
        "                    print(f\"    - {item}\")\n",
        "                if len(os.listdir(class_path)) > 5:\n",
        "                    print(f\"    ... and {len(os.listdir(class_path)) - 5} more items\")\n",
        "            except:\n",
        "                print(\"    (cannot read contents)\")\n",
        "    else:\n",
        "        print(\"❌ No valid data structure found in extracted files\")\n",
        "        print(\"Expected: labeled/ and unlabeled/ subdirectories\")\n",
        "        print(\"\\n� Current structure:\")\n",
        "        for item in os.listdir(DATA_DIR):\n",
        "            print(f\"  - {item}\")\n",
        "else:\n",
        "    print(\"❌ Data extraction failed or directory not found!\")\n",
        "    print(\"Please check the ZIP file and extraction process above\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjoaLGksMSOc",
        "outputId": "8aa1e438-34d7-4d17-d2c7-de0dfe03b09c"
      },
      "id": "mjoaLGksMSOc",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 ZIP file location: /content/drive/MyDrive/fish_cutouts.zip\n",
            "📁 Extraction target: /content/fish_cutouts\n",
            "✅ ZIP file found!\n",
            "📦 ZIP file size: 217.9 MB\n",
            "🧹 Removing existing extracted data...\n",
            "📤 Extracting ZIP file to local storage...\n",
            "⏳ This may take a few minutes for large datasets...\n",
            "🔍 DEBUG - Extracted items: ['__MACOSX', 'fish_cutouts']\n",
            "🔍 DEBUG - Data candidates: ['fish_cutouts']\n",
            "🔍 DEBUG - Checking fish_cutouts: ['dataset_info.json', 'unlabeled', 'labeled']\n",
            "✅ Found dataset in: fish_cutouts\n",
            "✅ Extraction completed in 8.4 seconds!\n",
            "\n",
            "📊 Validating extracted dataset...\n",
            "✅ Data directory found!\n",
            "🔍 DEBUG - Final directory structure:\n",
            "  📄 dataset_info.json\n",
            "  📂 unlabeled/\n",
            "    📄 21-05_WILD_061_R (2)_21-05_WILD_061_R (2)_frame_7317_det10_cls0.jpg\n",
            "    📄 21-05_WILD_065_L (1)_21-05_WILD_065_L (1)_frame_8983_det00_cls31.jpg\n",
            "    📄 17-04_NSC-S_080_17-04_NSC-S_080_frame_17371_det08_cls0.jpg\n",
            "    📄 17-04_NSC-S_082_17-04_NSC-S_082_frame_22205_det22_cls0.jpg\n",
            "    📄 17-04_NSC-S_012_17-04_NSC-S_012_frame_37333_det11_cls0.jpg\n",
            "    📄 17-04_NSC-S_082_17-04_NSC-S_082_frame_31645_det05_cls0.jpg\n",
            "    📄 14-01_Pondo_012_14-01_Pondo_012_frame_74052_det01_cls0.jpg\n",
            "    📄 17-04_NSC-S_079_17-04_NSC-S_079_frame_21318_det10_cls0.jpg\n",
            "    📄 21-05_WILD_063_L (1)_21-05_WILD_063_L (1)_frame_14784_det04_cls0.jpg\n",
            "    📄 21-05_WILD_078_L (1)_21-05_WILD_078_L (1)_frame_33865_det01_cls0.jpg\n",
            "    ... and 24005 more items\n",
            "  📂 labeled/\n",
            "    📂 Carangidae_Seriola_lalandi/\n",
            "    📂 Sparidae_Porcostoma_dentata/\n",
            "    📂 Sparidae_Pagellus_bellottii_natalensis/\n",
            "    📂 Sparidae_Diplodus_hottentotus/\n",
            "    📂 Sparidae_Polysteganus_praeorbitalis/\n",
            "    📂 Sparidae_Petrus_rupestris/\n",
            "    📂 Carangidae_Pseudocaranx_dentex/\n",
            "    📂 Sparidae_Rhabdosargus_thorpei/\n",
            "    📂 Sparidae_Cheimerius_nufar/\n",
            "    📂 Serranidae_Lipropoma_spp1/\n",
            "    ... and 27 more items\n",
            "📏 Extracted dataset size: 201.2 MB\n",
            "✅ Semi-supervised structure detected (labeled/ and unlabeled/ folders)\n",
            "📊 Found 37 species classes\n",
            "  - Carangidae_Seriola_lalandi: 24 samples\n",
            "  - Sparidae_Porcostoma_dentata: 447 samples\n",
            "  - Sparidae_Pagellus_bellottii_natalensis: 31 samples\n",
            "  - Sparidae_Diplodus_hottentotus: 110 samples\n",
            "  - Sparidae_Polysteganus_praeorbitalis: 256 samples\n",
            "  ... and 32 more classes with 4274 samples\n",
            "📊 Total labeled samples: 5,142\n",
            "📊 Unlabeled samples: 24,015\n",
            "\n",
            "🎯 Dataset ready for training!\n",
            "   Total samples: 29,157\n",
            "   Labeled: 5,142 (17.6%)\n",
            "   Unlabeled: 24,015 (82.4%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f0fe32",
      "metadata": {
        "id": "31f0fe32"
      },
      "source": [
        "## 📊 Step 6: Setup Weights & Biases (Optional)\n",
        "\n",
        "W&B provides excellent training visualization and experiment tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cf74493e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "cf74493e",
        "outputId": "0813aad4-6ee2-4a07-f4cd-9e052aa93a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔐 Setting up Weights & Biases...\n",
            "\n",
            "To use W&B:\n",
            "1. Go to https://wandb.ai and create a free account\n",
            "2. Get your API key from https://wandb.ai/authorize\n",
            "3. Run the cell below and paste your API key when prompted\n",
            "\n",
            "Or skip W&B by setting USE_WANDB = False below\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcativthomson\u001b[0m (\u001b[33mcativthomson-university-of-cape-town\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ W&B login successful!\n"
          ]
        }
      ],
      "source": [
        "# Setup Weights & Biases for experiment tracking\n",
        "import wandb\n",
        "\n",
        "# Login to W&B (you'll need to create a free account at wandb.ai)\n",
        "print(\"🔐 Setting up Weights & Biases...\")\n",
        "print(\"\\nTo use W&B:\")\n",
        "print(\"1. Go to https://wandb.ai and create a free account\")\n",
        "print(\"2. Get your API key from https://wandb.ai/authorize\")\n",
        "print(\"3. Run the cell below and paste your API key when prompted\")\n",
        "print(\"\\nOr skip W&B by setting USE_WANDB = False below\")\n",
        "\n",
        "# Set this to True if you want to use W&B, False to skip\n",
        "USE_WANDB = True  # 👈 Set to False if you don't want to use W&B\n",
        "\n",
        "if USE_WANDB:\n",
        "    try:\n",
        "        # Try to login (will prompt for API key if not already logged in)\n",
        "        wandb.login()\n",
        "        print(\"✅ W&B login successful!\")\n",
        "    except:\n",
        "        print(\"⚠️  W&B login failed. Training will continue without W&B logging.\")\n",
        "        USE_WANDB = False\n",
        "else:\n",
        "    print(\"ℹ️  Skipping W&B setup. Training will run without experiment tracking.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fe6af6d",
      "metadata": {
        "id": "0fe6af6d"
      },
      "source": [
        "## ⚙️ Step 7: Configure Training Parameters\n",
        "\n",
        "Adjust these parameters based on your needs and available GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "65e959fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65e959fd",
        "outputId": "87cb98d5-54ea-4723-9ec7-fefb67036416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎛️  Training Configuration:\n",
            "  mode: semi_supervised\n",
            "  epochs: 50\n",
            "  batch_size: 16\n",
            "  image_size: 224\n",
            "  num_workers: 2\n",
            "  val_split: 0.2\n",
            "  test_split: 0.2\n",
            "  model_name: vit_base_patch16_224\n",
            "  pretrained: True\n",
            "  dropout_rate: 0.1\n",
            "  learning_rate: 0.0001\n",
            "  weight_decay: 0.05\n",
            "  warmup_epochs: 5\n",
            "  consistency_weight: 2.0\n",
            "  pseudo_label_threshold: 0.7\n",
            "  temperature: 4.0\n",
            "  unlabeled_ratio: 2.0\n",
            "  ramp_up_epochs: 10\n",
            "  ema_momentum: 0.999\n",
            "  use_wandb: True\n",
            "  wandb_project: vit-fish-colab\n",
            "  save_frequency: 10\n",
            "  seed: 42\n",
            "\n",
            "💡 Tips for Colab:\n",
            "  - Batch size 16 should work on most Colab GPUs\n",
            "  - 50 epochs will take ~2-3 hours\n",
            "  - If you get GPU memory errors, reduce batch_size to 8\n",
            "  - Training will automatically save checkpoints every 10 epochs\n"
          ]
        }
      ],
      "source": [
        "# Training configuration\n",
        "TRAINING_CONFIG = {\n",
        "    # Basic settings\n",
        "    'mode': 'semi_supervised',  # 'supervised' or 'semi_supervised'\n",
        "    'epochs': 50,               # Reduced for Colab (normally 100)\n",
        "    'batch_size': 16,           # Reduced for GPU memory (normally 32)\n",
        "    'image_size': 224,\n",
        "    'num_workers': 2,           # Reduced for Colab\n",
        "\n",
        "    # Data splitting\n",
        "    'val_split': 0.2,          # 20% for validation\n",
        "    'test_split': 0.2,         # 20% for test\n",
        "\n",
        "    # Model settings\n",
        "    'model_name': 'vit_base_patch16_224',\n",
        "    'pretrained': True,\n",
        "    'dropout_rate': 0.1,\n",
        "\n",
        "    # Training hyperparameters\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 0.05,\n",
        "    'warmup_epochs': 5,        # Reduced for shorter training\n",
        "\n",
        "    # Semi-supervised settings\n",
        "    'consistency_weight': 2.0,\n",
        "    'pseudo_label_threshold': 0.7,\n",
        "    'temperature': 4.0,\n",
        "    'unlabeled_ratio': 2.0,\n",
        "    'ramp_up_epochs': 10,      # Reduced for shorter training\n",
        "\n",
        "    # EMA settings\n",
        "    'ema_momentum': 0.999,\n",
        "\n",
        "    # Logging\n",
        "    'use_wandb': USE_WANDB,\n",
        "    'wandb_project': 'vit-fish-colab',\n",
        "    'save_frequency': 10,      # Save every 10 epochs\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "print(\"🎛️  Training Configuration:\")\n",
        "for key, value in TRAINING_CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\n💡 Tips for Colab:\")\n",
        "print(f\"  - Batch size {TRAINING_CONFIG['batch_size']} should work on most Colab GPUs\")\n",
        "print(f\"  - {TRAINING_CONFIG['epochs']} epochs will take ~2-3 hours\")\n",
        "print(\"  - If you get GPU memory errors, reduce batch_size to 8\")\n",
        "print(\"  - Training will automatically save checkpoints every 10 epochs\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# SAVE PROJECT FILES TO GOOGLE DRIVE\n",
        "# ========================================\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Create project directory in Google Drive\n",
        "project_dir = '/content/drive/MyDrive/ViT-FishID/code'\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "\n",
        "# List of files to save (adjust paths as needed)\n",
        "files_to_save = [\n",
        "    'data.py',\n",
        "    'model.py',\n",
        "    'trainer.py',\n",
        "    'utils.py',\n",
        "    'train.py',\n",
        "    'ViT_FishID_Colab_Training.ipynb'  # Your notebook\n",
        "]\n",
        "\n",
        "print(\"💾 Saving project files to Google Drive...\")\n",
        "\n",
        "for filename in files_to_save:\n",
        "    try:\n",
        "        if os.path.exists(filename):\n",
        "            dest_path = os.path.join(project_dir, filename)\n",
        "            shutil.copy2(filename, dest_path)\n",
        "            print(f\"✅ Saved: {filename}\")\n",
        "        else:\n",
        "            print(f\"⚠️  File not found: {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error saving {filename}: {e}\")\n",
        "\n",
        "print(f\"\\n📁 Project files saved to: {project_dir}\")\n",
        "\n",
        "# Also save the species mapping if it exists\n",
        "try:\n",
        "    if os.path.exists('species_mapping.txt'):\n",
        "        shutil.copy2('species_mapping.txt', os.path.join(project_dir, 'species_mapping.txt'))\n",
        "        print(\"✅ Saved: species_mapping.txt\")\n",
        "except:\n",
        "    print(\"⚠️  species_mapping.txt not found\")\n",
        "\n",
        "print(\"\\n🎉 All files backed up to Google Drive!\")"
      ],
      "metadata": {
        "id": "6E6uZ51az36B"
      },
      "id": "6E6uZ51az36B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "aa9cbd50",
      "metadata": {
        "id": "aa9cbd50"
      },
      "source": [
        "## 🚀 Step 8: Start Training!\n",
        "\n",
        "This cell will start the semi-supervised training process. It may take 2-3 hours to complete."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the repository with the latest fixes\n",
        "import os\n",
        "os.chdir('/content/ViT-FishID')\n",
        "!git pull origin main\n",
        "\n",
        "print(\"✅ Repository updated with latest fixes!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN7mS9igO0sp",
        "outputId": "fa854a1f-84a7-469e-8b52-347dfc521741"
      },
      "id": "pN7mS9igO0sp",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  33% (1/3)\rUnpacking objects:  66% (2/3)\rUnpacking objects: 100% (3/3)\rUnpacking objects: 100% (3/3), 718 bytes | 718.00 KiB/s, done.\n",
            "From https://github.com/cat-thomson/ViT-FishID\n",
            " * branch            main       -> FETCH_HEAD\n",
            "   abd54d1..1739055  main       -> origin/main\n",
            "Updating abd54d1..1739055\n",
            "Fast-forward\n",
            " data.py | 38 \u001b[32m++++++++++++++++++++++++++++++++++\u001b[m\u001b[31m----\u001b[m\n",
            " 1 file changed, 34 insertions(+), 4 deletions(-)\n",
            "✅ Repository updated with latest fixes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build training command\n",
        "training_cmd = f\"\"\"python train.py \\\n",
        "    --data_dir \"{DATA_DIR}\" \\\n",
        "    --mode {TRAINING_CONFIG['mode']} \\\n",
        "    --epochs {TRAINING_CONFIG['epochs']} \\\n",
        "    --batch_size {TRAINING_CONFIG['batch_size']} \\\n",
        "    --image_size {TRAINING_CONFIG['image_size']} \\\n",
        "    --num_workers {TRAINING_CONFIG['num_workers']} \\\n",
        "    --val_split {TRAINING_CONFIG['val_split']} \\\n",
        "    --test_split {TRAINING_CONFIG['test_split']} \\\n",
        "    --model_name {TRAINING_CONFIG['model_name']} \\\n",
        "    --learning_rate {TRAINING_CONFIG['learning_rate']} \\\n",
        "    --weight_decay {TRAINING_CONFIG['weight_decay']} \\\n",
        "    --warmup_epochs {TRAINING_CONFIG['warmup_epochs']} \\\n",
        "    --consistency_weight {TRAINING_CONFIG['consistency_weight']} \\\n",
        "    --pseudo_label_threshold {TRAINING_CONFIG['pseudo_label_threshold']} \\\n",
        "    --temperature {TRAINING_CONFIG['temperature']} \\\n",
        "    --unlabeled_ratio {TRAINING_CONFIG['unlabeled_ratio']} \\\n",
        "    --ramp_up_epochs {TRAINING_CONFIG['ramp_up_epochs']} \\\n",
        "    --ema_momentum {TRAINING_CONFIG['ema_momentum']} \\\n",
        "    --save_frequency {TRAINING_CONFIG['save_frequency']} \\\n",
        "    --seed {TRAINING_CONFIG['seed']}\"\"\"\n",
        "\n",
        "if TRAINING_CONFIG['use_wandb']:\n",
        "    training_cmd += f\" --use_wandb --wandb_project {TRAINING_CONFIG['wandb_project']}\"\n",
        "\n",
        "if TRAINING_CONFIG['pretrained']:\n",
        "    training_cmd += \" --pretrained\"\n",
        "\n",
        "print(\"🚀 Starting ViT-FishID training...\")\n",
        "print(\"\\n📋 Training command:\")\n",
        "print(training_cmd.replace('\\\\', '').strip())\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Execute training\n",
        "!{training_cmd}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ Training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8m3vPtQN1FL",
        "outputId": "23ec36f3-ee9b-49e8-f476-9bd8246aafdc"
      },
      "id": "t8m3vPtQN1FL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting ViT-FishID training...\n",
            "\n",
            "📋 Training command:\n",
            "python train.py     --data_dir \"/content/fish_cutouts\"     --mode semi_supervised     --epochs 50     --batch_size 16     --image_size 224     --num_workers 2     --val_split 0.2     --test_split 0.2     --model_name vit_base_patch16_224     --learning_rate 0.0001     --weight_decay 0.05     --warmup_epochs 5     --consistency_weight 2.0     --pseudo_label_threshold 0.7     --temperature 4.0     --unlabeled_ratio 2.0     --ramp_up_epochs 10     --ema_momentum 0.999     --save_frequency 10     --seed 42 --use_wandb --wandb_project vit-fish-colab --pretrained\n",
            "\n",
            "============================================================\n",
            "2025-08-13 11:41:13.462656: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755085273.482871    7081 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755085273.489001    7081 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755085273.505551    7081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755085273.505592    7081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755085273.505597    7081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755085273.505601    7081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-13 11:41:13.510428: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Random seed set to 42\n",
            "Using GPU: Tesla T4\n",
            "🐟 ViT-FishID Training\n",
            "📊 Mode: semi_supervised\n",
            "🖥️  Device: cuda\n",
            "📁 Data directory: /content/fish_cutouts\n",
            "\n",
            "📦 Creating data loaders...\n",
            "⚠️  Warning: Some classes have only 1 sample(s). Using random splitting instead of stratified.\n",
            "   Classes with 1 sample: ['Carangidae_Caranx_heberi', 'Serranidae_Lipropoma_spp1', 'Sparidae_Sparodon_durbanesis']\n",
            "/content/ViT-FishID/data.py:229: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
            "📊 Dataset initialized:\n",
            "  - Labeled samples: 3,084\n",
            "  - Unlabeled samples: 6,168\n",
            "  - Total samples per epoch: 9,252\n",
            "📊 Semi-supervised data loaders created:\n",
            "  - Train labeled: 3,084\n",
            "  - Train unlabeled: 6,168\n",
            "  - Val samples: 1,029\n",
            "  - Test samples: 1,029\n",
            "  - Classes: 37\n",
            "  - Split ratios: Train=60.0%, Val=20.0%, Test=20.0%\n",
            "🏷️  Classes (37): ['Carangidae_Caranx_heberi', 'Carangidae_Pseudocaranx_dentex', 'Carangidae_Seriola_dumerili', 'Carangidae_Seriola_lalandi', 'Carangidae_Seriola_rivoliana', 'Carangidae_Trachurus_delagoa', 'Serranidae_Aulacocephalus_temminckii', 'Serranidae_Epinephelus_andersoni', 'Serranidae_Epinephelus_marginatus', 'Serranidae_Epinephelus_rivulatus', 'Serranidae_Epinephelus_tukula', 'Serranidae_Lipropoma_spp1', 'Serranidae_Serranus_knysnaensis', 'Sparidae_Argyrops_spinifer', 'Sparidae_Boopsoidea_inornata', 'Sparidae_Cheimerius_nufar', 'Sparidae_Chrysoblephus_anglicus', 'Sparidae_Chrysoblephus_cristiceps', 'Sparidae_Chrysoblephus_lophus', 'Sparidae_Chrysoblephus_puniceus', 'Sparidae_Cymatoceps_nasutus', 'Sparidae_Diplodus_capensis', 'Sparidae_Diplodus_hottentotus', 'Sparidae_Pachymetopon_aeneum', 'Sparidae_Pachymetopon_grande', 'Sparidae_Pagellus_bellottii_natalensis', 'Sparidae_Petrus_rupestris', 'Sparidae_Polyamblydon_germanum', 'Sparidae_Polysteganus_praeorbitalis', 'Sparidae_Polysteganus_undulosus', 'Sparidae_Porcostoma_dentata', 'Sparidae_Rhabdosargus_holubi', 'Sparidae_Rhabdosargus_sarba', 'Sparidae_Rhabdosargus_thorpei', 'Sparidae_Sarpa_salpa', 'Sparidae_Sparodon_durbanesis', 'Sparidae_Spondyliosoma_emarginatum']\n",
            "📊 Test set available with 1,029 samples for final evaluation\n",
            "\n",
            "🧠 Creating ViT model: vit_base_patch16_224\n",
            "model.safetensors: 100% 346M/346M [00:01<00:00, 180MB/s]\n",
            "✅ EMA Teacher initialized with momentum: 0.999\n",
            "📊 Model parameters: 85,828,645\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcativthomson\u001b[0m (\u001b[33mcativthomson-university-of-cape-town\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/ViT-FishID/wandb/run-20250813_114123-bijhyg5r\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msemi_supervised_vit_base_patch16_224_20250813_114123\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cativthomson-university-of-cape-town/vit-fish-colab\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cativthomson-university-of-cape-town/vit-fish-colab/runs/bijhyg5r\u001b[0m\n",
            "✅ W&B initialized: vit-fish-colab/semi_supervised_vit_base_patch16_224_20250813_114123\n",
            "\n",
            "🚀 Creating trainer...\n",
            "✅ Semi-Supervised Trainer initialized\n",
            "  - Consistency weight: 2.0\n",
            "  - Pseudo-label threshold: 0.7\n",
            "  - Learning rate: 0.0001\n",
            "  - Warmup epochs: 5\n",
            "  - Ramp-up epochs: 10\n",
            "\n",
            "🎯 Starting semi_supervised training...\n",
            "💡 Note: Test set is reserved for final evaluation and not used during training\n",
            "🚀 Starting training for 50 epochs...\n",
            "📁 Checkpoints will be saved to: ./checkpoints\n",
            "Epoch 0: 100% 578/578 [07:16<00:00,  1.32it/s, Total=2.5658, Sup=2.5658, Cons=0.0001, L-Acc=27.4%, P-Acc=0.0%]\n",
            "                                               \n",
            "📊 Epoch 1/50\n",
            "Train - Total Loss: 2.5658\n",
            "Train - Labeled Acc: 27.4%, Pseudo Acc: 0.0%\n",
            "Train - High-conf Pseudo: 0/6165 (0.0%)\n",
            "Student Val - Acc: 59.4%\n",
            "Teacher Val - Acc: 35.5%\n",
            "🏆 New best accuracy: 35.47%\n",
            "Checkpoint saved to ./checkpoints/checkpoint_epoch_0.pth\n",
            "Best model saved to ./checkpoints/model_best.pth\n",
            "Epoch 1: 100% 578/578 [07:20<00:00,  1.31it/s, Total=1.8692, Sup=1.8692, Cons=0.0001, L-Acc=46.2%, P-Acc=96.3%]\n",
            "                                               \n",
            "📊 Epoch 2/50\n",
            "Train - Total Loss: 1.8692\n",
            "Train - Labeled Acc: 46.2%, Pseudo Acc: 96.3%\n",
            "Train - High-conf Pseudo: 27/6165 (0.4%)\n",
            "Student Val - Acc: 62.5%\n",
            "Teacher Val - Acc: 64.6%\n",
            "🏆 New best accuracy: 64.63%\n",
            "Checkpoint saved to ./checkpoints/checkpoint_epoch_1.pth\n",
            "Best model saved to ./checkpoints/model_best.pth\n",
            "Epoch 2: 100% 578/578 [07:20<00:00,  1.31it/s, Total=1.7681, Sup=1.7743, Cons=0.0001, L-Acc=48.3%, P-Acc=91.0%]\n",
            "                                               \n",
            "📊 Epoch 3/50\n",
            "Train - Total Loss: 1.7681\n",
            "Train - Labeled Acc: 48.3%, Pseudo Acc: 91.0%\n",
            "Train - High-conf Pseudo: 689/6165 (11.2%)\n",
            "Student Val - Acc: 63.8%\n",
            "Teacher Val - Acc: 74.7%\n",
            "🏆 New best accuracy: 74.73%\n",
            "Checkpoint saved to ./checkpoints/checkpoint_epoch_2.pth\n",
            "Best model saved to ./checkpoints/model_best.pth\n",
            "Epoch 3: 100% 578/578 [07:20<00:00,  1.31it/s, Total=1.7659, Sup=1.7690, Cons=0.0001, L-Acc=48.2%, P-Acc=84.9%]\n",
            "                                               \n",
            "📊 Epoch 4/50\n",
            "Train - Total Loss: 1.7659\n",
            "Train - Labeled Acc: 48.2%, Pseudo Acc: 84.9%\n",
            "Train - High-conf Pseudo: 1602/6166 (26.0%)\n",
            "Student Val - Acc: 57.7%\n",
            "Teacher Val - Acc: 76.4%\n",
            "🏆 New best accuracy: 76.38%\n",
            "Checkpoint saved to ./checkpoints/checkpoint_epoch_3.pth\n",
            "Best model saved to ./checkpoints/model_best.pth\n",
            "Epoch 4: 100% 578/578 [07:20<00:00,  1.31it/s, Total=1.8011, Sup=1.8042, Cons=0.0001, L-Acc=47.1%, P-Acc=80.3%]\n",
            "                                               \n",
            "📊 Epoch 5/50\n",
            "Train - Total Loss: 1.8011\n",
            "Train - Labeled Acc: 47.1%, Pseudo Acc: 80.3%\n",
            "Train - High-conf Pseudo: 1989/6168 (32.2%)\n",
            "Student Val - Acc: 54.3%\n",
            "Teacher Val - Acc: 77.9%\n",
            "🏆 New best accuracy: 77.94%\n",
            "Checkpoint saved to ./checkpoints/checkpoint_epoch_4.pth\n",
            "Best model saved to ./checkpoints/model_best.pth\n",
            "Epoch 5: 100% 578/578 [07:20<00:00,  1.31it/s, Total=1.7687, Sup=1.7810, Cons=0.0001, L-Acc=46.6%, P-Acc=77.9%]\n",
            "                                               \n",
            "📊 Epoch 6/50\n",
            "Train - Total Loss: 1.7687\n",
            "Train - Labeled Acc: 46.6%, Pseudo Acc: 77.9%\n",
            "Train - High-conf Pseudo: 2175/6166 (35.3%)\n",
            "Student Val - Acc: 59.6%\n",
            "Teacher Val - Acc: 76.1%\n",
            "Epoch 6: 100% 578/578 [07:42<00:00,  1.25it/s, Total=1.7272, Sup=1.7272, Cons=0.0001, L-Acc=48.5%, P-Acc=79.5%]\n",
            "                                               \n",
            "📊 Epoch 7/50\n",
            "Train - Total Loss: 1.7272\n",
            "Train - Labeled Acc: 48.5%, Pseudo Acc: 79.5%\n",
            "Train - High-conf Pseudo: 2231/6165 (36.2%)\n",
            "Student Val - Acc: 59.9%\n",
            "Teacher Val - Acc: 76.9%\n",
            "Epoch 7: 100% 578/578 [07:43<00:00,  1.25it/s, Total=1.6071, Sup=1.6070, Cons=0.0001, L-Acc=52.1%, P-Acc=81.2%]\n",
            "                                               \n",
            "📊 Epoch 8/50\n",
            "Train - Total Loss: 1.6071\n",
            "Train - Labeled Acc: 52.1%, Pseudo Acc: 81.2%\n",
            "Train - High-conf Pseudo: 2329/6166 (37.8%)\n",
            "Student Val - Acc: 61.6%\n",
            "Teacher Val - Acc: 77.3%\n",
            "Epoch 8: 100% 578/578 [07:42<00:00,  1.25it/s, Total=1.5543, Sup=1.5542, Cons=0.0001, L-Acc=53.2%, P-Acc=82.0%]\n",
            "                                               \n",
            "📊 Epoch 9/50\n",
            "Train - Total Loss: 1.5543\n",
            "Train - Labeled Acc: 53.2%, Pseudo Acc: 82.0%\n",
            "Train - High-conf Pseudo: 2449/6165 (39.7%)\n",
            "Student Val - Acc: 63.6%\n",
            "Teacher Val - Acc: 77.1%\n",
            "Epoch 9: 100% 578/578 [07:42<00:00,  1.25it/s, Total=1.4659, Sup=1.4682, Cons=0.0001, L-Acc=56.7%, P-Acc=84.4%]\n",
            "                                               \n",
            "📊 Epoch 10/50\n",
            "Train - Total Loss: 1.4659\n",
            "Train - Labeled Acc: 56.7%, Pseudo Acc: 84.4%\n",
            "Train - High-conf Pseudo: 2591/6166 (42.0%)\n",
            "Student Val - Acc: 63.3%\n",
            "Teacher Val - Acc: 77.0%\n",
            "Epoch 10: 100% 578/578 [07:41<00:00,  1.25it/s, Total=1.4340, Sup=1.4363, Cons=0.0001, L-Acc=56.8%, P-Acc=83.0%]\n",
            "                                               \n",
            "📊 Epoch 11/50\n",
            "Train - Total Loss: 1.4340\n",
            "Train - Labeled Acc: 56.8%, Pseudo Acc: 83.0%\n",
            "Train - High-conf Pseudo: 2685/6166 (43.5%)\n",
            "Student Val - Acc: 65.3%\n",
            "Teacher Val - Acc: 77.5%\n",
            "Checkpoint saved to ./checkpoints/checkpoint_epoch_10.pth\n",
            "Epoch 11: 100% 578/578 [07:42<00:00,  1.25it/s, Total=1.3896, Sup=1.3894, Cons=0.0001, L-Acc=58.7%, P-Acc=83.3%]\n",
            "                                               \n",
            "📊 Epoch 12/50\n",
            "Train - Total Loss: 1.3896\n",
            "Train - Labeled Acc: 58.7%, Pseudo Acc: 83.3%\n",
            "Train - High-conf Pseudo: 2715/6165 (44.0%)\n",
            "Student Val - Acc: 68.2%\n",
            "Teacher Val - Acc: 79.1%\n",
            "🏆 New best accuracy: 79.11%\n",
            "Checkpoint saved to ./checkpoints/checkpoint_epoch_11.pth\n",
            "Best model saved to ./checkpoints/model_best.pth\n",
            "Epoch 12: 100% 578/578 [07:44<00:00,  1.25it/s, Total=1.3045, Sup=1.3088, Cons=0.0001, L-Acc=59.8%, P-Acc=83.8%]\n",
            "                                               \n",
            "📊 Epoch 13/50\n",
            "Train - Total Loss: 1.3045\n",
            "Train - Labeled Acc: 59.8%, Pseudo Acc: 83.8%\n",
            "Train - High-conf Pseudo: 2774/6165 (45.0%)\n",
            "Student Val - Acc: 65.1%\n",
            "Teacher Val - Acc: 79.2%\n",
            "🏆 New best accuracy: 79.20%\n",
            "Checkpoint saved to ./checkpoints/checkpoint_epoch_12.pth\n",
            "Best model saved to ./checkpoints/model_best.pth\n",
            "Epoch 13: 100% 578/578 [07:44<00:00,  1.24it/s, Total=1.2932, Sup=1.2952, Cons=0.0001, L-Acc=61.6%, P-Acc=83.8%]\n",
            "                                               \n",
            "📊 Epoch 14/50\n",
            "Train - Total Loss: 1.2932\n",
            "Train - Labeled Acc: 61.6%, Pseudo Acc: 83.8%\n",
            "Train - High-conf Pseudo: 2881/6165 (46.7%)\n",
            "Student Val - Acc: 70.1%\n",
            "Teacher Val - Acc: 78.4%\n",
            "Epoch 14: 100% 578/578 [07:42<00:00,  1.25it/s, Total=1.2240, Sup=1.2281, Cons=0.0001, L-Acc=62.8%, P-Acc=86.6%]\n",
            "                                               \n",
            "📊 Epoch 15/50\n",
            "Train - Total Loss: 1.2240\n",
            "Train - Labeled Acc: 62.8%, Pseudo Acc: 86.6%\n",
            "Train - High-conf Pseudo: 2954/6165 (47.9%)\n",
            "Student Val - Acc: 69.6%\n",
            "Teacher Val - Acc: 78.5%\n",
            "Epoch 15: 100% 578/578 [07:42<00:00,  1.25it/s, Total=1.1888, Sup=1.1885, Cons=0.0001, L-Acc=63.1%, P-Acc=85.0%]\n",
            "                                               \n",
            "📊 Epoch 16/50\n",
            "Train - Total Loss: 1.1888\n",
            "Train - Labeled Acc: 63.1%, Pseudo Acc: 85.0%\n",
            "Train - High-conf Pseudo: 3112/6165 (50.5%)\n",
            "Student Val - Acc: 72.7%\n",
            "Teacher Val - Acc: 78.8%\n",
            "Epoch 16: 100% 578/578 [07:43<00:00,  1.25it/s, Total=1.1082, Sup=1.1099, Cons=0.0001, L-Acc=65.7%, P-Acc=86.4%]\n",
            "                                               \n",
            "📊 Epoch 17/50\n",
            "Train - Total Loss: 1.1082\n",
            "Train - Labeled Acc: 65.7%, Pseudo Acc: 86.4%\n",
            "Train - High-conf Pseudo: 3079/6164 (50.0%)\n",
            "Student Val - Acc: 66.8%\n",
            "Teacher Val - Acc: 79.7%\n",
            "🏆 New best accuracy: 79.69%\n",
            "Checkpoint saved to ./checkpoints/checkpoint_epoch_16.pth\n",
            "Best model saved to ./checkpoints/model_best.pth\n",
            "Epoch 17: 100% 578/578 [07:46<00:00,  1.24it/s, Total=1.0614, Sup=1.0630, Cons=0.0001, L-Acc=66.8%, P-Acc=87.1%]\n",
            "                                               \n",
            "📊 Epoch 18/50\n",
            "Train - Total Loss: 1.0614\n",
            "Train - Labeled Acc: 66.8%, Pseudo Acc: 87.1%\n",
            "Train - High-conf Pseudo: 3185/6165 (51.7%)\n",
            "Student Val - Acc: 73.0%\n",
            "Teacher Val - Acc: 79.8%\n",
            "🏆 New best accuracy: 79.79%\n",
            "Checkpoint saved to ./checkpoints/checkpoint_epoch_17.pth\n",
            "Best model saved to ./checkpoints/model_best.pth\n",
            "Epoch 18: 100% 578/578 [07:44<00:00,  1.24it/s, Total=0.9907, Sup=0.9921, Cons=0.0001, L-Acc=70.4%, P-Acc=85.6%]\n",
            "                                               \n",
            "📊 Epoch 19/50\n",
            "Train - Total Loss: 0.9907\n",
            "Train - Labeled Acc: 70.4%, Pseudo Acc: 85.6%\n",
            "Train - High-conf Pseudo: 3350/6166 (54.3%)\n",
            "Student Val - Acc: 72.1%\n",
            "Teacher Val - Acc: 79.8%\n",
            "Epoch 19: 100% 578/578 [07:42<00:00,  1.25it/s, Total=0.9820, Sup=0.9817, Cons=0.0001, L-Acc=70.0%, P-Acc=85.4%]\n",
            "                                               \n",
            "📊 Epoch 20/50\n",
            "Train - Total Loss: 0.9820\n",
            "Train - Labeled Acc: 70.0%, Pseudo Acc: 85.4%\n",
            "Train - High-conf Pseudo: 3396/6165 (55.1%)\n",
            "Student Val - Acc: 69.9%\n",
            "Teacher Val - Acc: 80.0%\n",
            "🏆 New best accuracy: 79.98%\n",
            "Checkpoint saved to ./checkpoints/checkpoint_epoch_19.pth\n",
            "Best model saved to ./checkpoints/model_best.pth\n",
            "Epoch 20:  65% 378/578 [05:02<02:39,  1.26it/s, Total=0.8469, Sup=0.8489, Cons=0.0001, L-Acc=72.7%, P-Acc=86.5%]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EMERGENCY CHECKPOINT SAVE - RUN NOW!\n",
        "# ========================================\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Drive already mounted or mount failed\")\n",
        "\n",
        "# Create checkpoints directory in Google Drive\n",
        "checkpoint_dir = '/content/drive/MyDrive/ViT-FishID/checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Get current epoch from trainer (assuming your trainer variable is called 'trainer')\n",
        "current_epoch = getattr(trainer, 'current_epoch', 18)  # fallback to 18 if not found\n",
        "best_accuracy = getattr(trainer, 'best_accuracy', 0.0)\n",
        "\n",
        "print(f\"💾 Saving manual checkpoint for epoch {current_epoch}\")\n",
        "print(f\"📊 Current best accuracy: {best_accuracy:.2f}%\")\n",
        "\n",
        "# Create checkpoint dictionary with all necessary information\n",
        "emergency_checkpoint = {\n",
        "    'epoch': current_epoch,\n",
        "    'student_state_dict': trainer.student_model.state_dict(),\n",
        "    'ema_teacher_state_dict': trainer.ema_teacher.teacher.state_dict(),  # Note: .teacher not .teacher_model\n",
        "    'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
        "    'best_accuracy': best_accuracy,\n",
        "    'consistency_weight': trainer.consistency_weight,\n",
        "    'pseudo_label_threshold': trainer.pseudo_label_threshold,\n",
        "    'warmup_epochs': trainer.warmup_epochs,\n",
        "    'ramp_up_epochs': trainer.ramp_up_epochs,\n",
        "    'num_classes': trainer.num_classes,\n",
        "    'timestamp': str(time.time()),\n",
        "    'manual_save': True  # Flag to indicate this was a manual save\n",
        "}\n",
        "\n",
        "# Save the checkpoint\n",
        "checkpoint_filename = f'emergency_checkpoint_epoch_{current_epoch}.pth'\n",
        "checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
        "\n",
        "torch.save(emergency_checkpoint, checkpoint_path)\n",
        "\n",
        "print(f\"✅ Emergency checkpoint saved successfully!\")\n",
        "print(f\"📁 Location: {checkpoint_path}\")\n",
        "print(f\"📏 File size: {os.path.getsize(checkpoint_path) / (1024*1024):.1f} MB\")\n",
        "\n",
        "# Verify the checkpoint can be loaded\n",
        "try:\n",
        "    test_load = torch.load(checkpoint_path, map_location='cpu')\n",
        "    print(f\"✅ Checkpoint verification passed - contains {len(test_load)} keys\")\n",
        "    print(f\"🔍 Saved epoch: {test_load['epoch']}\")\n",
        "    print(f\"🔍 Best accuracy: {test_load['best_accuracy']:.2f}%\")\n",
        "    del test_load  # Free memory\n",
        "except Exception as e:\n",
        "    print(f\"❌ Checkpoint verification failed: {e}\")\n",
        "\n",
        "print(\"\\n🚨 Your training progress is now safely saved!\")\n",
        "print(\"🚨 If Colab times out, you can resume from this checkpoint.\")"
      ],
      "metadata": {
        "id": "tMxE98vEycYo"
      },
      "id": "tMxE98vEycYo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hwGTH1QH1vWd"
      },
      "id": "hwGTH1QH1vWd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b5af5177",
      "metadata": {
        "id": "b5af5177"
      },
      "source": [
        "## 📊 Step 9: Check Training Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87ea96e8",
      "metadata": {
        "id": "87ea96e8"
      },
      "outputs": [],
      "source": [
        "# Check for saved checkpoints\n",
        "import os\n",
        "import glob\n",
        "\n",
        "checkpoint_dir = '/content/ViT-FishID/checkpoints'\n",
        "print(f\"📁 Checking for checkpoints in: {checkpoint_dir}\")\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    checkpoints = glob.glob(os.path.join(checkpoint_dir, '*.pth'))\n",
        "    if checkpoints:\n",
        "        print(f\"\\n✅ Found {len(checkpoints)} checkpoint(s):\")\n",
        "        for checkpoint in sorted(checkpoints):\n",
        "            file_size = os.path.getsize(checkpoint) / (1024**2)  # MB\n",
        "            print(f\"  - {os.path.basename(checkpoint)} ({file_size:.1f} MB)\")\n",
        "\n",
        "        # Check if best model exists\n",
        "        best_model = os.path.join(checkpoint_dir, 'model_best.pth')\n",
        "        if os.path.exists(best_model):\n",
        "            print(f\"\\n🏆 Best model saved: model_best.pth\")\n",
        "    else:\n",
        "        print(\"❌ No checkpoints found\")\n",
        "else:\n",
        "    print(\"❌ Checkpoint directory not found\")\n",
        "\n",
        "# Show training logs summary\n",
        "print(\"\\n📊 Training Summary:\")\n",
        "print(\"Check the training output above for final accuracy metrics\")\n",
        "\n",
        "if TRAINING_CONFIG['use_wandb']:\n",
        "    print(\"\\n📈 For detailed metrics and visualizations, check your W&B dashboard:\")\n",
        "    print(f\"https://wandb.ai/your-username/{TRAINING_CONFIG['wandb_project']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff698a72",
      "metadata": {
        "id": "ff698a72"
      },
      "source": [
        "## 💾 Step 10: Download Model and Results\n",
        "\n",
        "Save your trained model to Google Drive for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89513455",
      "metadata": {
        "id": "89513455"
      },
      "outputs": [],
      "source": [
        "# Copy trained model to Google Drive\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "# Create a timestamped folder in Google Drive\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_dir = f'/content/drive/MyDrive/ViT-FishID_Training_{timestamp}'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "print(f\"💾 Saving results to Google Drive: {save_dir}\")\n",
        "\n",
        "# Copy checkpoints\n",
        "checkpoint_dir = '/content/ViT-FishID/checkpoints'\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    drive_checkpoint_dir = os.path.join(save_dir, 'checkpoints')\n",
        "    shutil.copytree(checkpoint_dir, drive_checkpoint_dir)\n",
        "    print(f\"✅ Checkpoints saved to: {drive_checkpoint_dir}\")\n",
        "\n",
        "# Save training configuration\n",
        "import json\n",
        "config_file = os.path.join(save_dir, 'training_config.json')\n",
        "with open(config_file, 'w') as f:\n",
        "    json.dump(TRAINING_CONFIG, f, indent=2)\n",
        "print(f\"✅ Training config saved to: {config_file}\")\n",
        "\n",
        "# Create a summary file\n",
        "summary_file = os.path.join(save_dir, 'training_summary.txt')\n",
        "with open(summary_file, 'w') as f:\n",
        "    f.write(f\"ViT-FishID Training Summary\\n\")\n",
        "    f.write(f\"========================\\n\\n\")\n",
        "    f.write(f\"Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(f\"Mode: {TRAINING_CONFIG['mode']}\\n\")\n",
        "    f.write(f\"Epochs: {TRAINING_CONFIG['epochs']}\\n\")\n",
        "    f.write(f\"Batch Size: {TRAINING_CONFIG['batch_size']}\\n\")\n",
        "    f.write(f\"Data Directory: {DATA_DIR}\\n\")\n",
        "    f.write(f\"\\nModel Architecture: {TRAINING_CONFIG['model_name']}\\n\")\n",
        "    f.write(f\"Learning Rate: {TRAINING_CONFIG['learning_rate']}\\n\")\n",
        "    f.write(f\"Consistency Weight: {TRAINING_CONFIG['consistency_weight']}\\n\")\n",
        "    f.write(f\"\\nCheckpoints saved in: checkpoints/\\n\")\n",
        "    f.write(f\"Best model: checkpoints/model_best.pth\\n\")\n",
        "\n",
        "print(f\"✅ Training summary saved to: {summary_file}\")\n",
        "\n",
        "print(f\"\\n🎉 All results saved to Google Drive!\")\n",
        "print(f\"📁 Location: {save_dir}\")\n",
        "print(f\"\\n💡 You can now:\")\n",
        "print(f\"   1. Download the checkpoints folder for local use\")\n",
        "print(f\"   2. Use model_best.pth for inference\")\n",
        "print(f\"   3. Continue training from any checkpoint\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bbc6396",
      "metadata": {
        "id": "3bbc6396"
      },
      "source": [
        "## 🧪 Step 11: Quick Model Evaluation (Optional)\n",
        "\n",
        "Test your trained model on a few sample images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "642c1e93",
      "metadata": {
        "id": "642c1e93"
      },
      "outputs": [],
      "source": [
        "# Quick evaluation of the trained model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if best model exists\n",
        "best_model_path = '/content/ViT-FishID/checkpoints/model_best.pth'\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    print(\"🧪 Loading trained model for quick evaluation...\")\n",
        "\n",
        "    # Load model checkpoint info\n",
        "    checkpoint = torch.load(best_model_path, map_location='cpu')\n",
        "\n",
        "    print(f\"📊 Model training info:\")\n",
        "    if 'epoch' in checkpoint:\n",
        "        print(f\"  - Best epoch: {checkpoint['epoch']}\")\n",
        "    if 'best_acc' in checkpoint:\n",
        "        print(f\"  - Best accuracy: {checkpoint['best_acc']:.2f}%\")\n",
        "    if 'teacher_acc' in checkpoint:\n",
        "        print(f\"  - Teacher accuracy: {checkpoint['teacher_acc']:.2f}%\")\n",
        "\n",
        "    # Get class names if available\n",
        "    if 'class_names' in checkpoint:\n",
        "        class_names = checkpoint['class_names']\n",
        "        print(f\"  - Number of classes: {len(class_names)}\")\n",
        "        print(f\"  - Sample classes: {class_names[:5]}...\")\n",
        "\n",
        "    print(\"\\n✅ Model evaluation completed! Check the metrics above.\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ No trained model found. Make sure training completed successfully.\")\n",
        "\n",
        "print(\"\\n💡 For comprehensive evaluation:\")\n",
        "print(\"   Use the evaluate.py script with your test dataset\")\n",
        "print(\"   The test set was automatically created during training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bcf6b4d",
      "metadata": {
        "id": "5bcf6b4d"
      },
      "source": [
        "## 🔧 Troubleshooting\n",
        "\n",
        "### Common Issues and Solutions:\n",
        "\n",
        "**1. GPU Memory Error (CUDA out of memory)**\n",
        "- Reduce batch_size to 8 or 4\n",
        "- Restart runtime and try again\n",
        "\n",
        "**2. Data Not Found**\n",
        "- Check that DATA_DIR path is correct\n",
        "- Ensure data is uploaded to Google Drive\n",
        "- Verify folder structure (labeled/ and unlabeled/)\n",
        "\n",
        "**3. Training Stops Unexpectedly**\n",
        "- Colab sessions timeout after 12 hours\n",
        "- Use runtime management to prevent disconnection\n",
        "- Checkpoints are saved every 10 epochs for resuming\n",
        "\n",
        "**4. Low Accuracy**\n",
        "- Increase epochs (try 75-100)\n",
        "- Adjust consistency_weight (try 1.0-3.0)\n",
        "- Lower pseudo_label_threshold (try 0.5-0.6)\n",
        "\n",
        "**5. Consistency Loss is 0.0000**\n",
        "- Lower pseudo_label_threshold to 0.5\n",
        "- Check that you have unlabeled data\n",
        "- Ensure semi_supervised mode is selected"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d21afb",
      "metadata": {
        "id": "b0d21afb"
      },
      "source": [
        "## 🚀 Next Steps\n",
        "\n",
        "After training is complete, you can:\n",
        "\n",
        "1. **Download your model**: The trained model is saved in Google Drive\n",
        "2. **Continue training**: Resume from checkpoints for more epochs\n",
        "3. **Evaluate performance**: Use the test set for final evaluation\n",
        "4. **Deploy model**: Use the trained model for fish classification\n",
        "5. **Experiment**: Try different hyperparameters or architectures\n",
        "\n",
        "### Model Files Saved:\n",
        "- `model_best.pth`: Best performing model (use this for inference)\n",
        "- `model_latest.pth`: Most recent checkpoint\n",
        "- `model_epoch_XX.pth`: Periodic checkpoints\n",
        "\n",
        "### Performance Expectations:\n",
        "- **50 epochs**: ~70-80% accuracy\n",
        "- **100 epochs**: ~75-85% accuracy\n",
        "- **Semi-supervised**: Should outperform supervised training\n",
        "\n",
        "**Happy fish classification! 🐟🎉**"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}