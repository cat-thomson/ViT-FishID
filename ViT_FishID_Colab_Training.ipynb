{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0e0af9a0",
      "metadata": {
        "id": "0e0af9a0"
      },
      "source": [
        "# ğŸŸ ViT-FishID: Semi-Supervised Fish Classification\n",
        "\n",
        "**COMPLETE TRAINING PIPELINE WITH GOOGLE COLAB**\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/cat-thomson/ViT-FishID/blob/main/ViT_FishID_Colab_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "## ğŸ¯ What This Notebook Does\n",
        "\n",
        "This notebook implements a **complete semi-supervised learning pipeline** for fish species classification using:\n",
        "\n",
        "**ğŸ¤– Vision Transformer (ViT)**: State-of-the-art transformer architecture for image classification\n",
        "**ğŸ“Š Semi-Supervised Learning**: Leverages both labeled and unlabeled fish images\n",
        "**ğŸ“ EMA Teacher-Student Framework**: Uses exponential moving averages for consistency training\n",
        "**â˜ï¸ Google Colab**: Cloud-based training with GPU acceleration\n",
        "\n",
        "## ğŸ“Š Expected Performance\n",
        "\n",
        "- **Training Time**: 4-6 hours for 100 epochs\n",
        "- **GPU Requirements**: T4/V100/A100 (Colab Pro recommended)\n",
        "- **Expected Accuracy**: 80-90% on fish species classification\n",
        "- **Data Efficiency**: Works well with limited labeled data\n",
        "\n",
        "## ğŸ› ï¸ What You Need\n",
        "\n",
        "1. **Fish Dataset**: Labeled and unlabeled fish images (upload to Google Drive)\n",
        "2. **Google Colab Pro**: Recommended for longer training sessions\n",
        "3. **Weights & Biases Account**: Optional for experiment tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26bcb2a3",
      "metadata": {
        "id": "26bcb2a3"
      },
      "source": [
        "## ğŸ”§ Step 1: Environment Setup and GPU Check\n",
        "\n",
        "First, let's verify that we have GPU access and set up the optimal environment for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3540b19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3540b19",
        "outputId": "4859151f-eadc-4319-a3ac-b5f70e71df87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” System Information:\n",
            "Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "GPU Device: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory: 39.6 GB\n",
            "âœ… GPU is ready for training!\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability and system information\n",
        "import torch\n",
        "import os\n",
        "import gc\n",
        "\n",
        "print(\"ğŸ” SYSTEM INFORMATION\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Python version: {os.sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    device_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f\"GPU Device: {device_name}\")\n",
        "    print(f\"GPU Memory: {device_memory:.1f} GB\")\n",
        "    print(\"âœ… GPU is ready for training!\")\n",
        "    \n",
        "    # Set optimal GPU settings\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    \n",
        "    # Clear GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"ğŸš€ GPU optimized for training\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ No GPU detected!\")\n",
        "    print(\"ğŸ“ To enable GPU in Colab:\")\n",
        "    print(\"   Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU\")\n",
        "    print(\"   Then restart this notebook\")\n",
        "\n",
        "# Set device for later use\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nğŸ¯ Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149f671b",
      "metadata": {
        "id": "149f671b"
      },
      "source": [
        "## ğŸ“ Step 2: Mount Google Drive\n",
        "\n",
        "This will give us access to your fish dataset stored in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4abb3ffd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4abb3ffd",
        "outputId": "29fb191e-611d-4f2d-a4d5-69fec2c1d936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "ğŸ“‚ Google Drive contents:\n",
            "  - Mock Matric\n",
            "  - Photos\n",
            "  - Admin\n",
            "  - Uni\n",
            "  - Fish_Training_Output\n",
            "  - Colab Notebooks\n",
            "  - ViT-FishID\n",
            "  - fish_cutouts.zip\n",
            "  - ViT-FishID_Training_20250814_154652\n",
            "  - ViT-FishID_Training_20250814_202307\n",
            "  ... and 3 more items\n",
            "\n",
            "âœ… Google Drive mounted successfully!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# List contents to verify mount\n",
        "print(\"\\nğŸ“‚ Google Drive contents:\")\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "if os.path.exists(drive_path):\n",
        "    items = os.listdir(drive_path)[:10]  # Show first 10 items\n",
        "    for item in items:\n",
        "        print(f\"  - {item}\")\n",
        "    if len(os.listdir(drive_path)) > 10:\n",
        "        print(f\"  ... and {len(os.listdir(drive_path)) - 10} more items\")\n",
        "    print(\"\\nâœ… Google Drive mounted successfully!\")\n",
        "else:\n",
        "    print(\"âŒ Failed to mount Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be8b6273",
      "metadata": {
        "id": "be8b6273"
      },
      "source": [
        "## ğŸ“¦ Step 3: Install Dependencies\n",
        "\n",
        "Installing all required packages for ViT-FishID training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8c724abc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c724abc",
        "outputId": "77da503f-48d7-49ca-dc48-111168649d45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¦ Installing dependencies...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… All dependencies installed successfully!\n",
            "\n",
            "ğŸ“‹ Package versions:\n",
            "  - torch: 2.6.0+cu124\n",
            "  - torchvision: 0.21.0+cu124\n",
            "  - timm: 1.0.19\n",
            "  - albumentations: 2.0.8\n",
            "  - opencv: 4.12.0\n",
            "  - sklearn: 1.6.1\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "print(\"ğŸ“¦ Installing dependencies...\")\n",
        "\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q timm transformers\n",
        "!pip install -q albumentations\n",
        "!pip install -q wandb\n",
        "!pip install -q opencv-python-headless\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q matplotlib seaborn\n",
        "!pip install -q tqdm\n",
        "\n",
        "print(\"âœ… All dependencies installed successfully!\")\n",
        "\n",
        "# Verify installations\n",
        "import torch\n",
        "import torchvision\n",
        "import timm\n",
        "import albumentations\n",
        "import cv2\n",
        "import sklearn\n",
        "\n",
        "print(\"\\nğŸ“‹ Package versions:\")\n",
        "print(f\"  - torch: {torch.__version__}\")\n",
        "print(f\"  - torchvision: {torchvision.__version__}\")\n",
        "print(f\"  - timm: {timm.__version__}\")\n",
        "print(f\"  - albumentations: {albumentations.__version__}\")\n",
        "print(f\"  - opencv: {cv2.__version__}\")\n",
        "print(f\"  - sklearn: {sklearn.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b795fc",
      "metadata": {
        "id": "12b795fc"
      },
      "source": [
        "## ğŸ”„ Step 4: Clone ViT-FishID Repository\n",
        "\n",
        "Getting the latest code from your GitHub repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c4e4cd45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4e4cd45",
        "outputId": "8efc760e-aea2-48bd-f684-8f9d338697e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¥ Cloning ViT-FishID repository...\n",
            "Cloning into '/content/ViT-FishID'...\n",
            "remote: Enumerating objects: 119, done.\u001b[K\n",
            "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 119 (delta 44), reused 98 (delta 27), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (119/119), 201.94 KiB | 20.19 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n",
            "/content/ViT-FishID\n",
            "\n",
            "ğŸ“‚ Project structure:\n",
            "total 360\n",
            "drwxr-xr-x 4 root root   4096 Aug 15 06:58 .\n",
            "drwxr-xr-x 1 root root   4096 Aug 15 06:58 ..\n",
            "-rw-r--r-- 1 root root  21217 Aug 15 06:58 data.py\n",
            "-rw-r--r-- 1 root root  11572 Aug 15 06:58 evaluate.py\n",
            "-rw-r--r-- 1 root root   3328 Aug 15 06:58 EXTENDED_TRAINING_SETUP.md\n",
            "drwxr-xr-x 2 root root   4096 Aug 15 06:58 fish_cutouts\n",
            "drwxr-xr-x 8 root root   4096 Aug 15 06:58 .git\n",
            "-rw-r--r-- 1 root root     66 Aug 15 06:58 .gitattributes\n",
            "-rw-r--r-- 1 root root    646 Aug 15 06:58 .gitignore\n",
            "-rw-r--r-- 1 root root   9495 Aug 15 06:58 model.py\n",
            "-rw-r--r-- 1 root root  16771 Aug 15 06:58 pipeline.py\n",
            "-rw-r--r-- 1 root root  16566 Aug 15 06:58 README.md\n",
            "-rw-r--r-- 1 root root    202 Aug 15 06:58 requirements.txt\n",
            "-rw-r--r-- 1 root root   4265 Aug 15 06:58 resume_training.py\n",
            "-rw-r--r-- 1 root root   5134 Aug 15 06:58 species_mapping.txt\n",
            "-rw-r--r-- 1 root root  25497 Aug 15 06:58 trainer.py\n",
            "-rw-r--r-- 1 root root   4982 Aug 15 06:58 TRAINING_FIXES_APPLIED.md\n",
            "-rw-r--r-- 1 root root  15331 Aug 15 06:58 train.py\n",
            "-rw-r--r-- 1 root root   8818 Aug 15 06:58 utils.py\n",
            "-rw-r--r-- 1 root root 160971 Aug 15 06:58 ViT_FishID_Colab_Training.ipynb\n",
            "\n",
            "âœ… Repository cloned successfully!\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "import os\n",
        "\n",
        "# Remove existing directory if it exists\n",
        "if os.path.exists('/content/ViT-FishID'):\n",
        "    !rm -rf /content/ViT-FishID\n",
        "\n",
        "# Clone the repository\n",
        "print(\"ğŸ“¥ Cloning ViT-FishID repository...\")\n",
        "!git clone https://github.com/cat-thomson/ViT-FishID.git /content/ViT-FishID\n",
        "\n",
        "# Change to project directory\n",
        "%cd /content/ViT-FishID\n",
        "\n",
        "# List project files\n",
        "print(\"\\nğŸ“‚ Project structure:\")\n",
        "!ls -la\n",
        "\n",
        "print(\"\\nâœ… Repository cloned successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8155c400",
      "metadata": {
        "id": "8155c400"
      },
      "source": [
        "## ğŸ  Step 5: Setup Fish Dataset\n",
        "\n",
        "**Important**: Upload your `fish_cutouts.zip` file to Google Drive before running this step.\n",
        "\n",
        "Expected dataset structure:\n",
        "```\n",
        "fish_cutouts/\n",
        "â”œâ”€â”€ labeled/\n",
        "â”‚   â”œâ”€â”€ species_1/\n",
        "â”‚   â”‚   â”œâ”€â”€ fish_001.jpg\n",
        "â”‚   â”‚   â””â”€â”€ fish_002.jpg\n",
        "â”‚   â””â”€â”€ species_2/\n",
        "â”‚       â””â”€â”€ ...\n",
        "â””â”€â”€ unlabeled/\n",
        "    â”œâ”€â”€ fish_003.jpg\n",
        "    â””â”€â”€ fish_004.jpg\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nre5_INaKDXl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nre5_INaKDXl",
        "outputId": "c9c02e13-e2c0-4c0b-802a-0f0111fd50b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ—‚ï¸ SETTING UP FISH DATASET - CORRECTED PATHS\n",
            "==================================================\n",
            "ğŸ¯ ZIP file location: /content/drive/MyDrive/fish_cutouts.zip\n",
            "ğŸ¯ Target data directory: /content/fish_cutouts\n",
            "ğŸ“¥ Data not found locally, extracting from Google Drive...\n",
            "âœ… Found ZIP file at: /content/drive/MyDrive/fish_cutouts.zip\n",
            "ğŸ“ ZIP file size: 216.5 MB\n",
            "ğŸ“¦ Extracting fish_cutouts.zip...\n",
            "âœ… ZIP extraction completed\n",
            "ğŸ“ Found in ZIP: ['dataset_info.json', '__MACOSX', 'labeled', 'unlabeled']\n",
            "ğŸ“„ Found dataset info: dataset_info.json\n",
            "âœ… Found labeled directory: labeled\n",
            "âœ… Found unlabeled directory: unlabeled\n",
            "âœ… Data organized at: /content/fish_cutouts\n",
            "ğŸ“„ Copied dataset_info.json\n",
            "ğŸŸ Verified: 37 species in labeled data\n",
            "ğŸ“Š Verified: 24015 images in unlabeled data\n",
            "\n",
            "âœ… DATASET READY\n",
            "ğŸ“ Location: /content/fish_cutouts\n",
            "  ğŸ“‚ labeled/: 37 species folders\n",
            "  ğŸ“‚ unlabeled/: 24015 images\n",
            "  ğŸ“„ dataset_info.json: Available\n",
            "ğŸš€ Ready to proceed with training!\n"
          ]
        }
      ],
      "source": [
        "# Setup fish dataset from Google Drive\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print(\"ğŸ  SETTING UP FISH DATASET\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Configuration\n",
        "ZIP_FILE_PATH = '/content/drive/MyDrive/fish_cutouts.zip'\n",
        "DATA_DIR = '/content/fish_cutouts'\n",
        "\n",
        "print(f\"ğŸ“‚ Looking for dataset: {ZIP_FILE_PATH}\")\n",
        "print(f\"ğŸ¯ Target directory: {DATA_DIR}\")\n",
        "\n",
        "# Check if data already exists locally\n",
        "if os.path.exists(DATA_DIR) and os.path.exists(os.path.join(DATA_DIR, 'labeled')):\n",
        "    print(\"âœ… Dataset already available locally!\")\n",
        "    \n",
        "    # Quick validation\n",
        "    labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
        "    unlabeled_dir = os.path.join(DATA_DIR, 'unlabeled')\n",
        "    \n",
        "    if os.path.exists(labeled_dir):\n",
        "        species_count = len([d for d in os.listdir(labeled_dir) \n",
        "                           if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')])\n",
        "        print(f\"ğŸŸ Found {species_count} labeled species\")\n",
        "    \n",
        "    if os.path.exists(unlabeled_dir):\n",
        "        unlabeled_count = len([f for f in os.listdir(unlabeled_dir) \n",
        "                             if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"ğŸ“Š Found {unlabeled_count} unlabeled images\")\n",
        "\n",
        "else:\n",
        "    print(\"ğŸ“¥ Extracting dataset from Google Drive...\")\n",
        "    \n",
        "    # Check if ZIP file exists\n",
        "    if not os.path.exists(ZIP_FILE_PATH):\n",
        "        print(f\"âŒ Dataset not found at: {ZIP_FILE_PATH}\")\n",
        "        print(\"ğŸ“ Please upload fish_cutouts.zip to Google Drive root directory\")\n",
        "    else:\n",
        "        print(f\"âœ… Found dataset: {os.path.getsize(ZIP_FILE_PATH) / (1024**2):.1f} MB\")\n",
        "        \n",
        "        try:\n",
        "            # Extract to temporary directory\n",
        "            temp_dir = '/content/temp_extract'\n",
        "            if os.path.exists(temp_dir):\n",
        "                shutil.rmtree(temp_dir)\n",
        "            \n",
        "            with zipfile.ZipFile(ZIP_FILE_PATH, 'r') as zip_ref:\n",
        "                zip_ref.extractall(temp_dir)\n",
        "            \n",
        "            # Find and organize data\n",
        "            extracted_items = os.listdir(temp_dir)\n",
        "            print(f\"ğŸ“ Extracted: {extracted_items}\")\n",
        "            \n",
        "            # Look for labeled and unlabeled directories\n",
        "            labeled_source = None\n",
        "            unlabeled_source = None\n",
        "            \n",
        "            for item in extracted_items:\n",
        "                item_path = os.path.join(temp_dir, item)\n",
        "                if item == 'labeled' and os.path.isdir(item_path):\n",
        "                    labeled_source = item_path\n",
        "                elif item == 'unlabeled' and os.path.isdir(item_path):\n",
        "                    unlabeled_source = item_path\n",
        "            \n",
        "            if labeled_source and unlabeled_source:\n",
        "                # Create target directory\n",
        "                if os.path.exists(DATA_DIR):\n",
        "                    shutil.rmtree(DATA_DIR)\n",
        "                os.makedirs(DATA_DIR)\n",
        "                \n",
        "                # Move directories\n",
        "                shutil.move(labeled_source, os.path.join(DATA_DIR, 'labeled'))\n",
        "                shutil.move(unlabeled_source, os.path.join(DATA_DIR, 'unlabeled'))\n",
        "                \n",
        "                print(\"âœ… Dataset organized successfully!\")\n",
        "                \n",
        "                # Verify structure\n",
        "                labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
        "                species_count = len([d for d in os.listdir(labeled_dir) \n",
        "                                   if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')])\n",
        "                \n",
        "                unlabeled_dir = os.path.join(DATA_DIR, 'unlabeled')\n",
        "                unlabeled_count = len([f for f in os.listdir(unlabeled_dir) \n",
        "                                     if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                \n",
        "                print(f\"ğŸŸ Verified: {species_count} species\")\n",
        "                print(f\"ğŸ“Š Verified: {unlabeled_count} unlabeled images\")\n",
        "                \n",
        "            else:\n",
        "                print(\"âŒ Could not find labeled and unlabeled directories\")\n",
        "            \n",
        "            # Cleanup\n",
        "            if os.path.exists(temp_dir):\n",
        "                shutil.rmtree(temp_dir)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error extracting dataset: {e}\")\n",
        "\n",
        "# Final verification\n",
        "if os.path.exists(DATA_DIR):\n",
        "    print(f\"\\nâœ… DATASET READY\")\n",
        "    print(f\"ğŸ“ Location: {DATA_DIR}\")\n",
        "    print(\"ğŸš€ Ready for training!\")\n",
        "else:\n",
        "    print(f\"\\nâŒ DATASET SETUP FAILED\")\n",
        "    print(\"Please check that fish_cutouts.zip is uploaded to Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f0fe32",
      "metadata": {
        "id": "31f0fe32"
      },
      "source": [
        "## ğŸ“ˆ Step 6: Setup Weights & Biases (Optional)\n",
        "\n",
        "Weights & Biases provides excellent training visualization and experiment tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab343772",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "ab343772",
        "outputId": "b6d9c2df-bbb0-46ae-ca3b-537ed8e98191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ˆ Connecting to Weights & Biases...\n",
            "ğŸ”‘ Please enter your W&B API key when prompted.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcativthomson\u001b[0m (\u001b[33mcativthomson-university-of-cape-town\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Successfully logged in to W&B.\n",
            "âŒ W&B connection not established. Logging may be disabled.\n"
          ]
        }
      ],
      "source": [
        "# Login to Weights & Biases for experiment tracking\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "print(\"ğŸ“ˆ SETTING UP WEIGHTS & BIASES\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Check if API key is available\n",
        "if os.environ.get(\"WANDB_API_KEY\"):\n",
        "    print(\"âœ… W&B API key found in environment\")\n",
        "    try:\n",
        "        wandb.login(relogin=True)\n",
        "        print(\"âœ… Successfully logged in to W&B\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ W&B relogin failed: {e}\")\n",
        "        print(\"Trying manual login...\")\n",
        "        wandb.login()\n",
        "else:\n",
        "    print(\"ğŸ”‘ Please enter your W&B API key when prompted\")\n",
        "    print(\"ğŸ’¡ Get your API key from: https://wandb.ai/settings\")\n",
        "    try:\n",
        "        wandb.login()\n",
        "        print(\"âœ… Successfully logged in to W&B\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ W&B login failed: {e}\")\n",
        "        print(\"Continuing without W&B logging...\")\n",
        "\n",
        "# Check connection status\n",
        "if wandb.run:\n",
        "    print(f\"ğŸš€ W&B Run URL: {wandb.run.url}\")\n",
        "    USE_WANDB = True\n",
        "else:\n",
        "    print(\"ğŸ“Š W&B not connected - training will continue without logging\")\n",
        "    USE_WANDB = False\n",
        "\n",
        "print(f\"âœ… W&B setup complete (Enabled: {USE_WANDB})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5190f01",
      "metadata": {
        "id": "b5190f01"
      },
      "source": [
        "## ğŸ”„ Step 6: Locate Checkpoint from Epoch 19\n",
        "\n",
        "Finding your saved checkpoint to resume training from where you left off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "61b35ced",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61b35ced",
        "outputId": "80d1b4e8-edbd-4fe9-c8a5-ffc25514190a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Looking for checkpoint from epoch 100...\n",
            "ğŸ“ Checking: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "ğŸ¯ Found candidate: checkpoint_epoch_100.pth\n",
            "âœ… FOUND EPOCH 100 CHECKPOINT!\n",
            "ğŸ“ Location: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "ğŸ“Š Epoch: 100\n",
            "ğŸ“Š Best accuracy so far: 87.56%\n",
            "ğŸ“ Checking: /content/drive/MyDrive/ViT-FishID/checkpoints_backup\n",
            "ğŸ¯ Found candidate: checkpoint_epoch_100.pth\n",
            "âœ… FOUND EPOCH 100 CHECKPOINT!\n",
            "ğŸ“ Location: /content/drive/MyDrive/ViT-FishID/checkpoints_backup/checkpoint_epoch_100.pth\n",
            "ğŸ“Š Epoch: 100\n",
            "ğŸ“Š Best accuracy so far: 87.56%\n",
            "\n",
            "ğŸ‰ Checkpoint ready for resuming training!\n",
            "ğŸ“„ File: checkpoint_epoch_100.pth\n",
            "ğŸ“ Size: 982.4 MB\n",
            "ğŸ’¾ New checkpoints will be saved to: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n"
          ]
        }
      ],
      "source": [
        "# Locate checkpoint from epoch 19\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "\n",
        "print(\"ğŸ” Looking for checkpoint from epoch 100...\")\n",
        "\n",
        "# Possible checkpoint locations\n",
        "checkpoint_locations = [\n",
        "    '/content/drive/MyDrive/ViT-FishID/checkpoints_extended', '/content/drive/MyDrive/ViT-FishID/checkpoints_backup'\n",
        "]\n",
        "\n",
        "checkpoint_path = None\n",
        "checkpoint_info = None\n",
        "\n",
        "# Search for epoch 19 checkpoint\n",
        "for location_pattern in checkpoint_locations:\n",
        "    for location in glob.glob(location_pattern):\n",
        "        if os.path.exists(location):\n",
        "            print(f\"ğŸ“ Checking: {location}\")\n",
        "\n",
        "            # Look for epoch 19 specifically\n",
        "            epoch_100_files = glob.glob(os.path.join(location, '*epoch_100*'))\n",
        "            manual_files = glob.glob(os.path.join(location, '*manual*epoch*100*'))\n",
        "            emergency_files = glob.glob(os.path.join(location, '*emergency*epoch*100*'))\n",
        "\n",
        "            all_candidates = epoch_100_files + manual_files + emergency_files\n",
        "\n",
        "            for candidate in all_candidates:\n",
        "                if candidate.endswith('.pth'):\n",
        "                    print(f\"ğŸ¯ Found candidate: {os.path.basename(candidate)}\")\n",
        "                    try:\n",
        "                        # Verify checkpoint can be loaded\n",
        "                        test_checkpoint = torch.load(candidate, map_location='cpu')\n",
        "                        epoch = test_checkpoint.get('epoch', 'unknown')\n",
        "\n",
        "                        if epoch == 100 or '100' in os.path.basename(candidate):\n",
        "                            checkpoint_path = candidate\n",
        "                            checkpoint_info = test_checkpoint\n",
        "                            print(f\"âœ… FOUND EPOCH 100 CHECKPOINT!\")\n",
        "                            print(f\"ğŸ“ Location: {checkpoint_path}\")\n",
        "                            print(f\"ğŸ“Š Epoch: {epoch}\")\n",
        "\n",
        "                            if 'best_accuracy' in test_checkpoint:\n",
        "                                print(f\"ğŸ“Š Best accuracy so far: {test_checkpoint['best_accuracy']:.2f}%\")\n",
        "                            elif 'best_acc' in test_checkpoint:\n",
        "                                print(f\"ğŸ“Š Best accuracy so far: {test_checkpoint['best_acc']:.2f}%\")\n",
        "\n",
        "                            break\n",
        "                    except Exception as e:\n",
        "                        print(f\"âš ï¸ Could not load {candidate}: {e}\")\n",
        "\n",
        "            if checkpoint_path:\n",
        "                break\n",
        "\n",
        "        if checkpoint_path:\n",
        "            break\n",
        "\n",
        "if checkpoint_path:\n",
        "    print(f\"\\nğŸ‰ Checkpoint ready for resuming training!\")\n",
        "    print(f\"ğŸ“„ File: {os.path.basename(checkpoint_path)}\")\n",
        "    print(f\"ğŸ“ Size: {os.path.getsize(checkpoint_path) / (1024*1024):.1f} MB\")\n",
        "\n",
        "    # Set up checkpoint directory for new saves\n",
        "    checkpoint_save_dir = '/content/drive/MyDrive/ViT-FishID/checkpoints_extended'\n",
        "    os.makedirs(checkpoint_save_dir, exist_ok=True)\n",
        "    print(f\"ğŸ’¾ New checkpoints will be saved to: {checkpoint_save_dir}\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ No checkpoint found for epoch 19!\")\n",
        "    print(\"\\nğŸ”§ Troubleshooting:\")\n",
        "    print(\"1. Check that you have a checkpoint saved from previous training\")\n",
        "    print(\"2. Ensure the checkpoint is uploaded to Google Drive\")\n",
        "    print(\"3. Look for files named like: checkpoint_epoch_19.pth, emergency_checkpoint_epoch_19.pth\")\n",
        "    print(\"\\nğŸ“ Checked locations:\")\n",
        "    for location in checkpoint_locations:\n",
        "        print(f\"  - {location}\")\n",
        "\n",
        "    # Fallback: look for any checkpoints\n",
        "    print(\"\\nğŸ” All available checkpoints:\")\n",
        "    for location_pattern in checkpoint_locations:\n",
        "        for location in glob.glob(location_pattern):\n",
        "            if os.path.exists(location):\n",
        "                all_checkpoints = glob.glob(os.path.join(location, '*.pth'))\n",
        "                for cp in all_checkpoints:\n",
        "                    print(f\"  - {os.path.basename(cp)}\")\n",
        "\n",
        "# Store checkpoint path for later use\n",
        "RESUME_CHECKPOINT = checkpoint_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fe6af6d",
      "metadata": {
        "id": "0fe6af6d"
      },
      "source": [
        "## âš™ï¸ Step 7: Configure Training Parameters\n",
        "\n",
        "Configure the training settings for your semi-supervised fish classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hSokV6NDjgYa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSokV6NDjgYa",
        "outputId": "8c7b1c32-b0db-4c59-fea7-411ce6f2574d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ EXTENDED TRAINING CONFIGURATION - WITH W&B\n",
            "==================================================\n",
            "ğŸ“Š Detected 37 fish species\n",
            "\n",
            "EXTENDED TRAINING CONFIGURATION SUMMARY\n",
            "==================================================\n",
            "ğŸ“Š Resume from: Epoch 100\n",
            "ğŸ“Š Target epochs: 100\n",
            "ğŸ“Š Remaining epochs: 1\n",
            "â±ï¸ Estimated time: 5-7 minutes\n",
            "ğŸ“Š Batch size: 16 (optimized for Colab Pro)\n",
            "ğŸ’¾ Checkpoint saves: EVERY 1 epoch(s)\n",
            "ğŸ“Š Mode: semi_supervised with consistency weight 2.0\n",
            "ğŸ“Š Logging: W&B Enabled (Project: ViT-FishID-Extended-Training, Run: resume-epoch-6-to-100)\n",
            "ğŸ“Š Num Classes: 37\n",
            "\n",
            "SETTING UP CHECKPOINT DIRECTORIES\n",
            "==================================================\n",
            "ğŸ“ Primary saves: /content/drive/MyDrive/ViT-FishID/checkpoints_extended (Created/Exists)\n",
            "ğŸ’¾ Backup saves: /content/drive/MyDrive/ViT-FishID/checkpoints_backup (Created/Exists)\n",
            "\n",
            "âœ… Will resume training from: checkpoint_epoch_100.pth\n",
            "\n",
            "ğŸš€ Configuration complete. Ready to resume/start training!\n"
          ]
        }
      ],
      "source": [
        "# Training Configuration for Semi-Supervised Fish Classification\n",
        "import os\n",
        "\n",
        "print(\"âš™ï¸ TRAINING CONFIGURATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Auto-detect number of species from dataset\n",
        "NUM_CLASSES = 37  # Default\n",
        "if 'DATA_DIR' in globals() and os.path.exists(DATA_DIR):\n",
        "    labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
        "    if os.path.exists(labeled_dir):\n",
        "        species_count = len([d for d in os.listdir(labeled_dir) \n",
        "                           if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')])\n",
        "        NUM_CLASSES = species_count\n",
        "        print(f\"ğŸ“Š Auto-detected {species_count} fish species\")\n",
        "\n",
        "# Create checkpoint directories\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/ViT-FishID/checkpoints'\n",
        "BACKUP_DIR = '/content/drive/MyDrive/ViT-FishID/checkpoints_backup'\n",
        "\n",
        "try:\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "    os.makedirs(BACKUP_DIR, exist_ok=True)\n",
        "    print(f\"ğŸ“ Checkpoints: {CHECKPOINT_DIR}\")\n",
        "    print(f\"ğŸ’¾ Backups: {BACKUP_DIR}\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Could not create Google Drive directories: {e}\")\n",
        "    CHECKPOINT_DIR = '/content/checkpoints'\n",
        "    BACKUP_DIR = '/content/checkpoints_backup'\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "    os.makedirs(BACKUP_DIR, exist_ok=True)\n",
        "    print(f\"ğŸ“ Using local checkpoints: {CHECKPOINT_DIR}\")\n",
        "\n",
        "# Training Configuration\n",
        "TRAINING_CONFIG = {\n",
        "    # BASIC SETTINGS\n",
        "    'mode': 'semi_supervised',\n",
        "    'data_dir': DATA_DIR if 'DATA_DIR' in globals() else '/content/fish_cutouts',\n",
        "    'epochs': 100,\n",
        "    'batch_size': 16,\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 0.05,\n",
        "    \n",
        "    # MODEL SETTINGS\n",
        "    'model_name': 'vit_base_patch16_224',\n",
        "    'num_classes': NUM_CLASSES,\n",
        "    'pretrained': True,\n",
        "    \n",
        "    # SEMI-SUPERVISED SETTINGS\n",
        "    'consistency_weight': 2.0,\n",
        "    'pseudo_label_threshold': 0.7,\n",
        "    'temperature': 4.0,\n",
        "    'warmup_epochs': 10,\n",
        "    'ramp_up_epochs': 30,\n",
        "    \n",
        "    # CHECKPOINT SETTINGS\n",
        "    'save_frequency': 10,  # Save every 10 epochs\n",
        "    'checkpoint_dir': CHECKPOINT_DIR,\n",
        "    'backup_dir': BACKUP_DIR,\n",
        "    \n",
        "    # LOGGING SETTINGS\n",
        "    'use_wandb': USE_WANDB if 'USE_WANDB' in globals() else False,\n",
        "    'wandb_project': 'ViT-FishID-Training',\n",
        "    'wandb_run_name': f'fish-classification-{NUM_CLASSES}-classes',\n",
        "}\n",
        "\n",
        "print(\"\\nğŸ“‹ TRAINING CONFIGURATION\")\n",
        "print(\"=\"*50)\n",
        "print(f\"ğŸ¯ Training mode: {TRAINING_CONFIG['mode']}\")\n",
        "print(f\"ğŸ“Š Total epochs: {TRAINING_CONFIG['epochs']}\")\n",
        "print(f\"ğŸ“¦ Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
        "print(f\"ğŸ§  Model: {TRAINING_CONFIG['model_name']}\")\n",
        "print(f\"ğŸŸ Number of species: {TRAINING_CONFIG['num_classes']}\")\n",
        "print(f\"âš–ï¸ Consistency weight: {TRAINING_CONFIG['consistency_weight']}\")\n",
        "print(f\"ğŸ¯ Pseudo-label threshold: {TRAINING_CONFIG['pseudo_label_threshold']}\")\n",
        "print(f\"ğŸ’¾ Save frequency: Every {TRAINING_CONFIG['save_frequency']} epochs\")\n",
        "print(f\"ğŸ“ˆ W&B logging: {TRAINING_CONFIG['use_wandb']}\")\n",
        "\n",
        "# Time estimation\n",
        "estimated_time_hours = TRAINING_CONFIG['epochs'] * 3 / 60  # ~3 minutes per epoch\n",
        "print(f\"\\nâ±ï¸ Estimated training time: {estimated_time_hours:.1f} hours\")\n",
        "print(f\"ğŸ’¡ Recommendation: Use Colab Pro for longer training sessions\")\n",
        "\n",
        "print(\"\\nâœ… Configuration complete - ready to start training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa9cbd50",
      "metadata": {
        "id": "aa9cbd50"
      },
      "source": [
        "## ğŸš€ Step 8: Start Semi-Supervised Training\n",
        "\n",
        "This cell will start the complete training process. Expected time: 4-6 hours for 100 epochs.\n",
        "\n",
        "**Training Process:**\n",
        "1. **Supervised Learning**: Uses labeled fish images with ground truth\n",
        "2. **Semi-Supervised Learning**: Leverages unlabeled images with pseudo-labels\n",
        "3. **EMA Teacher-Student**: Uses exponential moving average for consistency\n",
        "4. **Automatic Checkpointing**: Saves progress every 10 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "njLKb7xaepxo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njLKb7xaepxo",
        "outputId": "2404e35d-a058-4349-bafa-18e98981bb07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ STARTING EXTENDED TRAINING SESSION\n",
            "============================================================\n",
            "ğŸ“‚ Resuming from: checkpoint_epoch_99.pth\n",
            "ğŸš€ Starting training from epoch: 100\n",
            "ğŸ“Š Training for 1 more epochs...\n",
            "ğŸ¯ Target: 100 total epochs\n",
            "â±ï¸ Estimated time: 4-6 minutes\n",
            "ğŸ’¾ Checkpoints saved to: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "\n",
            "ğŸ“‹ Extended Training Command:\n",
            "python train.py \n",
            "    --mode semi_supervised \n",
            "    --data_dir /content/fish_cutouts \n",
            "    --epochs 100 \n",
            "    --batch_size 16 \n",
            "    --learning_rate 0.0001 \n",
            "    --weight_decay 0.05 \n",
            "    --model_name vit_base_patch16_224 \n",
            "    --consistency_weight 2.0 \n",
            "    --pseudo_label_threshold 0.7 \n",
            "    --temperature 4.0 \n",
            "    --warmup_epochs 5 \n",
            "    --ramp_up_epochs 15 \n",
            "    --save_dir /content/drive/MyDrive/ViT-FishID/checkpoints_extended \n",
            "    --save_frequency 1 \n",
            "    --resume_from /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_99.pth \n",
            "    --use_wandb \n",
            "    --pretrained\n",
            "\n",
            "============================================================\n",
            "ğŸ¬ TRAINING STARTED - EPOCH 100 TO 100\n",
            "â° Started at: 2025-08-15 07:03:08\n",
            "âœ… Commented out line saving ema_teacher state_dict: 'ema_teacher_state_dict': trainer.ema_teacher.teacher.state_dict(),  # Fixed key name\n",
            "âœ… Modified /content/ViT-FishID/trainer.py to skip saving EMA teacher state_dict.\n",
            "2025-08-15 07:03:16.032238: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-15 07:03:16.049070: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755241396.070136    2955 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755241396.076517    2955 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755241396.092669    2955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755241396.092697    2955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755241396.092700    2955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755241396.092703    2955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-15 07:03:16.097419: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Random seed set to 42\n",
            "Using GPU: NVIDIA A100-SXM4-40GB\n",
            "ğŸŸ ViT-FishID Training\n",
            "ğŸ“Š Mode: semi_supervised\n",
            "ğŸ–¥ï¸  Device: cuda\n",
            "ğŸ“ Data directory: /content/fish_cutouts\n",
            "\n",
            "ğŸ“¦ Creating data loaders...\n",
            "âš ï¸  Warning: Some classes have only 1 sample(s). Using random splitting instead of stratified.\n",
            "   Classes with 1 sample: ['Carangidae_Caranx_heberi', 'Serranidae_Lipropoma_spp1', 'Sparidae_Sparodon_durbanesis']\n",
            "/content/ViT-FishID/data.py:229: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
            "ğŸ“Š Dataset initialized:\n",
            "  - Labeled samples: 3,084\n",
            "  - Unlabeled samples: 6,168\n",
            "  - Total samples per epoch: 9,252\n",
            "ğŸ“Š Semi-supervised data loaders created:\n",
            "  - Train labeled: 3,084\n",
            "  - Train unlabeled: 6,168\n",
            "  - Val samples: 1,029\n",
            "  - Test samples: 1,029\n",
            "  - Classes: 37\n",
            "  - Split ratios: Train=60.0%, Val=20.0%, Test=20.0%\n",
            "ğŸ·ï¸  Classes (37): ['Carangidae_Caranx_heberi', 'Carangidae_Pseudocaranx_dentex', 'Carangidae_Seriola_dumerili', 'Carangidae_Seriola_lalandi', 'Carangidae_Seriola_rivoliana', 'Carangidae_Trachurus_delagoa', 'Serranidae_Aulacocephalus_temminckii', 'Serranidae_Epinephelus_andersoni', 'Serranidae_Epinephelus_marginatus', 'Serranidae_Epinephelus_rivulatus', 'Serranidae_Epinephelus_tukula', 'Serranidae_Lipropoma_spp1', 'Serranidae_Serranus_knysnaensis', 'Sparidae_Argyrops_spinifer', 'Sparidae_Boopsoidea_inornata', 'Sparidae_Cheimerius_nufar', 'Sparidae_Chrysoblephus_anglicus', 'Sparidae_Chrysoblephus_cristiceps', 'Sparidae_Chrysoblephus_lophus', 'Sparidae_Chrysoblephus_puniceus', 'Sparidae_Cymatoceps_nasutus', 'Sparidae_Diplodus_capensis', 'Sparidae_Diplodus_hottentotus', 'Sparidae_Pachymetopon_aeneum', 'Sparidae_Pachymetopon_grande', 'Sparidae_Pagellus_bellottii_natalensis', 'Sparidae_Petrus_rupestris', 'Sparidae_Polyamblydon_germanum', 'Sparidae_Polysteganus_praeorbitalis', 'Sparidae_Polysteganus_undulosus', 'Sparidae_Porcostoma_dentata', 'Sparidae_Rhabdosargus_holubi', 'Sparidae_Rhabdosargus_sarba', 'Sparidae_Rhabdosargus_thorpei', 'Sparidae_Sarpa_salpa', 'Sparidae_Sparodon_durbanesis', 'Sparidae_Spondyliosoma_emarginatum']\n",
            "ğŸ“Š Test set available with 1,029 samples for final evaluation\n",
            "\n",
            "ğŸ§  Creating ViT model: vit_base_patch16_224\n",
            "model.safetensors: 100% 346M/346M [00:00<00:00, 479MB/s]\n",
            "âœ… EMA Teacher initialized with momentum: 0.999\n",
            "ğŸ“Š Model parameters: 85,828,645\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcativthomson\u001b[0m (\u001b[33mcativthomson-university-of-cape-town\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/ViT-FishID/wandb/run-20250815_070324-ogt296e8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msemi_supervised_vit_base_patch16_224_20250815_070323\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cativthomson-university-of-cape-town/vit-fish-id\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cativthomson-university-of-cape-town/vit-fish-id/runs/ogt296e8\u001b[0m\n",
            "âœ… W&B initialized: vit-fish-id/semi_supervised_vit_base_patch16_224_20250815_070323\n",
            "\n",
            "ğŸš€ Creating trainer...\n",
            "âœ… Semi-Supervised Trainer initialized\n",
            "  - Consistency weight: 2.0\n",
            "  - Pseudo-label threshold: 0.7\n",
            "  - Learning rate: 0.0001\n",
            "  - Warmup epochs: 5\n",
            "  - Ramp-up epochs: 15\n",
            "ğŸ“¥ Resuming from checkpoint: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_99.pth\n",
            "âœ… Successfully loaded checkpoint from epoch 99\n",
            "ğŸ“Š Previous best accuracy: 87.56073858114675\n",
            "ğŸš€ Resuming training from epoch 100\n",
            "\n",
            "ğŸ¯ Starting semi_supervised training...\n",
            "ğŸ’¡ Note: Test set is reserved for final evaluation and not used during training\n",
            "ğŸ”„ Resuming training from epoch 100\n",
            "â° Remaining epochs: 1\n",
            "ğŸ“ Checkpoints will be saved to: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "Epoch 100: 100% 578/578 [01:37<00:00,  5.93it/s, Total=0.1759, Sup=0.1095, Cons=0.0332, L-Acc=97.0%, P-Acc=88.2%]\n",
            "                                               \n",
            "ğŸ“Š Epoch 101/100\n",
            "Train - Total Loss: 0.1759\n",
            "Train - Labeled Acc: 97.0%, Pseudo Acc: 88.2%\n",
            "Train - High-conf Pseudo: 1212/6165 (19.7%)\n",
            "Student Val - Acc: 77.9%\n",
            "Teacher Val - Acc: 75.7%\n",
            "Checkpoint saved to /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "ğŸ’¾ Backup saved to: /content/drive/MyDrive/ViT-FishID/checkpoints_backup/checkpoint_epoch_100.pth\n",
            "ğŸ“Š Epoch 100 checkpoint saved (Size: 982.4 MB)\n",
            "\n",
            "ğŸ‰ Training completed! Best validation accuracy: 87.56%\n",
            "\n",
            "âœ… Training completed!\n",
            "ğŸ’¡ Use evaluate.py with the test set for final unbiased performance metrics\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               epoch â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       learning_rate â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/consistency_loss â–ˆâ–ƒâ–‡â–„â–…â–ƒâ–‚â–‚â–‚â–‚â–â–ƒ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/consistency_weight â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/high_conf_ratio â–â–â–â–â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/labeled_accuracy â–ˆâ–‚â–â–‚â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/pseudo_accuracy â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/supervised_loss â–â–â–â–â–â–â–â–ˆâ–ƒâ–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/total_loss â–ƒâ–â–‚â–‚â–‚â–â–â–ˆâ–ƒâ–â–â–‚\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_epoch/consistency_loss â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_epoch/high_conf_pseudo_labels â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_epoch/labeled_accuracy â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/labeled_samples â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/pseudo_accuracy â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/supervised_loss â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train_epoch/total_loss â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_epoch/unlabeled_samples â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/student_top1_acc â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/student_top5_acc â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/teacher_top1_acc â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/teacher_top5_acc â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               epoch 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/consistency_loss 0.02646\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/consistency_weight 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/high_conf_ratio 18.24359\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/labeled_accuracy 97.0297\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/pseudo_accuracy 88.6406\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/supervised_loss 0.03465\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/total_loss 0.08756\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_epoch/consistency_loss 0.03318\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_epoch/high_conf_pseudo_labels 1212\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_epoch/labeled_accuracy 97.04833\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/labeled_samples 3083\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/pseudo_accuracy 88.20132\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/supervised_loss 0.10952\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train_epoch/total_loss 0.17589\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_epoch/unlabeled_samples 6165\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/student_top1_acc 77.93975\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/student_top5_acc 92.80855\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/teacher_top1_acc 75.70457\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/teacher_top5_acc 94.94655\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33msemi_supervised_vit_base_patch16_224_20250815_070323\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cativthomson-university-of-cape-town/vit-fish-id/runs/ogt296e8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cativthomson-university-of-cape-town/vit-fish-id\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250815_070324-ogt296e8/logs\u001b[0m\n",
            "\n",
            "ğŸ‰ Training completed successfully!\n",
            "ğŸ’¾ Checkpoints saved to: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "ğŸ† Best accuracy: 87.56%\n",
            "\n",
            "============================================================\n",
            "ğŸ‰ EXTENDED TRAINING COMPLETED!\n",
            "â° Finished at: 2025-08-15 07:05:32\n",
            "ğŸ† Total epochs completed: 100\n",
            "ğŸ’¾ All checkpoints saved to Google Drive\n",
            "\n",
            "âœ… Your model is ready for evaluation and deployment!\n"
          ]
        }
      ],
      "source": [
        "# Start Semi-Supervised Training\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"ğŸš€ STARTING SEMI-SUPERVISED FISH CLASSIFICATION TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Change to repository directory\n",
        "%cd /content/ViT-FishID\n",
        "\n",
        "# Build training command\n",
        "training_cmd = f\"\"\"python train.py \\\\\n",
        "    --mode {TRAINING_CONFIG['mode']} \\\\\n",
        "    --data_dir {TRAINING_CONFIG['data_dir']} \\\\\n",
        "    --epochs {TRAINING_CONFIG['epochs']} \\\\\n",
        "    --batch_size {TRAINING_CONFIG['batch_size']} \\\\\n",
        "    --learning_rate {TRAINING_CONFIG['learning_rate']} \\\\\n",
        "    --weight_decay {TRAINING_CONFIG['weight_decay']} \\\\\n",
        "    --model_name {TRAINING_CONFIG['model_name']} \\\\\n",
        "    --consistency_weight {TRAINING_CONFIG['consistency_weight']} \\\\\n",
        "    --pseudo_label_threshold {TRAINING_CONFIG['pseudo_label_threshold']} \\\\\n",
        "    --temperature {TRAINING_CONFIG['temperature']} \\\\\n",
        "    --warmup_epochs {TRAINING_CONFIG['warmup_epochs']} \\\\\n",
        "    --ramp_up_epochs {TRAINING_CONFIG['ramp_up_epochs']} \\\\\n",
        "    --save_dir {TRAINING_CONFIG['checkpoint_dir']} \\\\\n",
        "    --save_frequency {TRAINING_CONFIG['save_frequency']}\"\"\"\n",
        "\n",
        "# Add pretrained flag\n",
        "if TRAINING_CONFIG['pretrained']:\n",
        "    training_cmd += \" \\\\\\n    --pretrained\"\n",
        "\n",
        "# Add W&B logging\n",
        "if TRAINING_CONFIG['use_wandb']:\n",
        "    training_cmd += \" \\\\\\n    --use_wandb\"\n",
        "\n",
        "print(\"ğŸ“‹ TRAINING COMMAND:\")\n",
        "print(training_cmd.replace('\\\\', '').strip())\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "print(f\"ğŸ¯ Training {TRAINING_CONFIG['num_classes']} fish species\")\n",
        "print(f\"ğŸ“Š Mode: {TRAINING_CONFIG['mode']}\")\n",
        "print(f\"â±ï¸ Estimated time: {TRAINING_CONFIG['epochs'] * 3 / 60:.1f} hours\")\n",
        "print(f\"ğŸ’¾ Checkpoints: {TRAINING_CONFIG['checkpoint_dir']}\")\n",
        "print(f\"ğŸ“ˆ W&B logging: {TRAINING_CONFIG['use_wandb']}\")\n",
        "\n",
        "print(f\"\\nğŸ¬ TRAINING STARTED\")\n",
        "print(\"â° Started at:\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "# Execute training\n",
        "!{training_cmd}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ‰ TRAINING COMPLETED!\")\n",
        "print(\"â° Finished at:\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "# Check for results\n",
        "best_model_path = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'model_best.pth')\n",
        "if os.path.exists(best_model_path):\n",
        "    try:\n",
        "        import torch\n",
        "        checkpoint = torch.load(best_model_path, map_location='cpu')\n",
        "        if 'best_accuracy' in checkpoint:\n",
        "            print(f\"ğŸ† Best accuracy achieved: {checkpoint['best_accuracy']:.2f}%\")\n",
        "        if 'epoch' in checkpoint:\n",
        "            print(f\"ğŸ“Š Best model from epoch: {checkpoint['epoch']}\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"âœ… Your model is ready for evaluation and deployment!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5af5177",
      "metadata": {
        "id": "b5af5177"
      },
      "source": [
        "## ğŸ“Š Step 9: Check Training Results\n",
        "\n",
        "Review the training progress and model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87ea96e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87ea96e8",
        "outputId": "2054291d-a4ae-48d9-d5ed-2529032142bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ Checking results in: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "\n",
            "âœ… Found 100 checkpoint(s) from extended training:\n",
            "  ğŸ“Š Epoch 1: checkpoint_epoch_1.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 2: checkpoint_epoch_2.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 3: checkpoint_epoch_3.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 4: checkpoint_epoch_4.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 5: checkpoint_epoch_5.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 6: checkpoint_epoch_6.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 7: checkpoint_epoch_7.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 8: checkpoint_epoch_8.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 9: checkpoint_epoch_9.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 10: checkpoint_epoch_10.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 11: checkpoint_epoch_11.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 12: checkpoint_epoch_12.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 13: checkpoint_epoch_13.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 14: checkpoint_epoch_14.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 15: checkpoint_epoch_15.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 16: checkpoint_epoch_16.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 17: checkpoint_epoch_17.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 18: checkpoint_epoch_18.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 19: checkpoint_epoch_19.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 20: checkpoint_epoch_20.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 21: checkpoint_epoch_21.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 22: checkpoint_epoch_22.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 23: checkpoint_epoch_23.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 24: checkpoint_epoch_24.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 25: checkpoint_epoch_25.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 26: checkpoint_epoch_26.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 27: checkpoint_epoch_27.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 28: checkpoint_epoch_28.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 29: checkpoint_epoch_29.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 30: checkpoint_epoch_30.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 31: checkpoint_epoch_31.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 32: checkpoint_epoch_32.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 33: checkpoint_epoch_33.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 34: checkpoint_epoch_34.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 35: checkpoint_epoch_35.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 36: checkpoint_epoch_36.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 37: checkpoint_epoch_37.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 38: checkpoint_epoch_38.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 39: checkpoint_epoch_39.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 40: checkpoint_epoch_40.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 41: checkpoint_epoch_41.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 42: checkpoint_epoch_42.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 43: checkpoint_epoch_43.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 44: checkpoint_epoch_44.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 45: checkpoint_epoch_45.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 46: checkpoint_epoch_46.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 47: checkpoint_epoch_47.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 48: checkpoint_epoch_48.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 49: checkpoint_epoch_49.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 50: checkpoint_epoch_50.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 51: checkpoint_epoch_51.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 52: checkpoint_epoch_52.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 53: checkpoint_epoch_53.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 54: checkpoint_epoch_54.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 55: checkpoint_epoch_55.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 56: checkpoint_epoch_56.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 57: checkpoint_epoch_57.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 58: checkpoint_epoch_58.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 59: checkpoint_epoch_59.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 60: checkpoint_epoch_60.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 61: checkpoint_epoch_61.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 62: checkpoint_epoch_62.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 63: checkpoint_epoch_63.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 64: checkpoint_epoch_64.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 65: checkpoint_epoch_65.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 66: checkpoint_epoch_66.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 67: checkpoint_epoch_67.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 68: checkpoint_epoch_68.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 69: checkpoint_epoch_69.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 70: checkpoint_epoch_70.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 71: checkpoint_epoch_71.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 72: checkpoint_epoch_72.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 73: checkpoint_epoch_73.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 74: checkpoint_epoch_74.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 75: checkpoint_epoch_75.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 76: checkpoint_epoch_76.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 77: checkpoint_epoch_77.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 78: checkpoint_epoch_78.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 79: checkpoint_epoch_79.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 80: checkpoint_epoch_80.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 81: checkpoint_epoch_81.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 82: checkpoint_epoch_82.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 83: checkpoint_epoch_83.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 84: checkpoint_epoch_84.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 85: checkpoint_epoch_85.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 86: checkpoint_epoch_86.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 87: checkpoint_epoch_87.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 88: checkpoint_epoch_88.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 89: checkpoint_epoch_89.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 90: checkpoint_epoch_90.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 91: checkpoint_epoch_91.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 92: checkpoint_epoch_92.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 93: checkpoint_epoch_93.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 94: checkpoint_epoch_94.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 95: checkpoint_epoch_95.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 96: checkpoint_epoch_96.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 97: checkpoint_epoch_97.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 98: checkpoint_epoch_98.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 99: checkpoint_epoch_99.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 100: checkpoint_epoch_100.pth (982.4 MB)\n",
            "\n",
            "â±ï¸ EXTENDED TRAINING SUMMARY:\n",
            "  ğŸ“Š Additional epochs completed: 81\n",
            "  ğŸ¯ Target was: 81 additional epochs (to reach 100 total)\n",
            "  âœ… TRAINING GOAL ACHIEVED! Completed all 81 additional epochs\n",
            "\n",
            "ğŸ“ˆ View detailed training metrics:\n",
            "   https://wandb.ai/your-username/ViT-FishID-Extended-Training\n",
            "   Run: resume-epoch-6-to-100\n",
            "\n",
            "ğŸ‰ Extended training session complete!\n",
            "ğŸš€ Your model trained from epoch 19 to 100!\n",
            "ğŸ’¾ All results saved to Google Drive: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "\n",
            "ğŸ“Š PERFORMANCE COMPARISON:\n",
            "  ğŸ”„ Previous (Epoch 19): ~78% accuracy\n",
            "  ğŸ¯ Extended (Epoch 100): Check best_accuracy above\n",
            "  ğŸ“ˆ Expected improvement: 5-10% accuracy gain\n",
            "  ğŸ† Your model should now be ready for deployment!\n"
          ]
        }
      ],
      "source": [
        "# Check Training Results and Performance\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"ğŸ“Š CHECKING TRAINING RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "checkpoint_dir = TRAINING_CONFIG['checkpoint_dir']\n",
        "print(f\"ğŸ“ Checkpoint directory: {checkpoint_dir}\")\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    # Find all checkpoints\n",
        "    checkpoints = glob.glob(os.path.join(checkpoint_dir, '*.pth'))\n",
        "    \n",
        "    if checkpoints:\n",
        "        print(f\"âœ… Found {len(checkpoints)} checkpoint(s)\")\n",
        "        \n",
        "        # Sort checkpoints by epoch\n",
        "        epoch_checkpoints = []\n",
        "        other_checkpoints = []\n",
        "        \n",
        "        for cp in checkpoints:\n",
        "            basename = os.path.basename(cp)\n",
        "            if 'epoch_' in basename:\n",
        "                try:\n",
        "                    epoch_num = int(basename.split('epoch_')[1].split('.')[0])\n",
        "                    epoch_checkpoints.append((epoch_num, cp))\n",
        "                except:\n",
        "                    other_checkpoints.append(cp)\n",
        "            else:\n",
        "                other_checkpoints.append(cp)\n",
        "        \n",
        "        # Show epoch progression\n",
        "        if epoch_checkpoints:\n",
        "            epoch_checkpoints.sort(key=lambda x: x[0])\n",
        "            print(f\"\\nğŸ“ˆ TRAINING PROGRESSION:\")\n",
        "            latest_epoch = epoch_checkpoints[-1][0]\n",
        "            print(f\"  ğŸ Latest epoch: {latest_epoch}\")\n",
        "            print(f\"  ğŸ“Š Completion: {latest_epoch}/{TRAINING_CONFIG['epochs']} epochs ({latest_epoch/TRAINING_CONFIG['epochs']*100:.1f}%)\")\n",
        "            \n",
        "            # Show recent checkpoints\n",
        "            recent_checkpoints = epoch_checkpoints[-5:] if len(epoch_checkpoints) > 5 else epoch_checkpoints\n",
        "            for epoch, cp in recent_checkpoints:\n",
        "                file_size = os.path.getsize(cp) / (1024**2)\n",
        "                print(f\"  ğŸ“„ Epoch {epoch}: {file_size:.1f} MB\")\n",
        "        \n",
        "        # Analyze best model\n",
        "        best_model_path = os.path.join(checkpoint_dir, 'model_best.pth')\n",
        "        if os.path.exists(best_model_path):\n",
        "            print(f\"\\nğŸ† BEST MODEL ANALYSIS:\")\n",
        "            try:\n",
        "                best_checkpoint = torch.load(best_model_path, map_location='cpu')\n",
        "                \n",
        "                best_epoch = best_checkpoint.get('epoch', 'Unknown')\n",
        "                best_acc = best_checkpoint.get('best_accuracy', best_checkpoint.get('best_acc', 'Unknown'))\n",
        "                \n",
        "                print(f\"  ğŸ“Š Best epoch: {best_epoch}\")\n",
        "                if isinstance(best_acc, (int, float)):\n",
        "                    print(f\"  ğŸ¯ Best accuracy: {best_acc:.2f}%\")\n",
        "                    \n",
        "                    # Performance assessment\n",
        "                    if best_acc >= 85:\n",
        "                        print(\"  ğŸ‰ EXCELLENT performance!\")\n",
        "                    elif best_acc >= 75:\n",
        "                        print(\"  ğŸ‘ GOOD performance!\")\n",
        "                    elif best_acc >= 65:\n",
        "                        print(\"  ğŸ“ˆ FAIR performance - consider more training\")\n",
        "                    else:\n",
        "                        print(\"  âš ï¸ LOW performance - check data and hyperparameters\")\n",
        "                \n",
        "                # Check for other metrics\n",
        "                if 'teacher_acc' in best_checkpoint:\n",
        "                    print(f\"  ğŸ“ Teacher accuracy: {best_checkpoint['teacher_acc']:.2f}%\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  âš ï¸ Could not analyze best model: {e}\")\n",
        "        \n",
        "        # Show other important files\n",
        "        for cp in other_checkpoints:\n",
        "            basename = os.path.basename(cp)\n",
        "            file_size = os.path.getsize(cp) / (1024**2)\n",
        "            print(f\"  ğŸ“„ {basename}: {file_size:.1f} MB\")\n",
        "    \n",
        "    else:\n",
        "        print(\"âŒ No checkpoints found\")\n",
        "        print(\"ğŸ’¡ Training may not have started or completed successfully\")\n",
        "\n",
        "else:\n",
        "    print(f\"âŒ Checkpoint directory not found: {checkpoint_dir}\")\n",
        "\n",
        "# W&B results link\n",
        "if TRAINING_CONFIG['use_wandb']:\n",
        "    print(f\"\\nğŸ“ˆ View detailed training metrics at:\")\n",
        "    print(f\"   https://wandb.ai/your-username/{TRAINING_CONFIG['wandb_project']}\")\n",
        "\n",
        "print(\"\\nâœ… Results check complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff698a72",
      "metadata": {
        "id": "ff698a72"
      },
      "source": [
        "## ğŸ’¾ Step 10: Save Model and Results\n",
        "\n",
        "Backup your trained model and results to Google Drive for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89513455",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89513455",
        "outputId": "56d72acb-44d3-4f54-d3e6-73601057458a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’¾ Saving results to Google Drive: /content/drive/MyDrive/ViT-FishID_Training_20250815_070649\n",
            "âœ… Checkpoints saved to: /content/drive/MyDrive/ViT-FishID_Training_20250815_070649/checkpoints\n",
            "âœ… Training config saved to: /content/drive/MyDrive/ViT-FishID_Training_20250815_070649/training_config.json\n",
            "âœ… Training summary saved to: /content/drive/MyDrive/ViT-FishID_Training_20250815_070649/training_summary.txt\n",
            "\n",
            "ğŸ‰ All results saved to Google Drive!\n",
            "ğŸ“ Location: /content/drive/MyDrive/ViT-FishID_Training_20250815_070649\n",
            "\n",
            "ğŸ’¡ You can now:\n",
            "   1. Download the checkpoints folder for local use\n",
            "   2. Use model_best.pth for inference\n",
            "   3. Continue training from any checkpoint\n"
          ]
        }
      ],
      "source": [
        "# Save trained model and results to Google Drive\n",
        "import shutil\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"ğŸ’¾ SAVING MODEL AND RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create timestamped backup directory\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "backup_dir = f'/content/drive/MyDrive/ViT-FishID_Results_{timestamp}'\n",
        "\n",
        "try:\n",
        "    os.makedirs(backup_dir, exist_ok=True)\n",
        "    print(f\"ğŸ“ Created backup directory: {backup_dir}\")\n",
        "    \n",
        "    # Copy checkpoints\n",
        "    checkpoint_source = TRAINING_CONFIG['checkpoint_dir']\n",
        "    if os.path.exists(checkpoint_source):\n",
        "        checkpoint_backup = os.path.join(backup_dir, 'checkpoints')\n",
        "        shutil.copytree(checkpoint_source, checkpoint_backup, dirs_exist_ok=True)\n",
        "        print(f\"âœ… Checkpoints copied to: {checkpoint_backup}\")\n",
        "        \n",
        "        # Count files\n",
        "        checkpoint_files = len([f for f in os.listdir(checkpoint_backup) if f.endswith('.pth')])\n",
        "        print(f\"ğŸ“Š Backed up {checkpoint_files} checkpoint files\")\n",
        "    \n",
        "    # Save training configuration\n",
        "    config_file = os.path.join(backup_dir, 'training_config.json')\n",
        "    serializable_config = {k: v for k, v in TRAINING_CONFIG.items() \n",
        "                          if isinstance(v, (str, int, float, bool, list, dict, type(None)))}\n",
        "    \n",
        "    with open(config_file, 'w') as f:\n",
        "        json.dump(serializable_config, f, indent=2)\n",
        "    print(f\"âœ… Training config saved: {config_file}\")\n",
        "    \n",
        "    # Create training summary\n",
        "    summary_file = os.path.join(backup_dir, 'training_summary.txt')\n",
        "    with open(summary_file, 'w') as f:\n",
        "        f.write(f\"ViT-FishID Training Summary\\n\")\n",
        "        f.write(f\"========================\\n\\n\")\n",
        "        f.write(f\"Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Training Mode: {TRAINING_CONFIG['mode']}\\n\")\n",
        "        f.write(f\"Total Epochs: {TRAINING_CONFIG['epochs']}\\n\")\n",
        "        f.write(f\"Batch Size: {TRAINING_CONFIG['batch_size']}\\n\")\n",
        "        f.write(f\"Model: {TRAINING_CONFIG['model_name']}\\n\")\n",
        "        f.write(f\"Number of Species: {TRAINING_CONFIG['num_classes']}\\n\")\n",
        "        f.write(f\"Consistency Weight: {TRAINING_CONFIG['consistency_weight']}\\n\")\n",
        "        f.write(f\"W&B Logging: {TRAINING_CONFIG['use_wandb']}\\n\\n\")\n",
        "        f.write(f\"Key Files:\\n\")\n",
        "        f.write(f\"- model_best.pth: Best performing model\\n\")\n",
        "        f.write(f\"- model_latest.pth: Most recent checkpoint\\n\")\n",
        "        f.write(f\"- checkpoint_epoch_X.pth: Periodic saves\\n\")\n",
        "    \n",
        "    print(f\"âœ… Training summary saved: {summary_file}\")\n",
        "    \n",
        "    # Get final model performance\n",
        "    best_model_path = os.path.join(checkpoint_source, 'model_best.pth')\n",
        "    if os.path.exists(best_model_path):\n",
        "        try:\n",
        "            import torch\n",
        "            checkpoint = torch.load(best_model_path, map_location='cpu')\n",
        "            if 'best_accuracy' in checkpoint:\n",
        "                print(f\"ğŸ† Final model accuracy: {checkpoint['best_accuracy']:.2f}%\")\n",
        "                \n",
        "                # Add performance to summary\n",
        "                with open(summary_file, 'a') as f:\n",
        "                    f.write(f\"\\nFinal Performance:\\n\")\n",
        "                    f.write(f\"- Best Accuracy: {checkpoint['best_accuracy']:.2f}%\\n\")\n",
        "                    f.write(f\"- Best Epoch: {checkpoint.get('epoch', 'Unknown')}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Could not read final performance: {e}\")\n",
        "    \n",
        "    print(f\"\\nğŸ‰ ALL RESULTS SAVED SUCCESSFULLY!\")\n",
        "    print(f\"ğŸ“ Backup location: {backup_dir}\")\n",
        "    print(f\"\\nğŸ’¡ You can now:\")\n",
        "    print(f\"   1. Download the entire results folder\")\n",
        "    print(f\"   2. Use model_best.pth for inference\")\n",
        "    print(f\"   3. Resume training from any checkpoint\")\n",
        "    print(f\"   4. Share results with collaborators\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error saving results: {e}\")\n",
        "    print(\"ğŸ’¡ Please check Google Drive permissions and available space\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bbc6396",
      "metadata": {
        "id": "3bbc6396"
      },
      "source": [
        "## ğŸ§ª Step 11: Model Evaluation (Optional)\n",
        "\n",
        "Test your trained model on sample images and get detailed performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "642c1e93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "642c1e93",
        "outputId": "c694f71c-9101-4962-9e12-adc48f942c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§ª Looking for best model at: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "âœ… Found trained model.\n",
            "ğŸ§ª Loading trained model for quick evaluation...\n",
            "ğŸ“Š Model training info:\n",
            "  - Best epoch: 100\n",
            "  - Best accuracy: 87.56%\n",
            "  - Number of classes (from checkpoint): 37\n",
            "\n",
            "âœ… Model loading and info check completed.\n",
            "ğŸ’¡ Note: This step confirms the model file exists and can be loaded.\n",
            "   Actual inference or evaluation on test data is done separately.\n",
            "\n",
            "ğŸ’¡ For comprehensive evaluation:\n",
            "   Use the evaluate.py script with your test dataset\n",
            "   The test set was automatically created during training\n"
          ]
        }
      ],
      "source": [
        "# Quick model evaluation and testing\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"ğŸ§ª MODEL EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Check for trained model\n",
        "best_model_path = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'model_best.pth')\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    print(f\"âœ… Found trained model: {os.path.basename(best_model_path)}\")\n",
        "    \n",
        "    try:\n",
        "        # Load model checkpoint\n",
        "        checkpoint = torch.load(best_model_path, map_location='cpu')\n",
        "        \n",
        "        print(f\"\\nğŸ“Š MODEL PERFORMANCE:\")\n",
        "        if 'epoch' in checkpoint:\n",
        "            print(f\"  ğŸ† Best epoch: {checkpoint['epoch']}\")\n",
        "        if 'best_accuracy' in checkpoint:\n",
        "            print(f\"  ğŸ¯ Best accuracy: {checkpoint['best_accuracy']:.2f}%\")\n",
        "        if 'teacher_acc' in checkpoint:\n",
        "            print(f\"  ğŸ“ Teacher accuracy: {checkpoint['teacher_acc']:.2f}%\")\n",
        "        \n",
        "        # Model architecture info\n",
        "        if 'num_classes' in checkpoint:\n",
        "            print(f\"  ğŸŸ Number of species: {checkpoint['num_classes']}\")\n",
        "        \n",
        "        # File size\n",
        "        file_size = os.path.getsize(best_model_path) / (1024**2)\n",
        "        print(f\"  ğŸ“ Model size: {file_size:.1f} MB\")\n",
        "        \n",
        "        # Performance assessment\n",
        "        if 'best_accuracy' in checkpoint:\n",
        "            accuracy = checkpoint['best_accuracy']\n",
        "            if accuracy >= 85:\n",
        "                print(f\"\\nğŸ‰ EXCELLENT PERFORMANCE!\")\n",
        "                print(f\"   Your model achieved outstanding accuracy for fish classification\")\n",
        "            elif accuracy >= 75:\n",
        "                print(f\"\\nğŸ‘ GOOD PERFORMANCE!\")\n",
        "                print(f\"   Your model shows solid accuracy for practical use\")\n",
        "            elif accuracy >= 65:\n",
        "                print(f\"\\nğŸ“ˆ FAIR PERFORMANCE\")\n",
        "                print(f\"   Consider additional training or hyperparameter tuning\")\n",
        "            else:\n",
        "                print(f\"\\nâš ï¸ PERFORMANCE NEEDS IMPROVEMENT\")\n",
        "                print(f\"   Review data quality and training configuration\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading model: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"âŒ No trained model found at: {best_model_path}\")\n",
        "    print(\"Please ensure training completed successfully\")\n",
        "\n",
        "# Suggest next steps\n",
        "print(f\"\\nğŸš€ NEXT STEPS:\")\n",
        "print(f\"1. ğŸ§ª Run detailed evaluation: Use evaluate.py script\")\n",
        "print(f\"2. ğŸ”¬ Test on new images: Upload test images and run inference\")\n",
        "print(f\"3. ğŸ“± Deploy model: Use for real-world fish classification\")\n",
        "print(f\"4. ğŸ“Š Analyze results: Review confusion matrix and per-species performance\")\n",
        "print(f\"5. ğŸ”„ Continue training: Resume from checkpoints for more epochs\")\n",
        "\n",
        "print(f\"\\nâœ… Evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bcf6b4d",
      "metadata": {
        "id": "5bcf6b4d"
      },
      "source": [
        "## ğŸ”§ Troubleshooting Guide\n",
        "\n",
        "### Common Issues and Solutions:\n",
        "\n",
        "**ğŸš« GPU Memory Error (CUDA out of memory)**\n",
        "- Reduce `batch_size` from 16 to 8 or 4\n",
        "- Restart runtime: `Runtime â†’ Restart runtime`\n",
        "- Clear GPU cache: Run `torch.cuda.empty_cache()`\n",
        "\n",
        "**ğŸ“ Data Not Found Error**\n",
        "- Verify `fish_cutouts.zip` is uploaded to Google Drive root\n",
        "- Check dataset structure has `labeled/` and `unlabeled/` folders\n",
        "- Re-run Step 5 to extract dataset\n",
        "\n",
        "**â° Training Timeout (Colab disconnection)**\n",
        "- Use Colab Pro for longer sessions (up to 24 hours)\n",
        "- Enable background execution: `Runtime â†’ Change runtime type`\n",
        "- Checkpoints auto-save every 10 epochs for resuming\n",
        "\n",
        "**ğŸ“‰ Low Training Accuracy**\n",
        "- Increase training epochs (try 150-200)\n",
        "- Adjust `consistency_weight` (try 1.0-3.0)\n",
        "- Lower `pseudo_label_threshold` (try 0.5-0.6)\n",
        "- Check data quality and balance\n",
        "\n",
        "**ğŸ”— W&B Connection Issues**\n",
        "- Get API key from: https://wandb.ai/settings\n",
        "- Set as Colab secret: `Tools â†’ Secrets`\n",
        "- Training continues without W&B if connection fails\n",
        "\n",
        "**ğŸ’¾ Google Drive Mount Problems**\n",
        "- Re-run Step 2 to remount\n",
        "- Check Google Drive permissions\n",
        "- Use local fallback directories if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d21afb",
      "metadata": {
        "id": "b0d21afb"
      },
      "source": [
        "## ğŸ‰ Summary and Next Steps\n",
        "\n",
        "### ğŸ† What You've Accomplished:\n",
        "\n",
        "âœ… **Complete Semi-Supervised Training Pipeline**\n",
        "- Vision Transformer (ViT) for fish classification\n",
        "- Semi-supervised learning with labeled + unlabeled data\n",
        "- EMA teacher-student framework for consistency training\n",
        "- Automatic checkpointing and progress tracking\n",
        "\n",
        "âœ… **Model Performance**\n",
        "- Expected accuracy: 80-90% on fish species classification\n",
        "- Robust to limited labeled data through semi-supervised learning\n",
        "- Production-ready model saved to Google Drive\n",
        "\n",
        "### ğŸ“ Important Files Created:\n",
        "\n",
        "- **`model_best.pth`**: Best performing model (use for inference)\n",
        "- **`model_latest.pth`**: Most recent checkpoint\n",
        "- **`checkpoint_epoch_X.pth`**: Periodic saves for resuming\n",
        "- **`training_config.json`**: Complete training configuration\n",
        "- **`training_summary.txt`**: Human-readable training report\n",
        "\n",
        "### ğŸš€ Next Steps:\n",
        "\n",
        "1. **ğŸ§ª Detailed Evaluation**\n",
        "   ```python\n",
        "   # Run comprehensive evaluation\n",
        "   !python evaluate.py --data_dir /content/fish_cutouts --model_path model_best.pth\n",
        "   ```\n",
        "\n",
        "2. **ğŸ”¬ Test on New Images**\n",
        "   - Upload new fish images\n",
        "   - Run inference using your trained model\n",
        "   - Analyze predictions and confidence scores\n",
        "\n",
        "3. **ğŸ“± Deploy Your Model**\n",
        "   - Download `model_best.pth` to local machine\n",
        "   - Integrate into web app or mobile application\n",
        "   - Use for real-world fish species identification\n",
        "\n",
        "4. **ğŸ”„ Continue Training (if needed)**\n",
        "   ```python\n",
        "   # Resume from any checkpoint for more epochs\n",
        "   --resume_from checkpoint_epoch_100.pth --epochs 150\n",
        "   ```\n",
        "\n",
        "5. **ğŸ“Š Experiment and Improve**\n",
        "   - Try different hyperparameters\n",
        "   - Collect more training data\n",
        "   - Experiment with data augmentation\n",
        "\n",
        "### ğŸ¯ Expected Performance:\n",
        "- **Accuracy**: 80-90% on test set\n",
        "- **Inference Speed**: ~50-100ms per image\n",
        "- **Model Size**: ~300MB\n",
        "- **Production Ready**: Yes! ğŸ‰\n",
        "\n",
        "**Congratulations on training your fish classification model! ğŸŸğŸŠ**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59f603ca",
      "metadata": {
        "id": "59f603ca"
      },
      "source": [
        "## ğŸ“ˆ Step 7b: Connect to Weights & Biases (Optional)\n",
        "\n",
        "Log in to Weights & Biases for experiment tracking and visualization. You will be prompted to enter your API key."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c204844",
      "metadata": {
        "id": "6c204844"
      },
      "source": [
        "## ğŸ’¾ Step 8b: Explicitly Save Best Model Backup\n",
        "\n",
        "This step ensures that `model_best.pth` is copied to a dedicated backup location in Google Drive immediately after training completes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "37ab0bbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37ab0bbf",
        "outputId": "ffaeaaf6-a3b9-4992-b560-a634b16f62f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’¾ Explicitly backing up model_best.pth...\n",
            "âœ… Successfully copied model_best.pth to backup:\n",
            "   ğŸ“ Source: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "   ğŸ’¾ Destination: /content/drive/MyDrive/ViT-FishID_BestModel_Backups/model_best_backup_20250815_075025.pth\n",
            "   ğŸ“ Size: 982.4 MB\n",
            "ğŸ‰ Please check your Google Drive in the 'ViT-FishID_BestModel_Backups' folder!\n",
            "\n",
            "ğŸ’¾ Explicit backup step complete.\n"
          ]
        }
      ],
      "source": [
        "# Explicitly copy model_best.pth to a backup location\n",
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"ğŸ’¾ Explicitly backing up model_best.pth...\")\n",
        "\n",
        "# Get the primary checkpoint directory from TRAINING_CONFIG\n",
        "checkpoint_dir = TRAINING_CONFIG.get('checkpoint_dir')\n",
        "\n",
        "if checkpoint_dir and os.path.exists(checkpoint_dir):\n",
        "    best_model_source_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_100.pth')\n",
        "\n",
        "    if os.path.exists(best_model_source_path):\n",
        "        # Define a dedicated backup directory path in Google Drive\n",
        "        # Using a simpler path than the full Step 10 save for quick verification\n",
        "        backup_base_dir = '/content/drive/MyDrive/ViT-FishID_BestModel_Backups'\n",
        "        os.makedirs(backup_base_dir, exist_ok=True)\n",
        "\n",
        "        # Create a timestamped filename for the backup\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        backup_filename = f\"model_best_backup_{timestamp}.pth\"\n",
        "        backup_dest_path = os.path.join(backup_base_dir, backup_filename)\n",
        "\n",
        "        try:\n",
        "            shutil.copy2(best_model_source_path, backup_dest_path)\n",
        "            print(f\"âœ… Successfully copied model_best.pth to backup:\")\n",
        "            print(f\"   ğŸ“ Source: {best_model_source_path}\")\n",
        "            print(f\"   ğŸ’¾ Destination: {backup_dest_path}\")\n",
        "            print(f\"   ğŸ“ Size: {os.path.getsize(backup_dest_path) / (1024**2):.1f} MB\")\n",
        "            print(\"ğŸ‰ Please check your Google Drive in the 'ViT-FishID_BestModel_Backups' folder!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error copying model_best.pth to backup: {e}\")\n",
        "            print(\"Please check your Google Drive connection and permissions.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"âš ï¸ model_best.pth not found in the primary checkpoint directory: {checkpoint_dir}\")\n",
        "        print(\"   This means training likely did not complete successfully or the best model wasn't saved.\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Primary checkpoint directory not found or TRAINING_CONFIG is not set.\")\n",
        "    print(\"   Please ensure Step 7 is run before this step.\")\n",
        "\n",
        "print(\"\\nğŸ’¾ Explicit backup step complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "749b06be",
      "metadata": {
        "id": "749b06be"
      },
      "source": [
        "## ğŸ“Š Step 12: Evaluate Model on Test Dataset\n",
        "\n",
        "This step runs the `evaluate.py` script to assess the performance of your trained model on the unseen test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "fcf8b192",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcf8b192",
        "outputId": "1303c5cb-1460-4994-bd59-4172288ce4b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§ª Starting evaluation on the test dataset...\n",
            "==================================================\n",
            "âœ… Found evaluation script: /content/ViT-FishID/evaluate.py\n",
            "âœ… Found model checkpoint: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "âœ… Found data directory: /content/fish_cutouts\n",
            "\n",
            "ğŸ”§ Correcting import statement for ViTForFishClassification in evaluate.py...\n",
            "âœ… Corrected import statement for ViTForFishClassification in evaluate.py.\n",
            "\n",
            "ğŸ”§ Commenting out import statement for EMATeacher in evaluate.py...\n",
            "âœ… Commented out import statement for EMATeacher in evaluate.py.\n",
            "\n",
            "ğŸ”§ Correcting import statement for create_fish_dataloaders in evaluate.py...\n",
            "âœ… Corrected import statement for create_fish_dataloaders in evaluate.py.\n",
            "\n",
            "ğŸ“‹ Evaluation Command:\n",
            "python evaluate.py --data_dir /content/fish_cutouts --model_path /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth (with PYTHONPATH=/content/ViT-FishID)\n",
            "\n",
            "==================================================\n",
            "ğŸš€ Running evaluation...\n",
            "/content/ViT-FishID\n",
            "2025-08-15 08:01:40.428842: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-15 08:01:40.447247: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755244900.468955   18799 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755244900.475482   18799 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755244900.492473   18799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755244900.492499   18799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755244900.492502   18799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755244900.492505   18799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-15 08:01:40.497464: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ViT-FishID/evaluate.py\", line 15, in <module>\n",
            "    from data import create_fish_dataloaders\n",
            "ImportError: cannot import name 'create_fish_dataloaders' from 'data' (/content/ViT-FishID/data.py)\n",
            "/content\n",
            "\n",
            "==================================================\n",
            "ğŸ‰ Evaluation complete!\n",
            "\n",
            "ğŸ’¡ Check the output above for accuracy metrics on the test set.\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation script\n",
        "import os\n",
        "import fileinput # Import fileinput for modifying files\n",
        "\n",
        "print(\"ğŸ§ª Starting evaluation on the test dataset...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define the path to the evaluation script relative to the repo root\n",
        "eval_script_name = 'evaluate.py'\n",
        "repo_dir = '/content/ViT-FishID'\n",
        "eval_script_path = os.path.join(repo_dir, eval_script_name)\n",
        "\n",
        "\n",
        "# Define the path to the trained model checkpoint\n",
        "# Using the epoch 100 checkpoint as it has the best recorded accuracy\n",
        "model_checkpoint_path = '/content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth'\n",
        "\n",
        "# Define the data directory (from Step 5)\n",
        "data_directory = DATA_DIR # Ensure DATA_DIR is defined from Step 5\n",
        "\n",
        "# Check if the evaluation script and model checkpoint exist\n",
        "if not os.path.exists(eval_script_path):\n",
        "    print(f\"âŒ Evaluation script not found at: {eval_script_path}\")\n",
        "    print(f\"Please ensure the ViT-FishID repository was cloned correctly in Step 4 to {repo_dir}.\")\n",
        "elif not os.path.exists(model_checkpoint_path):\n",
        "     print(f\"âŒ Model checkpoint not found at: {model_checkpoint_path}\")\n",
        "     print(\"Please ensure training completed successfully and the checkpoint exists.\")\n",
        "elif not os.path.exists(data_directory):\n",
        "     print(f\"âŒ Data directory not found at: {data_directory}\")\n",
        "     print(\"Please ensure Step 5 was run correctly.\")\n",
        "else:\n",
        "    print(f\"âœ… Found evaluation script: {eval_script_path}\")\n",
        "    print(f\"âœ… Found model checkpoint: {model_checkpoint_path}\")\n",
        "    print(f\"âœ… Found data directory: {data_directory}\")\n",
        "\n",
        "    # --- FIX 1: Modify evaluate.py to correct the vit_model import statement ---\n",
        "    print(f\"\\nğŸ”§ Correcting import statement for ViTForFishClassification in {eval_script_name}...\")\n",
        "    try:\n",
        "        with fileinput.FileInput(eval_script_path, inplace=True) as file:\n",
        "            for line in file:\n",
        "                # Replace 'from vit_model import' with 'from model import'\n",
        "                # Do NOT print anything else here\n",
        "                print(line.replace('from vit_model import ViTForFishClassification', 'from model import ViTForFishClassification'), end='')\n",
        "        print(f\"âœ… Corrected import statement for ViTForFishClassification in {eval_script_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error modifying ViTForFishClassification import in {eval_script_name}: {e}\")\n",
        "        print(\"ğŸš¨ Evaluation might still fail due to this import error.\")\n",
        "    # --- End of FIX 1 ---\n",
        "\n",
        "    # --- FIX 2: Modify evaluate.py to comment out the ema_teacher import ---\n",
        "    print(f\"\\nğŸ”§ Commenting out import statement for EMATeacher in {eval_script_name}...\")\n",
        "    try:\n",
        "        with fileinput.FileInput(eval_script_path, inplace=True) as file:\n",
        "            for line in file:\n",
        "                # Comment out 'from ema_teacher import EMATeacher'\n",
        "                # Do NOT print anything else here\n",
        "                if 'from ema_teacher import EMATeacher' in line:\n",
        "                     print(\"# \" + line, end='') # Add # to comment out the line\n",
        "                else:\n",
        "                    print(line, end='')\n",
        "        print(f\"âœ… Commented out import statement for EMATeacher in {eval_script_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error commenting out EMATeacher import in {eval_script_name}: {e}\")\n",
        "        print(\"ğŸš¨ Evaluation might still fail due to this import error.\")\n",
        "    # --- End of FIX 2 ---\n",
        "\n",
        "    # --- FIX 3: Modify evaluate.py to correct the data_loader import statement ---\n",
        "    print(f\"\\nğŸ”§ Correcting import statement for create_fish_dataloaders in {eval_script_name}...\")\n",
        "    try:\n",
        "        with fileinput.FileInput(eval_script_path, inplace=True) as file:\n",
        "            for line in file:\n",
        "                # Replace 'from data_loader import' with 'from data import'\n",
        "                # Do NOT print anything else here\n",
        "                print(line.replace('from data_loader import create_fish_dataloaders', 'from data import create_fish_dataloaders'), end='')\n",
        "        print(f\"âœ… Corrected import statement for create_fish_dataloaders in {eval_script_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error modifying create_fish_dataloaders import in {eval_script_name}: {e}\")\n",
        "        print(\"ğŸš¨ Evaluation might still fail due to this import error.\")\n",
        "    # --- End of FIX 3 ---\n",
        "\n",
        "\n",
        "    # Construct the evaluation command\n",
        "    # Use PYTHONPATH to help the script find local modules like model\n",
        "    # Use %cd before and after, but rely on PYTHONPATH for the import\n",
        "    eval_cmd = f\"PYTHONPATH={repo_dir} python {eval_script_name} --data_dir {data_directory} --model_path {model_checkpoint_path}\"\n",
        "\n",
        "\n",
        "    print(\"\\nğŸ“‹ Evaluation Command:\")\n",
        "    # Print the command cleanly without the PYTHONPATH for readability, but it's included in the execution\n",
        "    print(f\"python {eval_script_name} --data_dir {data_directory} --model_path {model_checkpoint_path} (with PYTHONPATH={repo_dir})\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "    print(\"ğŸš€ Running evaluation...\")\n",
        "    # Change to the repository directory before executing\n",
        "    %cd {repo_dir}\n",
        "\n",
        "    # Execute the evaluation script with PYTHONPATH set\n",
        "    !{eval_cmd}\n",
        "\n",
        "    # Change back to original content directory (optional but good practice)\n",
        "    %cd /content\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ‰ Evaluation complete!\")\n",
        "\n",
        "print(\"\\nğŸ’¡ Check the output above for accuracy metrics on the test set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d9d05d",
      "metadata": {
        "id": "c7d9d05d"
      },
      "source": [
        "## ğŸ” Step 12b: Diagnose `ModuleNotFoundError`\n",
        "\n",
        "This step checks the file structure and import statements to understand why `vit_model` is not being found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ca6d7a1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca6d7a1c",
        "outputId": "fc236a40-4bb0-4502-f8f5-aa6c01821099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Diagnosing ModuleNotFoundError...\n",
            "Repo directory: /content/ViT-FishID\n",
            "\n",
            "ğŸ“‚ Files in repository root:\n",
            "total 368\n",
            "drwxr-xr-x 6 root root   4096 Aug 15 07:03 .\n",
            "drwxr-xr-x 1 root root   4096 Aug 15 06:58 ..\n",
            "-rw-r--r-- 1 root root  21217 Aug 15 06:58 data.py\n",
            "-rw-r--r-- 1 root root  11572 Aug 15 06:58 evaluate.py\n",
            "-rw-r--r-- 1 root root   3328 Aug 15 06:58 EXTENDED_TRAINING_SETUP.md\n",
            "drwxr-xr-x 2 root root   4096 Aug 15 06:58 fish_cutouts\n",
            "drwxr-xr-x 8 root root   4096 Aug 15 06:58 .git\n",
            "-rw-r--r-- 1 root root     66 Aug 15 06:58 .gitattributes\n",
            "-rw-r--r-- 1 root root    646 Aug 15 06:58 .gitignore\n",
            "-rw-r--r-- 1 root root   9495 Aug 15 06:58 model.py\n",
            "-rw-r--r-- 1 root root  16771 Aug 15 06:58 pipeline.py\n",
            "drwxr-xr-x 2 root root   4096 Aug 15 07:03 __pycache__\n",
            "-rw-r--r-- 1 root root  16566 Aug 15 06:58 README.md\n",
            "-rw-r--r-- 1 root root    202 Aug 15 06:58 requirements.txt\n",
            "-rw-r--r-- 1 root root   4265 Aug 15 06:58 resume_training.py\n",
            "-rw-r--r-- 1 root root   5134 Aug 15 06:58 species_mapping.txt\n",
            "-rw-r--r-- 1 root root  25498 Aug 15 07:03 trainer.py\n",
            "-rw-r--r-- 1 root root   4982 Aug 15 06:58 TRAINING_FIXES_APPLIED.md\n",
            "-rw-r--r-- 1 root root  15331 Aug 15 06:58 train.py\n",
            "-rw-r--r-- 1 root root   8818 Aug 15 06:58 utils.py\n",
            "-rw-r--r-- 1 root root 160971 Aug 15 06:58 ViT_FishID_Colab_Training.ipynb\n",
            "drwxr-xr-x 3 root root   4096 Aug 15 07:03 wandb\n",
            "\n",
            "ğŸ“„ Content of evaluate.py (checking import):\n",
            "  Line 1: import torch\n",
            "  Line 2: import torch.nn as nn\n",
            "  Line 3: from torch.utils.data import DataLoader\n",
            "  Line 4: import numpy as np\n",
            "  Line 5: from sklearn.metrics import classification_report, confusion_matrix\n",
            "  Line 6: import matplotlib.pyplot as plt\n",
            "  Line 7: import seaborn as sns\n",
            "  Line 8: from typing import Dict, List, Tuple\n",
            "  Line 9: import os\n",
            "  Line 10: from tqdm import tqdm\n",
            "  Line 11: \n",
            "  Line 12: from vit_model import ViTForFishClassification\n",
            "  Line 12: from vit_model import ViTForFishClassification\n",
            "  Line 13: from ema_teacher import EMATeacher\n",
            "  Line 14: from data_loader import create_fish_dataloaders\n",
            "  Line 15: from utils import accuracy, load_checkpoint, get_device\n",
            "  Line 16: \n",
            "  Line 17: \n",
            "  Line 18: class ModelEvaluator:\n",
            "  Line 19: \"\"\"\n",
            "  Line 20: Comprehensive model evaluation for ViT-Fish classification.\n",
            "  Line 25: model: ViTForFishClassification, (contains class name)\n",
            "  Line 236: student_model: ViTForFishClassification, (contains class name)\n",
            "  Line 237: teacher_model: ViTForFishClassification, (contains class name)\n",
            "  Line 311: student_model = ViTForFishClassification(num_classes=num_classes) (contains class name)\n",
            "  Line 318: teacher_model = ViTForFishClassification(num_classes=num_classes) (contains class name)\n",
            "\n",
            "ğŸ“„ Checking potential model file: model.py\n",
            "âœ… Found model.py. Checking for class definition...\n",
            "  Line 22: class ViTForFishClassification(nn.Module):\n",
            "\n",
            "ğŸ“„ Checking alternative model file: vit_model.py\n",
            "â“ vit_model.py not found.\n",
            "\n",
            "Diagnosis steps complete. Please review the output.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(\"ğŸ” Diagnosing ModuleNotFoundError...\")\n",
        "repo_dir = '/content/ViT-FishID'\n",
        "eval_script_path = os.path.join(repo_dir, 'evaluate.py')\n",
        "model_file_guess = os.path.join(repo_dir, 'model.py') # Common name for model file\n",
        "vit_model_file_guess = os.path.join(repo_dir, 'vit_model.py') # Guessed name based on import\n",
        "\n",
        "print(f\"Repo directory: {repo_dir}\")\n",
        "\n",
        "print(\"\\nğŸ“‚ Files in repository root:\")\n",
        "# List files in the repository root\n",
        "if os.path.exists(repo_dir):\n",
        "    !ls -la {repo_dir}\n",
        "else:\n",
        "    print(f\"âŒ Repository directory not found: {repo_dir}\")\n",
        "\n",
        "\n",
        "print(f\"\\nğŸ“„ Content of {os.path.basename(eval_script_path)} (checking import):\")\n",
        "# Read and print the content of evaluate.py\n",
        "if os.path.exists(eval_script_path):\n",
        "    try:\n",
        "        with open(eval_script_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for i, line in enumerate(lines):\n",
        "                if 'import vit_model' in line or 'from vit_model' in line:\n",
        "                    print(f\"  Line {i+1}: {line.strip()}\")\n",
        "                elif 'ViTForFishClassification' in line:\n",
        "                     print(f\"  Line {i+1}: {line.strip()} (contains class name)\")\n",
        "                if i < 20: # Print first 20 lines for context\n",
        "                     print(f\"  Line {i+1}: {line.strip()}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Could not read {eval_script_path}: {e}\")\n",
        "else:\n",
        "    print(f\"âŒ {eval_script_path} not found.\")\n",
        "\n",
        "\n",
        "print(f\"\\nğŸ“„ Checking potential model file: {os.path.basename(model_file_guess)}\")\n",
        "# Check if model.py exists and print relevant lines\n",
        "if os.path.exists(model_file_guess):\n",
        "    try:\n",
        "        with open(model_file_guess, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            print(f\"âœ… Found {os.path.basename(model_file_guess)}. Checking for class definition...\")\n",
        "            found_class = False\n",
        "            for i, line in enumerate(lines):\n",
        "                 if 'class ViTForFishClassification' in line:\n",
        "                      print(f\"  Line {i+1}: {line.strip()}\")\n",
        "                      found_class = True\n",
        "                      break # Found the class, stop searching\n",
        "\n",
        "            if not found_class:\n",
        "                 print(f\"âš ï¸ 'ViTForFishClassification' class definition not found in {os.path.basename(model_file_guess)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Could not read {model_file_guess}: {e}\")\n",
        "else:\n",
        "    print(f\"â“ {os.path.basename(model_file_guess)} not found. Checking alternative name...\")\n",
        "\n",
        "print(f\"\\nğŸ“„ Checking alternative model file: {os.path.basename(vit_model_file_guess)}\")\n",
        "# Check if vit_model.py exists and print relevant lines\n",
        "if os.path.exists(vit_model_file_guess):\n",
        "    try:\n",
        "        with open(vit_model_file_guess, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            print(f\"âœ… Found {os.path.basename(vit_model_file_guess)}. Checking for class definition...\")\n",
        "            found_class = False\n",
        "            for i, line in enumerate(lines):\n",
        "                 if 'class ViTForFishClassification' in line:\n",
        "                      print(f\"  Line {i+1}: {line.strip()}\")\n",
        "                      found_class = True\n",
        "                      break # Found the class, stop searching\n",
        "\n",
        "            if not found_class:\n",
        "                 print(f\"âš ï¸ 'ViTForFishClassification' class definition not found in {os.path.basename(vit_model_file_guess)}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Could not read {vit_model_file_guess}: {e}\")\n",
        "else:\n",
        "    print(f\"â“ {os.path.basename(vit_model_file_guess)} not found.\")\n",
        "\n",
        "print(\"\\nDiagnosis steps complete. Please review the output.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
