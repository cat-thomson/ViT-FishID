{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0e0af9a0",
      "metadata": {
        "id": "0e0af9a0"
      },
      "source": [
        "# ğŸš€ ViT-FishID: Resume Training from Epoch 19\n",
        "\n",
        "**COLAB PRO EXTENDED TRAINING**\n",
        "- Resume from: Epoch 19 checkpoint\n",
        "- Target epochs: 100 total epochs (81 remaining)\n",
        "- Expected training time: 6-8 hours with Colab Pro\n",
        "- GPU: Tesla T4/V100/A100 (depending on availability)\n",
        "\n",
        "This notebook will:\n",
        "1. âœ… Resume training from your saved checkpoint at epoch 19\n",
        "2. âœ… Train for 100 total epochs (81 more epochs)\n",
        "3. âœ… Save checkpoints to Google Drive every 10 epochs\n",
        "4. âœ… Use semi-supervised learning with your fish dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "796c6e33",
      "metadata": {
        "id": "796c6e33"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cat-thomson/ViT-FishID/blob/main/ViT_FishID_Colab_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f2f60df",
      "metadata": {
        "id": "1f2f60df"
      },
      "source": [
        "# ğŸŸ ViT-FishID: Extended Training Session\n",
        "\n",
        "**RESUME FROM EPOCH 19 - COLAB PRO**\n",
        "\n",
        "This notebook resumes training from your saved checkpoint and runs for 100 total epochs.\n",
        "\n",
        "**Current Status:**\n",
        "- âœ… Previous training: 19 epochs completed\n",
        "- ğŸ¯ Target: 100 total epochs (81 remaining)\n",
        "- â±ï¸ Expected time: 6-8 hours with Colab Pro\n",
        "- ğŸ’¾ Auto-save every 10 epochs to Google Drive\n",
        "\n",
        "**Performance Target:**\n",
        "- Previous: ~78% validation accuracy at epoch 19\n",
        "- Expected: 85-90% accuracy after 100 epochs\n",
        "- Memory: ~8-12GB GPU memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26bcb2a3",
      "metadata": {
        "id": "26bcb2a3"
      },
      "source": [
        "## ğŸš€ Step 1: Setup and GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f3540b19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3540b19",
        "outputId": "4859151f-eadc-4319-a3ac-b5f70e71df87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” System Information:\n",
            "Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "GPU Device: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory: 39.6 GB\n",
            "âœ… GPU is ready for training!\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"ğŸ” System Information:\")\n",
        "print(f\"Python version: {os.sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    print(\"âœ… GPU is ready for training!\")\n",
        "else:\n",
        "    print(\"âŒ No GPU detected. Please enable GPU runtime:\")\n",
        "    print(\"   Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149f671b",
      "metadata": {
        "id": "149f671b"
      },
      "source": [
        "## ğŸ“ Step 2: Mount Google Drive\n",
        "\n",
        "This will give us access to your fish dataset stored in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4abb3ffd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4abb3ffd",
        "outputId": "29fb191e-611d-4f2d-a4d5-69fec2c1d936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "ğŸ“‚ Google Drive contents:\n",
            "  - Mock Matric\n",
            "  - Photos\n",
            "  - Admin\n",
            "  - Uni\n",
            "  - Fish_Training_Output\n",
            "  - Colab Notebooks\n",
            "  - ViT-FishID\n",
            "  - fish_cutouts.zip\n",
            "  - ViT-FishID_Training_20250814_154652\n",
            "  - ViT-FishID_Training_20250814_202307\n",
            "  ... and 3 more items\n",
            "\n",
            "âœ… Google Drive mounted successfully!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# List contents to verify mount\n",
        "print(\"\\nğŸ“‚ Google Drive contents:\")\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "if os.path.exists(drive_path):\n",
        "    items = os.listdir(drive_path)[:10]  # Show first 10 items\n",
        "    for item in items:\n",
        "        print(f\"  - {item}\")\n",
        "    if len(os.listdir(drive_path)) > 10:\n",
        "        print(f\"  ... and {len(os.listdir(drive_path)) - 10} more items\")\n",
        "    print(\"\\nâœ… Google Drive mounted successfully!\")\n",
        "else:\n",
        "    print(\"âŒ Failed to mount Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be8b6273",
      "metadata": {
        "id": "be8b6273"
      },
      "source": [
        "## ğŸ“¦ Step 3: Install Dependencies\n",
        "\n",
        "Installing all required packages for ViT-FishID training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8c724abc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c724abc",
        "outputId": "77da503f-48d7-49ca-dc48-111168649d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Installing dependencies...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… All dependencies installed successfully!\n",
            "\n",
            "ğŸ“‹ Package versions:\n",
            "  - torch: 2.6.0+cu124\n",
            "  - torchvision: 0.21.0+cu124\n",
            "  - timm: 1.0.19\n",
            "  - albumentations: 2.0.8\n",
            "  - opencv: 4.12.0\n",
            "  - sklearn: 1.6.1\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "print(\"ğŸ“¦ Installing dependencies...\")\n",
        "\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q timm transformers\n",
        "!pip install -q albumentations\n",
        "!pip install -q wandb\n",
        "!pip install -q opencv-python-headless\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q matplotlib seaborn\n",
        "!pip install -q tqdm\n",
        "\n",
        "print(\"âœ… All dependencies installed successfully!\")\n",
        "\n",
        "# Verify installations\n",
        "import torch\n",
        "import torchvision\n",
        "import timm\n",
        "import albumentations\n",
        "import cv2\n",
        "import sklearn\n",
        "\n",
        "print(\"\\nğŸ“‹ Package versions:\")\n",
        "print(f\"  - torch: {torch.__version__}\")\n",
        "print(f\"  - torchvision: {torchvision.__version__}\")\n",
        "print(f\"  - timm: {timm.__version__}\")\n",
        "print(f\"  - albumentations: {albumentations.__version__}\")\n",
        "print(f\"  - opencv: {cv2.__version__}\")\n",
        "print(f\"  - sklearn: {sklearn.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b795fc",
      "metadata": {
        "id": "12b795fc"
      },
      "source": [
        "## ğŸ”„ Step 4: Clone ViT-FishID Repository\n",
        "\n",
        "Getting the latest code from your GitHub repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c4e4cd45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4e4cd45",
        "outputId": "8efc760e-aea2-48bd-f684-8f9d338697e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ Cloning ViT-FishID repository...\n",
            "Cloning into '/content/ViT-FishID'...\n",
            "remote: Enumerating objects: 119, done.\u001b[K\n",
            "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 119 (delta 44), reused 98 (delta 27), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (119/119), 201.94 KiB | 20.19 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n",
            "/content/ViT-FishID\n",
            "\n",
            "ğŸ“‚ Project structure:\n",
            "total 360\n",
            "drwxr-xr-x 4 root root   4096 Aug 15 06:58 .\n",
            "drwxr-xr-x 1 root root   4096 Aug 15 06:58 ..\n",
            "-rw-r--r-- 1 root root  21217 Aug 15 06:58 data.py\n",
            "-rw-r--r-- 1 root root  11572 Aug 15 06:58 evaluate.py\n",
            "-rw-r--r-- 1 root root   3328 Aug 15 06:58 EXTENDED_TRAINING_SETUP.md\n",
            "drwxr-xr-x 2 root root   4096 Aug 15 06:58 fish_cutouts\n",
            "drwxr-xr-x 8 root root   4096 Aug 15 06:58 .git\n",
            "-rw-r--r-- 1 root root     66 Aug 15 06:58 .gitattributes\n",
            "-rw-r--r-- 1 root root    646 Aug 15 06:58 .gitignore\n",
            "-rw-r--r-- 1 root root   9495 Aug 15 06:58 model.py\n",
            "-rw-r--r-- 1 root root  16771 Aug 15 06:58 pipeline.py\n",
            "-rw-r--r-- 1 root root  16566 Aug 15 06:58 README.md\n",
            "-rw-r--r-- 1 root root    202 Aug 15 06:58 requirements.txt\n",
            "-rw-r--r-- 1 root root   4265 Aug 15 06:58 resume_training.py\n",
            "-rw-r--r-- 1 root root   5134 Aug 15 06:58 species_mapping.txt\n",
            "-rw-r--r-- 1 root root  25497 Aug 15 06:58 trainer.py\n",
            "-rw-r--r-- 1 root root   4982 Aug 15 06:58 TRAINING_FIXES_APPLIED.md\n",
            "-rw-r--r-- 1 root root  15331 Aug 15 06:58 train.py\n",
            "-rw-r--r-- 1 root root   8818 Aug 15 06:58 utils.py\n",
            "-rw-r--r-- 1 root root 160971 Aug 15 06:58 ViT_FishID_Colab_Training.ipynb\n",
            "\n",
            "âœ… Repository cloned successfully!\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "import os\n",
        "\n",
        "# Remove existing directory if it exists\n",
        "if os.path.exists('/content/ViT-FishID'):\n",
        "    !rm -rf /content/ViT-FishID\n",
        "\n",
        "# Clone the repository\n",
        "print(\"ğŸ“¥ Cloning ViT-FishID repository...\")\n",
        "!git clone https://github.com/cat-thomson/ViT-FishID.git /content/ViT-FishID\n",
        "\n",
        "# Change to project directory\n",
        "%cd /content/ViT-FishID\n",
        "\n",
        "# List project files\n",
        "print(\"\\nğŸ“‚ Project structure:\")\n",
        "!ls -la\n",
        "\n",
        "print(\"\\nâœ… Repository cloned successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8155c400",
      "metadata": {
        "id": "8155c400"
      },
      "source": [
        "## ğŸ—‚ï¸ Step 5: Setup Data Path and Extraction\n",
        "\n",
        "**IMPORTANT:** Specify the path to your fish dataset ZIP file in Google Drive.\n",
        "\n",
        "This step will:\n",
        "1. Locate your `fish_cutouts.zip` file in Google Drive\n",
        "2. Extract it to Colab's local storage for faster access\n",
        "3. Validate the data structure\n",
        "\n",
        "Expected structure after extraction:\n",
        "```\n",
        "fish_cutouts/\n",
        "â”œâ”€â”€ labeled/\n",
        "â”‚   â”œâ”€â”€ species_1/\n",
        "â”‚   â”‚   â”œâ”€â”€ fish_001.jpg\n",
        "â”‚   â”‚   â””â”€â”€ fish_002.jpg\n",
        "â”‚   â””â”€â”€ species_2/\n",
        "â”‚       â””â”€â”€ ...\n",
        "â””â”€â”€ unlabeled/\n",
        "    â”œâ”€â”€ fish_003.jpg\n",
        "    â””â”€â”€ fish_004.jpg\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup data path and extraction - CORRECTED PATHS\n",
        "import zipfile\n",
        "import shutil\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(\"ğŸ—‚ï¸ SETTING UP FISH DATASET - CORRECTED PATHS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Configuration - CORRECTED file paths\n",
        "ZIP_FILE_PATH = '/content/drive/MyDrive/fish_cutouts.zip'  # Correct location\n",
        "DATA_DIR = '/content/fish_cutouts'\n",
        "\n",
        "print(f\"ğŸ¯ ZIP file location: {ZIP_FILE_PATH}\")\n",
        "print(f\"ğŸ¯ Target data directory: {DATA_DIR}\")\n",
        "\n",
        "# Check if data already exists locally (from previous session)\n",
        "if os.path.exists(DATA_DIR) and os.path.exists(os.path.join(DATA_DIR, 'labeled')):\n",
        "    print(\"âœ… Data already available locally from previous session!\")\n",
        "\n",
        "    # Quick validation\n",
        "    labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
        "    unlabeled_dir = os.path.join(DATA_DIR, 'unlabeled')\n",
        "\n",
        "    if os.path.exists(labeled_dir):\n",
        "        labeled_species = [d for d in os.listdir(labeled_dir)\n",
        "                          if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')]\n",
        "        print(f\"ğŸŸ Found {len(labeled_species)} labeled species\")\n",
        "\n",
        "    if os.path.exists(unlabeled_dir):\n",
        "        unlabeled_files = [f for f in os.listdir(unlabeled_dir)\n",
        "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        print(f\"ğŸ“Š Found {len(unlabeled_files)} unlabeled images\")\n",
        "\n",
        "    print(\"âœ… Data validation passed - ready for training!\")\n",
        "\n",
        "else:\n",
        "    print(\"ğŸ“¥ Data not found locally, extracting from Google Drive...\")\n",
        "\n",
        "    # Check if ZIP file exists\n",
        "    if os.path.exists(ZIP_FILE_PATH):\n",
        "        print(f\"âœ… Found ZIP file at: {ZIP_FILE_PATH}\")\n",
        "        print(f\"ğŸ“ ZIP file size: {os.path.getsize(ZIP_FILE_PATH) / (1024**2):.1f} MB\")\n",
        "\n",
        "        # Clean extraction\n",
        "        temp_extract_dir = '/content/temp_fish_extract'\n",
        "        if os.path.exists(temp_extract_dir):\n",
        "            shutil.rmtree(temp_extract_dir)\n",
        "\n",
        "        try:\n",
        "            # Extract ZIP file directly\n",
        "            print(f\"ğŸ“¦ Extracting {os.path.basename(ZIP_FILE_PATH)}...\")\n",
        "            with zipfile.ZipFile(ZIP_FILE_PATH, 'r') as zip_ref:\n",
        "                zip_ref.extractall(temp_extract_dir)\n",
        "\n",
        "            print(\"âœ… ZIP extraction completed\")\n",
        "\n",
        "            # Check what was extracted\n",
        "            extracted_items = os.listdir(temp_extract_dir)\n",
        "            print(f\"ğŸ“ Found in ZIP: {extracted_items}\")\n",
        "\n",
        "            # Based on your description: dataset_info.json, labeled, unlabeled, MACOS\n",
        "            # Look for labeled and unlabeled directories directly\n",
        "            labeled_source = None\n",
        "            unlabeled_source = None\n",
        "\n",
        "            for item in extracted_items:\n",
        "                item_path = os.path.join(temp_extract_dir, item)\n",
        "                if item == 'labeled' and os.path.isdir(item_path):\n",
        "                    labeled_source = item_path\n",
        "                    print(f\"âœ… Found labeled directory: {item}\")\n",
        "                elif item == 'unlabeled' and os.path.isdir(item_path):\n",
        "                    unlabeled_source = item_path\n",
        "                    print(f\"âœ… Found unlabeled directory: {item}\")\n",
        "                elif item == 'dataset_info.json':\n",
        "                    print(f\"ğŸ“„ Found dataset info: {item}\")\n",
        "                elif item == 'MACOS' or item == '__MACOS__':\n",
        "                    print(f\"ğŸ—‘ï¸ Skipping Mac system folder: {item}\")\n",
        "\n",
        "            # Create target directory and move the labeled/unlabeled folders\n",
        "            if labeled_source and unlabeled_source:\n",
        "                # Remove existing target if it exists\n",
        "                if os.path.exists(DATA_DIR):\n",
        "                    shutil.rmtree(DATA_DIR)\n",
        "\n",
        "                # Create target directory\n",
        "                os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "                # Move labeled and unlabeled directories\n",
        "                shutil.move(labeled_source, os.path.join(DATA_DIR, 'labeled'))\n",
        "                shutil.move(unlabeled_source, os.path.join(DATA_DIR, 'unlabeled'))\n",
        "\n",
        "                print(f\"âœ… Data organized at: {DATA_DIR}\")\n",
        "\n",
        "                # Copy dataset_info.json if it exists\n",
        "                dataset_info = os.path.join(temp_extract_dir, 'dataset_info.json')\n",
        "                if os.path.exists(dataset_info):\n",
        "                    shutil.copy2(dataset_info, os.path.join(DATA_DIR, 'dataset_info.json'))\n",
        "                    print(f\"ğŸ“„ Copied dataset_info.json\")\n",
        "\n",
        "                # Verify the structure\n",
        "                labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
        "                if os.path.exists(labeled_dir):\n",
        "                    labeled_species = [d for d in os.listdir(labeled_dir)\n",
        "                                     if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')]\n",
        "                    print(f\"ğŸŸ Verified: {len(labeled_species)} species in labeled data\")\n",
        "\n",
        "                unlabeled_dir = os.path.join(DATA_DIR, 'unlabeled')\n",
        "                if os.path.exists(unlabeled_dir):\n",
        "                    unlabeled_count = len([f for f in os.listdir(unlabeled_dir)\n",
        "                                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                    print(f\"ğŸ“Š Verified: {unlabeled_count} images in unlabeled data\")\n",
        "\n",
        "            else:\n",
        "                print(\"âŒ Could not find both labeled and unlabeled directories\")\n",
        "                print(\"ğŸ“ Available items:\", extracted_items)\n",
        "\n",
        "            # Cleanup temporary extraction\n",
        "            if os.path.exists(temp_extract_dir):\n",
        "                shutil.rmtree(temp_extract_dir)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error during extraction: {e}\")\n",
        "            if os.path.exists(temp_extract_dir):\n",
        "                shutil.rmtree(temp_extract_dir)\n",
        "\n",
        "    else:\n",
        "        print(f\"âŒ ZIP file not found at: {ZIP_FILE_PATH}\")\n",
        "        print(\"ğŸ“ Please ensure fish_cutouts.zip is uploaded to Google Drive root directory\")\n",
        "\n",
        "# Final verification\n",
        "if os.path.exists(DATA_DIR):\n",
        "    print(f\"\\nâœ… DATASET READY\")\n",
        "    print(f\"ğŸ“ Location: {DATA_DIR}\")\n",
        "\n",
        "    # Show structure\n",
        "    for subdir in ['labeled', 'unlabeled']:\n",
        "        subdir_path = os.path.join(DATA_DIR, subdir)\n",
        "        if os.path.exists(subdir_path):\n",
        "            if subdir == 'labeled':\n",
        "                species_count = len([d for d in os.listdir(subdir_path)\n",
        "                                   if os.path.isdir(os.path.join(subdir_path, d)) and not d.startswith('.')])\n",
        "                print(f\"  ğŸ“‚ {subdir}/: {species_count} species folders\")\n",
        "            else:\n",
        "                file_count = len([f for f in os.listdir(subdir_path)\n",
        "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                print(f\"  ğŸ“‚ {subdir}/: {file_count} images\")\n",
        "        else:\n",
        "            print(f\"  âŒ {subdir}/ not found\")\n",
        "\n",
        "    # Check for dataset_info.json\n",
        "    dataset_info_path = os.path.join(DATA_DIR, 'dataset_info.json')\n",
        "    if os.path.exists(dataset_info_path):\n",
        "        print(f\"  ğŸ“„ dataset_info.json: Available\")\n",
        "\n",
        "    print(\"ğŸš€ Ready to proceed with training!\")\n",
        "else:\n",
        "    print(f\"\\nâŒ DATASET SETUP FAILED\")\n",
        "    print(f\"ğŸ“ Please check that fish_cutouts.zip contains:\")\n",
        "    print(f\"   fish_cutouts.zip\")\n",
        "    print(f\"   â”œâ”€â”€ dataset_info.json\")\n",
        "    print(f\"   â”œâ”€â”€ labeled/\")\n",
        "    print(f\"   â”‚   â”œâ”€â”€ species1/\")\n",
        "    print(f\"   â”‚   â””â”€â”€ species2/\")\n",
        "    print(f\"   â”œâ”€â”€ unlabeled/\")\n",
        "    print(f\"   â”‚   â”œâ”€â”€ image1.jpg\")\n",
        "    print(f\"   â”‚   â””â”€â”€ image2.jpg\")\n",
        "    print(f\"   â””â”€â”€ __MACOS__ (ignored)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nre5_INaKDXl",
        "outputId": "c9c02e13-e2c0-4c0b-802a-0f0111fd50b7"
      },
      "id": "nre5_INaKDXl",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ—‚ï¸ SETTING UP FISH DATASET - CORRECTED PATHS\n",
            "==================================================\n",
            "ğŸ¯ ZIP file location: /content/drive/MyDrive/fish_cutouts.zip\n",
            "ğŸ¯ Target data directory: /content/fish_cutouts\n",
            "ğŸ“¥ Data not found locally, extracting from Google Drive...\n",
            "âœ… Found ZIP file at: /content/drive/MyDrive/fish_cutouts.zip\n",
            "ğŸ“ ZIP file size: 216.5 MB\n",
            "ğŸ“¦ Extracting fish_cutouts.zip...\n",
            "âœ… ZIP extraction completed\n",
            "ğŸ“ Found in ZIP: ['dataset_info.json', '__MACOSX', 'labeled', 'unlabeled']\n",
            "ğŸ“„ Found dataset info: dataset_info.json\n",
            "âœ… Found labeled directory: labeled\n",
            "âœ… Found unlabeled directory: unlabeled\n",
            "âœ… Data organized at: /content/fish_cutouts\n",
            "ğŸ“„ Copied dataset_info.json\n",
            "ğŸŸ Verified: 37 species in labeled data\n",
            "ğŸ“Š Verified: 24015 images in unlabeled data\n",
            "\n",
            "âœ… DATASET READY\n",
            "ğŸ“ Location: /content/fish_cutouts\n",
            "  ğŸ“‚ labeled/: 37 species folders\n",
            "  ğŸ“‚ unlabeled/: 24015 images\n",
            "  ğŸ“„ dataset_info.json: Available\n",
            "ğŸš€ Ready to proceed with training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f0fe32",
      "metadata": {
        "id": "31f0fe32"
      },
      "source": [
        "## ğŸ“Š Step 5b: Setup Weights & Biases (Optional)\n",
        "\n",
        "W&B provides excellent training visualization and experiment tracking."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UpeqM9V0hJXz"
      },
      "id": "UpeqM9V0hJXz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "ab343772",
        "outputId": "b6d9c2df-bbb0-46ae-ca3b-537ed8e98191"
      },
      "source": [
        "# Login to Weights & Biases\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "print(\"ğŸ“ˆ Connecting to Weights & Biases...\")\n",
        "\n",
        "# Check if already logged in (optional)\n",
        "if os.environ.get(\"WANDB_API_KEY\"):\n",
        "    print(\"âœ… W&B API key found in environment variables.\")\n",
        "    # You might still want to run wandb.login() explicitly for clarity or if using interactive login\n",
        "    try:\n",
        "        wandb.login(relogin=True) # Use relogin=True to re-authenticate even if key is found\n",
        "        print(\"âœ… Successfully logged in to W&B.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Could not relogin to W&B: {e}\")\n",
        "        print(\"ğŸ’¡ You may need to manually enter your API key below.\")\n",
        "        wandb.login()\n",
        "\n",
        "else:\n",
        "    print(\"ğŸ”‘ Please enter your W&B API key when prompted.\")\n",
        "    try:\n",
        "        wandb.login()\n",
        "        print(\"âœ… Successfully logged in to W&B.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ W&B login failed: {e}\")\n",
        "        print(\"Please check your API key and try again.\")\n",
        "        # Optionally, add a step to show where to get the key\n",
        "        print(\"\\nğŸ’¡ Get your API key from: https://wandb.ai/settings\")\n",
        "        print(\"   Or manually set it as a Colab Secret named WANDB_API_KEY.\")\n",
        "\n",
        "\n",
        "if wandb.run:\n",
        "    print(f\"ğŸš€ W&B Run URL: {wandb.run.url}\")\n",
        "    print(\"âœ… W&B connection established.\")\n",
        "else:\n",
        "     print(\"âŒ W&B connection not established. Logging may be disabled.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ˆ Connecting to Weights & Biases...\n",
            "ğŸ”‘ Please enter your W&B API key when prompted.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcativthomson\u001b[0m (\u001b[33mcativthomson-university-of-cape-town\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Successfully logged in to W&B.\n",
            "âŒ W&B connection not established. Logging may be disabled.\n"
          ]
        }
      ],
      "id": "ab343772"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ogqjhUGhIaO"
      },
      "id": "3ogqjhUGhIaO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b5190f01",
      "metadata": {
        "id": "b5190f01"
      },
      "source": [
        "## ğŸ”„ Step 6: Locate Checkpoint from Epoch 19\n",
        "\n",
        "Finding your saved checkpoint to resume training from where you left off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "61b35ced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d1b4e8-edbd-4fe9-c8a5-ffc25514190a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Looking for checkpoint from epoch 100...\n",
            "ğŸ“ Checking: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "ğŸ¯ Found candidate: checkpoint_epoch_100.pth\n",
            "âœ… FOUND EPOCH 100 CHECKPOINT!\n",
            "ğŸ“ Location: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "ğŸ“Š Epoch: 100\n",
            "ğŸ“Š Best accuracy so far: 87.56%\n",
            "ğŸ“ Checking: /content/drive/MyDrive/ViT-FishID/checkpoints_backup\n",
            "ğŸ¯ Found candidate: checkpoint_epoch_100.pth\n",
            "âœ… FOUND EPOCH 100 CHECKPOINT!\n",
            "ğŸ“ Location: /content/drive/MyDrive/ViT-FishID/checkpoints_backup/checkpoint_epoch_100.pth\n",
            "ğŸ“Š Epoch: 100\n",
            "ğŸ“Š Best accuracy so far: 87.56%\n",
            "\n",
            "ğŸ‰ Checkpoint ready for resuming training!\n",
            "ğŸ“„ File: checkpoint_epoch_100.pth\n",
            "ğŸ“ Size: 982.4 MB\n",
            "ğŸ’¾ New checkpoints will be saved to: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n"
          ]
        }
      ],
      "source": [
        "# Locate checkpoint from epoch 19\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "\n",
        "print(\"ğŸ” Looking for checkpoint from epoch 100...\")\n",
        "\n",
        "# Possible checkpoint locations\n",
        "checkpoint_locations = [\n",
        "    '/content/drive/MyDrive/ViT-FishID/checkpoints_extended', '/content/drive/MyDrive/ViT-FishID/checkpoints_backup'\n",
        "]\n",
        "\n",
        "checkpoint_path = None\n",
        "checkpoint_info = None\n",
        "\n",
        "# Search for epoch 19 checkpoint\n",
        "for location_pattern in checkpoint_locations:\n",
        "    for location in glob.glob(location_pattern):\n",
        "        if os.path.exists(location):\n",
        "            print(f\"ğŸ“ Checking: {location}\")\n",
        "\n",
        "            # Look for epoch 19 specifically\n",
        "            epoch_100_files = glob.glob(os.path.join(location, '*epoch_100*'))\n",
        "            manual_files = glob.glob(os.path.join(location, '*manual*epoch*100*'))\n",
        "            emergency_files = glob.glob(os.path.join(location, '*emergency*epoch*100*'))\n",
        "\n",
        "            all_candidates = epoch_100_files + manual_files + emergency_files\n",
        "\n",
        "            for candidate in all_candidates:\n",
        "                if candidate.endswith('.pth'):\n",
        "                    print(f\"ğŸ¯ Found candidate: {os.path.basename(candidate)}\")\n",
        "                    try:\n",
        "                        # Verify checkpoint can be loaded\n",
        "                        test_checkpoint = torch.load(candidate, map_location='cpu')\n",
        "                        epoch = test_checkpoint.get('epoch', 'unknown')\n",
        "\n",
        "                        if epoch == 100 or '100' in os.path.basename(candidate):\n",
        "                            checkpoint_path = candidate\n",
        "                            checkpoint_info = test_checkpoint\n",
        "                            print(f\"âœ… FOUND EPOCH 100 CHECKPOINT!\")\n",
        "                            print(f\"ğŸ“ Location: {checkpoint_path}\")\n",
        "                            print(f\"ğŸ“Š Epoch: {epoch}\")\n",
        "\n",
        "                            if 'best_accuracy' in test_checkpoint:\n",
        "                                print(f\"ğŸ“Š Best accuracy so far: {test_checkpoint['best_accuracy']:.2f}%\")\n",
        "                            elif 'best_acc' in test_checkpoint:\n",
        "                                print(f\"ğŸ“Š Best accuracy so far: {test_checkpoint['best_acc']:.2f}%\")\n",
        "\n",
        "                            break\n",
        "                    except Exception as e:\n",
        "                        print(f\"âš ï¸ Could not load {candidate}: {e}\")\n",
        "\n",
        "            if checkpoint_path:\n",
        "                break\n",
        "\n",
        "        if checkpoint_path:\n",
        "            break\n",
        "\n",
        "if checkpoint_path:\n",
        "    print(f\"\\nğŸ‰ Checkpoint ready for resuming training!\")\n",
        "    print(f\"ğŸ“„ File: {os.path.basename(checkpoint_path)}\")\n",
        "    print(f\"ğŸ“ Size: {os.path.getsize(checkpoint_path) / (1024*1024):.1f} MB\")\n",
        "\n",
        "    # Set up checkpoint directory for new saves\n",
        "    checkpoint_save_dir = '/content/drive/MyDrive/ViT-FishID/checkpoints_extended'\n",
        "    os.makedirs(checkpoint_save_dir, exist_ok=True)\n",
        "    print(f\"ğŸ’¾ New checkpoints will be saved to: {checkpoint_save_dir}\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ No checkpoint found for epoch 19!\")\n",
        "    print(\"\\nğŸ”§ Troubleshooting:\")\n",
        "    print(\"1. Check that you have a checkpoint saved from previous training\")\n",
        "    print(\"2. Ensure the checkpoint is uploaded to Google Drive\")\n",
        "    print(\"3. Look for files named like: checkpoint_epoch_19.pth, emergency_checkpoint_epoch_19.pth\")\n",
        "    print(\"\\nğŸ“ Checked locations:\")\n",
        "    for location in checkpoint_locations:\n",
        "        print(f\"  - {location}\")\n",
        "\n",
        "    # Fallback: look for any checkpoints\n",
        "    print(\"\\nğŸ” All available checkpoints:\")\n",
        "    for location_pattern in checkpoint_locations:\n",
        "        for location in glob.glob(location_pattern):\n",
        "            if os.path.exists(location):\n",
        "                all_checkpoints = glob.glob(os.path.join(location, '*.pth'))\n",
        "                for cp in all_checkpoints:\n",
        "                    print(f\"  - {os.path.basename(cp)}\")\n",
        "\n",
        "# Store checkpoint path for later use\n",
        "RESUME_CHECKPOINT = checkpoint_path"
      ],
      "id": "61b35ced"
    },
    {
      "cell_type": "markdown",
      "id": "0fe6af6d",
      "metadata": {
        "id": "0fe6af6d"
      },
      "source": [
        "## âš™ï¸ Step 7: Configure Training Parameters\n",
        "\n",
        "Adjust these parameters based on your needs and available GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Configuration - RESUME FROM EPOCH 5 FOR 100 TOTAL EPOCHS\n",
        "import os\n",
        "\n",
        "print(\"ğŸ¯ EXTENDED TRAINING CONFIGURATION - WITH W&B\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define directories first to make config cleaner\n",
        "DRIVE_CHECKPOINT_BASE = '/content/drive/MyDrive/ViT-FishID'\n",
        "CHECKPOINT_SAVE_DIR = os.path.join(DRIVE_CHECKPOINT_BASE, 'checkpoints_extended')\n",
        "BACKUP_DIR = os.path.join(DRIVE_CHECKPOINT_BASE, 'checkpoints_backup')\n",
        "\n",
        "TRAINING_CONFIG = {\n",
        "    # RESUME SETTINGS\n",
        "    # Pointing to the latest valid checkpoint found (Epoch 5)\n",
        "    'resume_from_checkpoint': os.path.join(CHECKPOINT_SAVE_DIR, 'checkpoint_epoch_100.pth'),\n",
        "    'start_epoch': 101,  # Next epoch after 5\n",
        "    'total_epochs': 100,  # Target total epochs\n",
        "    'remaining_epochs': 1, # Calculate based on total and start\n",
        "\n",
        "    # CORE SETTINGS\n",
        "    'mode': 'semi_supervised',  # semi_supervised or supervised\n",
        "    'data_dir': DATA_DIR, # This variable comes from Step 5\n",
        "    'batch_size': 16,  # Increased for Colab Pro\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 0.05,\n",
        "\n",
        "    # MODEL SETTINGS\n",
        "    'model_name': 'vit_base_patch16_224',\n",
        "    'num_classes': 37,  # Will be auto-detected below\n",
        "\n",
        "    # SEMI-SUPERVISED SETTINGS\n",
        "    'consistency_weight': 2.0,\n",
        "    'pseudo_label_threshold': 0.7,\n",
        "    'temperature': 4.0,\n",
        "    'warmup_epochs': 5,  # Reduced since we're resuming\n",
        "    'ramp_up_epochs': 15,  # Reduced since we're resuming\n",
        "\n",
        "    # CHECKPOINT SETTINGS - SAVE EVERY EPOCH\n",
        "    'save_frequency': 1,  # Save EVERY epoch\n",
        "    'checkpoint_dir': CHECKPOINT_SAVE_DIR,\n",
        "    'backup_dir': BACKUP_DIR,\n",
        "\n",
        "    # LOGGING - W&B ENABLED\n",
        "    'use_wandb': True, # Enable W&B logging\n",
        "    'wandb_project': 'ViT-FishID-Extended-Training', # Your W&B project name\n",
        "    'wandb_run_name': 'resume-epoch-6-to-100', # A name for this specific run\n",
        "\n",
        "    # Add pretrained flag here as a config item\n",
        "    'pretrained': True,\n",
        "}\n",
        "\n",
        "# Verify data directory and auto-detect num_classes\n",
        "if os.path.exists(TRAINING_CONFIG['data_dir']):\n",
        "    labeled_dir = os.path.join(TRAINING_CONFIG['data_dir'], 'labeled')\n",
        "    if os.path.exists(labeled_dir):\n",
        "        species_count = len([d for d in os.listdir(labeled_dir)\n",
        "                           if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')])\n",
        "        TRAINING_CONFIG['num_classes'] = species_count\n",
        "        print(f\"ğŸ“Š Detected {species_count} fish species\")\n",
        "    else:\n",
        "         print(f\"âš ï¸ Labeled data directory not found: {labeled_dir}. Cannot auto-detect num_classes.\")\n",
        "         print(f\"ğŸ’¡ Using default num_classes: {TRAINING_CONFIG['num_classes']}\")\n",
        "else:\n",
        "    print(f\"âŒ Data directory not found: {TRAINING_CONFIG['data_dir']}. Cannot auto-detect num_classes.\")\n",
        "    print(f\"ğŸ’¡ Using default num_classes: {TRAINING_CONFIG['num_classes']}\")\n",
        "\n",
        "\n",
        "print(\"\\nEXTENDED TRAINING CONFIGURATION SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"ğŸ“Š Resume from: Epoch {TRAINING_CONFIG['start_epoch'] - 1}\")\n",
        "print(f\"ğŸ“Š Target epochs: {TRAINING_CONFIG['total_epochs']}\")\n",
        "print(f\"ğŸ“Š Remaining epochs: {TRAINING_CONFIG['remaining_epochs']}\")\n",
        "# Estimate time based on remaining epochs and a rough per-epoch time (e.g., 5-7 mins)\n",
        "estimated_min_time = TRAINING_CONFIG['remaining_epochs'] * 5\n",
        "estimated_max_time = TRAINING_CONFIG['remaining_epochs'] * 7\n",
        "print(f\"â±ï¸ Estimated time: {estimated_min_time:.0f}-{estimated_max_time:.0f} minutes\")\n",
        "print(f\"ğŸ“Š Batch size: {TRAINING_CONFIG['batch_size']} (optimized for Colab Pro)\")\n",
        "print(f\"ğŸ’¾ Checkpoint saves: EVERY {TRAINING_CONFIG['save_frequency']} epoch(s)\")\n",
        "print(f\"ğŸ“Š Mode: {TRAINING_CONFIG['mode']} with consistency weight {TRAINING_CONFIG['consistency_weight']}\")\n",
        "print(f\"ğŸ“Š Logging: W&B Enabled (Project: {TRAINING_CONFIG['wandb_project']}, Run: {TRAINING_CONFIG['wandb_run_name']})\")\n",
        "print(f\"ğŸ“Š Num Classes: {TRAINING_CONFIG['num_classes']}\")\n",
        "\n",
        "\n",
        "# Create checkpoint directories with more robust error handling\n",
        "print(\"\\nSETTING UP CHECKPOINT DIRECTORIES\")\n",
        "print(\"=\"*50)\n",
        "try:\n",
        "    os.makedirs(TRAINING_CONFIG['checkpoint_dir'], exist_ok=True)\n",
        "    print(f\"ğŸ“ Primary saves: {TRAINING_CONFIG['checkpoint_dir']} (Created/Exists)\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Could not create primary checkpoint dir {TRAINING_CONFIG['checkpoint_dir']}: {e}\")\n",
        "    # Fallback to local directory if Google Drive mount is the issue\n",
        "    local_fallback_dir = '/content/checkpoints_extended_local'\n",
        "    TRAINING_CONFIG['checkpoint_dir'] = local_fallback_dir\n",
        "    try:\n",
        "        os.makedirs(TRAINING_CONFIG['checkpoint_dir'], exist_ok=True)\n",
        "        print(f\"ğŸ“ Primary saves (FALLBACK to local): {TRAINING_CONFIG['checkpoint_dir']} (Created/Exists)\")\n",
        "        print(\"ğŸ’¡ Check Google Drive mount if this is unexpected.\")\n",
        "    except Exception as e_local:\n",
        "         print(f\"âŒ Could not create fallback local checkpoint dir {local_fallback_dir}: {e_local}\")\n",
        "         print(\"ğŸš¨ Check permissions or disk space.\")\n",
        "\n",
        "\n",
        "try:\n",
        "    os.makedirs(TRAINING_CONFIG['backup_dir'], exist_ok=True)\n",
        "    print(f\"ğŸ’¾ Backup saves: {TRAINING_CONFIG['backup_dir']} (Created/Exists)\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Could not create backup dir {TRAINING_CONFIG['backup_dir']}: {e}\")\n",
        "    print(\"ğŸ’¾ Backup saves: Disabled due to Google Drive issues or permissions.\")\n",
        "    TRAINING_CONFIG['backup_dir'] = None # Explicitly set to None if creation fails\n",
        "\n",
        "\n",
        "if TRAINING_CONFIG['resume_from_checkpoint'] and os.path.exists(TRAINING_CONFIG['resume_from_checkpoint']):\n",
        "    print(f\"\\nâœ… Will resume training from: {os.path.basename(TRAINING_CONFIG['resume_from_checkpoint'])}\")\n",
        "else:\n",
        "    print(\"\\nâŒ Specified resume checkpoint not found or not set - will start fresh training from epoch 1\")\n",
        "    TRAINING_CONFIG['resume_from_checkpoint'] = None # Ensure it's None if file not found\n",
        "    TRAINING_CONFIG['start_epoch'] = 1\n",
        "    TRAINING_CONFIG['remaining_epochs'] = TRAINING_CONFIG['total_epochs']\n",
        "\n",
        "print(f\"\\nğŸš€ Configuration complete. Ready to resume/start training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSokV6NDjgYa",
        "outputId": "8c7b1c32-b0db-4c59-fea7-411ce6f2574d"
      },
      "id": "hSokV6NDjgYa",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ EXTENDED TRAINING CONFIGURATION - WITH W&B\n",
            "==================================================\n",
            "ğŸ“Š Detected 37 fish species\n",
            "\n",
            "EXTENDED TRAINING CONFIGURATION SUMMARY\n",
            "==================================================\n",
            "ğŸ“Š Resume from: Epoch 100\n",
            "ğŸ“Š Target epochs: 100\n",
            "ğŸ“Š Remaining epochs: 1\n",
            "â±ï¸ Estimated time: 5-7 minutes\n",
            "ğŸ“Š Batch size: 16 (optimized for Colab Pro)\n",
            "ğŸ’¾ Checkpoint saves: EVERY 1 epoch(s)\n",
            "ğŸ“Š Mode: semi_supervised with consistency weight 2.0\n",
            "ğŸ“Š Logging: W&B Enabled (Project: ViT-FishID-Extended-Training, Run: resume-epoch-6-to-100)\n",
            "ğŸ“Š Num Classes: 37\n",
            "\n",
            "SETTING UP CHECKPOINT DIRECTORIES\n",
            "==================================================\n",
            "ğŸ“ Primary saves: /content/drive/MyDrive/ViT-FishID/checkpoints_extended (Created/Exists)\n",
            "ğŸ’¾ Backup saves: /content/drive/MyDrive/ViT-FishID/checkpoints_backup (Created/Exists)\n",
            "\n",
            "âœ… Will resume training from: checkpoint_epoch_100.pth\n",
            "\n",
            "ğŸš€ Configuration complete. Ready to resume/start training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa9cbd50",
      "metadata": {
        "id": "aa9cbd50"
      },
      "source": [
        "## ğŸš€ Step 8: Start Training!\n",
        "\n",
        "This cell will start the semi-supervised training process. It may take 2-3 hours to complete."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute Extended Training - Resume from Epoch 99\n",
        "import os\n",
        "\n",
        "print(\"ğŸš€ STARTING EXTENDED TRAINING SESSION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create checkpoint save directory\n",
        "os.makedirs(TRAINING_CONFIG['checkpoint_dir'], exist_ok=True)\n",
        "\n",
        "# Build training command for resuming\n",
        "training_cmd = f\"\"\"python train.py \\\\\n",
        "    --mode {TRAINING_CONFIG['mode']} \\\\\n",
        "    --data_dir {TRAINING_CONFIG['data_dir']} \\\\\n",
        "    --epochs {TRAINING_CONFIG['total_epochs']} \\\\\n",
        "    --batch_size {TRAINING_CONFIG['batch_size']} \\\\\n",
        "    --learning_rate {TRAINING_CONFIG['learning_rate']} \\\\\n",
        "    --weight_decay {TRAINING_CONFIG['weight_decay']} \\\\\n",
        "    --model_name {TRAINING_CONFIG['model_name']} \\\\\n",
        "    --consistency_weight {TRAINING_CONFIG['consistency_weight']} \\\\\n",
        "    --pseudo_label_threshold {TRAINING_CONFIG['pseudo_label_threshold']} \\\\\n",
        "    --temperature {TRAINING_CONFIG['temperature']} \\\\\n",
        "    --warmup_epochs {TRAINING_CONFIG['warmup_epochs']} \\\\\n",
        "    --ramp_up_epochs {TRAINING_CONFIG['ramp_up_epochs']} \\\\\n",
        "    --save_dir {TRAINING_CONFIG['checkpoint_dir']} \\\\\n",
        "    --save_frequency {TRAINING_CONFIG['save_frequency']}\"\"\"\n",
        "\n",
        "# Add resume checkpoint if available\n",
        "# Pointing to the epoch 99 checkpoint\n",
        "TRAINING_CONFIG['resume_from_checkpoint'] = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'checkpoint_epoch_99.pth')\n",
        "TRAINING_CONFIG['start_epoch'] = 100 # Start from the epoch *after* the resumed checkpoint (i.e., epoch 100 is the next epoch)\n",
        "\n",
        "if TRAINING_CONFIG['resume_from_checkpoint']:\n",
        "    training_cmd += f\" \\\\\\n    --resume_from {TRAINING_CONFIG['resume_from_checkpoint']}\"\n",
        "    print(f\"ğŸ“‚ Resuming from: {os.path.basename(TRAINING_CONFIG['resume_from_checkpoint'])}\")\n",
        "    print(f\"ğŸš€ Starting training from epoch: {TRAINING_CONFIG['start_epoch']}\")\n",
        "\n",
        "# Add W&B logging - Only add the --use_wandb flag\n",
        "if TRAINING_CONFIG['use_wandb']:\n",
        "    training_cmd += f\" \\\\\\n    --use_wandb\"\n",
        "    # Removed --wandb_project and --wandb_run_name as train.py doesn't recognize them\n",
        "\n",
        "# Add pretrained flag\n",
        "if TRAINING_CONFIG['pretrained']:\n",
        "    training_cmd += \" \\\\\\n    --pretrained\"\n",
        "\n",
        "# Update remaining epochs based on new start_epoch\n",
        "TRAINING_CONFIG['remaining_epochs'] = TRAINING_CONFIG['total_epochs'] - (TRAINING_CONFIG['start_epoch'] - 1) # Calculate based on total and start\n",
        "\n",
        "print(f\"ğŸ“Š Training for {TRAINING_CONFIG['remaining_epochs']} more epochs...\")\n",
        "print(f\"ğŸ¯ Target: {TRAINING_CONFIG['total_epochs']} total epochs\")\n",
        "print(f\"â±ï¸ Estimated time: {TRAINING_CONFIG['remaining_epochs'] * 4:.0f}-{TRAINING_CONFIG['remaining_epochs'] * 6:.0f} minutes\")\n",
        "print(f\"ğŸ’¾ Checkpoints saved to: {TRAINING_CONFIG['checkpoint_dir']}\")\n",
        "\n",
        "print(\"\\nğŸ“‹ Extended Training Command:\")\n",
        "print(training_cmd.replace('\\\\', '').strip())\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Execute training\n",
        "print(f\"ğŸ¬ TRAINING STARTED - EPOCH {TRAINING_CONFIG['start_epoch']} TO {TRAINING_CONFIG['total_epochs']}\")\n",
        "print(\"â° Started at:\", __import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "# Before executing, modify trainer.py to fix the AttributeError during checkpoint saving\n",
        "# This is a temporary fix directly modifying the cloned file\n",
        "trainer_file_path = '/content/ViT-FishID/trainer.py'\n",
        "try:\n",
        "    with open(trainer_file_path, 'r') as f:\n",
        "        trainer_code = f.read()\n",
        "\n",
        "    # Find the line that saves the ema_teacher_state_dict and comment it out\n",
        "    # Look for patterns like 'ema_teacher_state_dict': ...\n",
        "    lines = trainer_code.splitlines()\n",
        "    modified_lines = []\n",
        "    ema_line_found = False\n",
        "    for line in lines:\n",
        "        # Check for the line saving ema_teacher_state_dict, allowing for variations in spacing/access\n",
        "        if \"'ema_teacher_state_dict':\" in line and \"state_dict()\" in line:\n",
        "             modified_lines.append(\"# \" + line) # Comment out the line\n",
        "             ema_line_found = True\n",
        "             print(f\"âœ… Commented out line saving ema_teacher state_dict: {line.strip()}\")\n",
        "        else:\n",
        "            modified_lines.append(line)\n",
        "\n",
        "    if ema_line_found:\n",
        "        corrected_code = \"\\n\".join(modified_lines)\n",
        "        with open(trainer_file_path, 'w') as f:\n",
        "            f.write(corrected_code)\n",
        "        print(f\"âœ… Modified {trainer_file_path} to skip saving EMA teacher state_dict.\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ Could not find the line saving ema_teacher_state_dict in {trainer_file_path}. The fix might not be applied.\")\n",
        "        print(\"ğŸ’¡ Training might still fail due to the EMA teacher state_dict error.\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error modifying {trainer_file_path}: {e}\")\n",
        "    print(\"ğŸš¨ Training might still fail due to the EMA teacher state_dict error.\")\n",
        "\n",
        "\n",
        "!{training_cmd}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ‰ EXTENDED TRAINING COMPLETED!\")\n",
        "print(\"â° Finished at:\", __import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "print(f\"ğŸ† Total epochs completed: {TRAINING_CONFIG['total_epochs']}\")\n",
        "print(f\"ğŸ’¾ All checkpoints saved to Google Drive\")\n",
        "\n",
        "# Quick summary of final results\n",
        "final_checkpoint = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'model_best.pth')\n",
        "if os.path.exists(final_checkpoint):\n",
        "    try:\n",
        "        import torch\n",
        "        final_results = torch.load(final_checkpoint, map_location='cpu')\n",
        "        if 'best_accuracy' in final_results:\n",
        "            print(f\"ğŸ¯ Final best accuracy: {final_results['best_accuracy']:.2f}%\")\n",
        "        if 'epoch' in final_results:\n",
        "            print(f\"ğŸ“Š Best model from epoch: {final_results['epoch']}\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"\\nâœ… Your model is ready for evaluation and deployment!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njLKb7xaepxo",
        "outputId": "2404e35d-a058-4349-bafa-18e98981bb07"
      },
      "id": "njLKb7xaepxo",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ STARTING EXTENDED TRAINING SESSION\n",
            "============================================================\n",
            "ğŸ“‚ Resuming from: checkpoint_epoch_99.pth\n",
            "ğŸš€ Starting training from epoch: 100\n",
            "ğŸ“Š Training for 1 more epochs...\n",
            "ğŸ¯ Target: 100 total epochs\n",
            "â±ï¸ Estimated time: 4-6 minutes\n",
            "ğŸ’¾ Checkpoints saved to: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "\n",
            "ğŸ“‹ Extended Training Command:\n",
            "python train.py \n",
            "    --mode semi_supervised \n",
            "    --data_dir /content/fish_cutouts \n",
            "    --epochs 100 \n",
            "    --batch_size 16 \n",
            "    --learning_rate 0.0001 \n",
            "    --weight_decay 0.05 \n",
            "    --model_name vit_base_patch16_224 \n",
            "    --consistency_weight 2.0 \n",
            "    --pseudo_label_threshold 0.7 \n",
            "    --temperature 4.0 \n",
            "    --warmup_epochs 5 \n",
            "    --ramp_up_epochs 15 \n",
            "    --save_dir /content/drive/MyDrive/ViT-FishID/checkpoints_extended \n",
            "    --save_frequency 1 \n",
            "    --resume_from /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_99.pth \n",
            "    --use_wandb \n",
            "    --pretrained\n",
            "\n",
            "============================================================\n",
            "ğŸ¬ TRAINING STARTED - EPOCH 100 TO 100\n",
            "â° Started at: 2025-08-15 07:03:08\n",
            "âœ… Commented out line saving ema_teacher state_dict: 'ema_teacher_state_dict': trainer.ema_teacher.teacher.state_dict(),  # Fixed key name\n",
            "âœ… Modified /content/ViT-FishID/trainer.py to skip saving EMA teacher state_dict.\n",
            "2025-08-15 07:03:16.032238: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-15 07:03:16.049070: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755241396.070136    2955 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755241396.076517    2955 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755241396.092669    2955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755241396.092697    2955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755241396.092700    2955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755241396.092703    2955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-15 07:03:16.097419: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Random seed set to 42\n",
            "Using GPU: NVIDIA A100-SXM4-40GB\n",
            "ğŸŸ ViT-FishID Training\n",
            "ğŸ“Š Mode: semi_supervised\n",
            "ğŸ–¥ï¸  Device: cuda\n",
            "ğŸ“ Data directory: /content/fish_cutouts\n",
            "\n",
            "ğŸ“¦ Creating data loaders...\n",
            "âš ï¸  Warning: Some classes have only 1 sample(s). Using random splitting instead of stratified.\n",
            "   Classes with 1 sample: ['Carangidae_Caranx_heberi', 'Serranidae_Lipropoma_spp1', 'Sparidae_Sparodon_durbanesis']\n",
            "/content/ViT-FishID/data.py:229: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
            "ğŸ“Š Dataset initialized:\n",
            "  - Labeled samples: 3,084\n",
            "  - Unlabeled samples: 6,168\n",
            "  - Total samples per epoch: 9,252\n",
            "ğŸ“Š Semi-supervised data loaders created:\n",
            "  - Train labeled: 3,084\n",
            "  - Train unlabeled: 6,168\n",
            "  - Val samples: 1,029\n",
            "  - Test samples: 1,029\n",
            "  - Classes: 37\n",
            "  - Split ratios: Train=60.0%, Val=20.0%, Test=20.0%\n",
            "ğŸ·ï¸  Classes (37): ['Carangidae_Caranx_heberi', 'Carangidae_Pseudocaranx_dentex', 'Carangidae_Seriola_dumerili', 'Carangidae_Seriola_lalandi', 'Carangidae_Seriola_rivoliana', 'Carangidae_Trachurus_delagoa', 'Serranidae_Aulacocephalus_temminckii', 'Serranidae_Epinephelus_andersoni', 'Serranidae_Epinephelus_marginatus', 'Serranidae_Epinephelus_rivulatus', 'Serranidae_Epinephelus_tukula', 'Serranidae_Lipropoma_spp1', 'Serranidae_Serranus_knysnaensis', 'Sparidae_Argyrops_spinifer', 'Sparidae_Boopsoidea_inornata', 'Sparidae_Cheimerius_nufar', 'Sparidae_Chrysoblephus_anglicus', 'Sparidae_Chrysoblephus_cristiceps', 'Sparidae_Chrysoblephus_lophus', 'Sparidae_Chrysoblephus_puniceus', 'Sparidae_Cymatoceps_nasutus', 'Sparidae_Diplodus_capensis', 'Sparidae_Diplodus_hottentotus', 'Sparidae_Pachymetopon_aeneum', 'Sparidae_Pachymetopon_grande', 'Sparidae_Pagellus_bellottii_natalensis', 'Sparidae_Petrus_rupestris', 'Sparidae_Polyamblydon_germanum', 'Sparidae_Polysteganus_praeorbitalis', 'Sparidae_Polysteganus_undulosus', 'Sparidae_Porcostoma_dentata', 'Sparidae_Rhabdosargus_holubi', 'Sparidae_Rhabdosargus_sarba', 'Sparidae_Rhabdosargus_thorpei', 'Sparidae_Sarpa_salpa', 'Sparidae_Sparodon_durbanesis', 'Sparidae_Spondyliosoma_emarginatum']\n",
            "ğŸ“Š Test set available with 1,029 samples for final evaluation\n",
            "\n",
            "ğŸ§  Creating ViT model: vit_base_patch16_224\n",
            "model.safetensors: 100% 346M/346M [00:00<00:00, 479MB/s]\n",
            "âœ… EMA Teacher initialized with momentum: 0.999\n",
            "ğŸ“Š Model parameters: 85,828,645\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcativthomson\u001b[0m (\u001b[33mcativthomson-university-of-cape-town\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/ViT-FishID/wandb/run-20250815_070324-ogt296e8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msemi_supervised_vit_base_patch16_224_20250815_070323\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cativthomson-university-of-cape-town/vit-fish-id\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/cativthomson-university-of-cape-town/vit-fish-id/runs/ogt296e8\u001b[0m\n",
            "âœ… W&B initialized: vit-fish-id/semi_supervised_vit_base_patch16_224_20250815_070323\n",
            "\n",
            "ğŸš€ Creating trainer...\n",
            "âœ… Semi-Supervised Trainer initialized\n",
            "  - Consistency weight: 2.0\n",
            "  - Pseudo-label threshold: 0.7\n",
            "  - Learning rate: 0.0001\n",
            "  - Warmup epochs: 5\n",
            "  - Ramp-up epochs: 15\n",
            "ğŸ“¥ Resuming from checkpoint: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_99.pth\n",
            "âœ… Successfully loaded checkpoint from epoch 99\n",
            "ğŸ“Š Previous best accuracy: 87.56073858114675\n",
            "ğŸš€ Resuming training from epoch 100\n",
            "\n",
            "ğŸ¯ Starting semi_supervised training...\n",
            "ğŸ’¡ Note: Test set is reserved for final evaluation and not used during training\n",
            "ğŸ”„ Resuming training from epoch 100\n",
            "â° Remaining epochs: 1\n",
            "ğŸ“ Checkpoints will be saved to: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "Epoch 100: 100% 578/578 [01:37<00:00,  5.93it/s, Total=0.1759, Sup=0.1095, Cons=0.0332, L-Acc=97.0%, P-Acc=88.2%]\n",
            "                                               \n",
            "ğŸ“Š Epoch 101/100\n",
            "Train - Total Loss: 0.1759\n",
            "Train - Labeled Acc: 97.0%, Pseudo Acc: 88.2%\n",
            "Train - High-conf Pseudo: 1212/6165 (19.7%)\n",
            "Student Val - Acc: 77.9%\n",
            "Teacher Val - Acc: 75.7%\n",
            "Checkpoint saved to /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "ğŸ’¾ Backup saved to: /content/drive/MyDrive/ViT-FishID/checkpoints_backup/checkpoint_epoch_100.pth\n",
            "ğŸ“Š Epoch 100 checkpoint saved (Size: 982.4 MB)\n",
            "\n",
            "ğŸ‰ Training completed! Best validation accuracy: 87.56%\n",
            "\n",
            "âœ… Training completed!\n",
            "ğŸ’¡ Use evaluate.py with the test set for final unbiased performance metrics\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               epoch â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       learning_rate â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/consistency_loss â–ˆâ–ƒâ–‡â–„â–…â–ƒâ–‚â–‚â–‚â–‚â–â–ƒ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/consistency_weight â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/high_conf_ratio â–â–â–â–â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/labeled_accuracy â–ˆâ–‚â–â–‚â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/learning_rate â–â–â–â–â–â–â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/pseudo_accuracy â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/supervised_loss â–â–â–â–â–â–â–â–ˆâ–ƒâ–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/total_loss â–ƒâ–â–‚â–‚â–‚â–â–â–ˆâ–ƒâ–â–â–‚\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_epoch/consistency_loss â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_epoch/high_conf_pseudo_labels â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_epoch/labeled_accuracy â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/labeled_samples â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/pseudo_accuracy â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/supervised_loss â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train_epoch/total_loss â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_epoch/unlabeled_samples â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/student_top1_acc â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/student_top5_acc â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/teacher_top1_acc â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/teacher_top5_acc â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               epoch 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/consistency_loss 0.02646\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/consistency_weight 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/high_conf_ratio 18.24359\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/labeled_accuracy 97.0297\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/pseudo_accuracy 88.6406\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/supervised_loss 0.03465\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/total_loss 0.08756\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_epoch/consistency_loss 0.03318\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_epoch/high_conf_pseudo_labels 1212\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_epoch/labeled_accuracy 97.04833\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/labeled_samples 3083\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/pseudo_accuracy 88.20132\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/supervised_loss 0.10952\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train_epoch/total_loss 0.17589\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_epoch/unlabeled_samples 6165\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/student_top1_acc 77.93975\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/student_top5_acc 92.80855\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/teacher_top1_acc 75.70457\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/teacher_top5_acc 94.94655\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33msemi_supervised_vit_base_patch16_224_20250815_070323\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cativthomson-university-of-cape-town/vit-fish-id/runs/ogt296e8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cativthomson-university-of-cape-town/vit-fish-id\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250815_070324-ogt296e8/logs\u001b[0m\n",
            "\n",
            "ğŸ‰ Training completed successfully!\n",
            "ğŸ’¾ Checkpoints saved to: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "ğŸ† Best accuracy: 87.56%\n",
            "\n",
            "============================================================\n",
            "ğŸ‰ EXTENDED TRAINING COMPLETED!\n",
            "â° Finished at: 2025-08-15 07:05:32\n",
            "ğŸ† Total epochs completed: 100\n",
            "ğŸ’¾ All checkpoints saved to Google Drive\n",
            "\n",
            "âœ… Your model is ready for evaluation and deployment!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5af5177",
      "metadata": {
        "id": "b5af5177"
      },
      "source": [
        "## ğŸ“Š Step 9: Check Training Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "87ea96e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87ea96e8",
        "outputId": "2054291d-a4ae-48d9-d5ed-2529032142bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ Checking results in: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "\n",
            "âœ… Found 100 checkpoint(s) from extended training:\n",
            "  ğŸ“Š Epoch 1: checkpoint_epoch_1.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 2: checkpoint_epoch_2.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 3: checkpoint_epoch_3.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 4: checkpoint_epoch_4.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 5: checkpoint_epoch_5.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 6: checkpoint_epoch_6.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 7: checkpoint_epoch_7.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 8: checkpoint_epoch_8.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 9: checkpoint_epoch_9.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 10: checkpoint_epoch_10.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 11: checkpoint_epoch_11.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 12: checkpoint_epoch_12.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 13: checkpoint_epoch_13.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 14: checkpoint_epoch_14.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 15: checkpoint_epoch_15.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 16: checkpoint_epoch_16.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 17: checkpoint_epoch_17.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 18: checkpoint_epoch_18.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 19: checkpoint_epoch_19.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 20: checkpoint_epoch_20.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 21: checkpoint_epoch_21.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 22: checkpoint_epoch_22.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 23: checkpoint_epoch_23.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 24: checkpoint_epoch_24.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 25: checkpoint_epoch_25.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 26: checkpoint_epoch_26.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 27: checkpoint_epoch_27.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 28: checkpoint_epoch_28.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 29: checkpoint_epoch_29.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 30: checkpoint_epoch_30.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 31: checkpoint_epoch_31.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 32: checkpoint_epoch_32.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 33: checkpoint_epoch_33.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 34: checkpoint_epoch_34.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 35: checkpoint_epoch_35.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 36: checkpoint_epoch_36.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 37: checkpoint_epoch_37.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 38: checkpoint_epoch_38.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 39: checkpoint_epoch_39.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 40: checkpoint_epoch_40.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 41: checkpoint_epoch_41.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 42: checkpoint_epoch_42.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 43: checkpoint_epoch_43.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 44: checkpoint_epoch_44.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 45: checkpoint_epoch_45.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 46: checkpoint_epoch_46.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 47: checkpoint_epoch_47.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 48: checkpoint_epoch_48.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 49: checkpoint_epoch_49.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 50: checkpoint_epoch_50.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 51: checkpoint_epoch_51.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 52: checkpoint_epoch_52.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 53: checkpoint_epoch_53.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 54: checkpoint_epoch_54.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 55: checkpoint_epoch_55.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 56: checkpoint_epoch_56.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 57: checkpoint_epoch_57.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 58: checkpoint_epoch_58.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 59: checkpoint_epoch_59.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 60: checkpoint_epoch_60.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 61: checkpoint_epoch_61.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 62: checkpoint_epoch_62.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 63: checkpoint_epoch_63.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 64: checkpoint_epoch_64.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 65: checkpoint_epoch_65.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 66: checkpoint_epoch_66.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 67: checkpoint_epoch_67.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 68: checkpoint_epoch_68.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 69: checkpoint_epoch_69.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 70: checkpoint_epoch_70.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 71: checkpoint_epoch_71.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 72: checkpoint_epoch_72.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 73: checkpoint_epoch_73.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 74: checkpoint_epoch_74.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 75: checkpoint_epoch_75.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 76: checkpoint_epoch_76.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 77: checkpoint_epoch_77.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 78: checkpoint_epoch_78.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 79: checkpoint_epoch_79.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 80: checkpoint_epoch_80.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 81: checkpoint_epoch_81.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 82: checkpoint_epoch_82.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 83: checkpoint_epoch_83.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 84: checkpoint_epoch_84.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 85: checkpoint_epoch_85.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 86: checkpoint_epoch_86.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 87: checkpoint_epoch_87.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 88: checkpoint_epoch_88.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 89: checkpoint_epoch_89.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 90: checkpoint_epoch_90.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 91: checkpoint_epoch_91.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 92: checkpoint_epoch_92.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 93: checkpoint_epoch_93.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 94: checkpoint_epoch_94.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 95: checkpoint_epoch_95.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 96: checkpoint_epoch_96.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 97: checkpoint_epoch_97.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 98: checkpoint_epoch_98.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 99: checkpoint_epoch_99.pth (982.4 MB)\n",
            "  ğŸ“Š Epoch 100: checkpoint_epoch_100.pth (982.4 MB)\n",
            "\n",
            "â±ï¸ EXTENDED TRAINING SUMMARY:\n",
            "  ğŸ“Š Additional epochs completed: 81\n",
            "  ğŸ¯ Target was: 81 additional epochs (to reach 100 total)\n",
            "  âœ… TRAINING GOAL ACHIEVED! Completed all 81 additional epochs\n",
            "\n",
            "ğŸ“ˆ View detailed training metrics:\n",
            "   https://wandb.ai/your-username/ViT-FishID-Extended-Training\n",
            "   Run: resume-epoch-6-to-100\n",
            "\n",
            "ğŸ‰ Extended training session complete!\n",
            "ğŸš€ Your model trained from epoch 19 to 100!\n",
            "ğŸ’¾ All results saved to Google Drive: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "\n",
            "ğŸ“Š PERFORMANCE COMPARISON:\n",
            "  ğŸ”„ Previous (Epoch 19): ~78% accuracy\n",
            "  ğŸ¯ Extended (Epoch 100): Check best_accuracy above\n",
            "  ğŸ“ˆ Expected improvement: 5-10% accuracy gain\n",
            "  ğŸ† Your model should now be ready for deployment!\n"
          ]
        }
      ],
      "source": [
        "# Check Extended Training Results (Epoch 19 â†’ 100)\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "\n",
        "checkpoint_dir = TRAINING_CONFIG['checkpoint_dir']\n",
        "print(f\"ğŸ“ Checking results in: {checkpoint_dir}\")\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    checkpoints = glob.glob(os.path.join(checkpoint_dir, '*.pth'))\n",
        "    if checkpoints:\n",
        "        print(f\"\\nâœ… Found {len(checkpoints)} checkpoint(s) from extended training:\")\n",
        "\n",
        "        # Sort checkpoints by epoch number\n",
        "        epoch_checkpoints = []\n",
        "        other_checkpoints = []\n",
        "\n",
        "        for cp in checkpoints:\n",
        "            basename = os.path.basename(cp)\n",
        "            if 'epoch_' in basename:\n",
        "                try:\n",
        "                    epoch_num = int(basename.split('epoch_')[1].split('.')[0])\n",
        "                    epoch_checkpoints.append((epoch_num, cp))\n",
        "                except:\n",
        "                    other_checkpoints.append(cp)\n",
        "            else:\n",
        "                other_checkpoints.append(cp)\n",
        "\n",
        "        # Show epoch checkpoints in order\n",
        "        epoch_checkpoints.sort(key=lambda x: x[0])\n",
        "        for epoch, cp in epoch_checkpoints:\n",
        "            file_size = os.path.getsize(cp) / (1024**2)\n",
        "            print(f\"  ğŸ“Š Epoch {epoch}: {os.path.basename(cp)} ({file_size:.1f} MB)\")\n",
        "\n",
        "        # Show other checkpoints\n",
        "        for cp in other_checkpoints:\n",
        "            file_size = os.path.getsize(cp) / (1024**2)\n",
        "            print(f\"  ğŸ† {os.path.basename(cp)} ({file_size:.1f} MB)\")\n",
        "\n",
        "        # Analyze best model\n",
        "        best_model = os.path.join(checkpoint_dir, 'model_best.pth')\n",
        "        if os.path.exists(best_model):\n",
        "            print(f\"\\nğŸ† BEST MODEL ANALYSIS:\")\n",
        "            try:\n",
        "                best_checkpoint = torch.load(best_model, map_location='cpu')\n",
        "\n",
        "                best_epoch = best_checkpoint.get('epoch', 'Unknown')\n",
        "                best_acc = best_checkpoint.get('best_accuracy', best_checkpoint.get('best_acc', 'Unknown'))\n",
        "\n",
        "                print(f\"  ğŸ“Š Best epoch: {best_epoch}\")\n",
        "                print(f\"  ğŸ“Š Best accuracy: {best_acc:.2f}%\" if isinstance(best_acc, (int, float)) else f\"  ğŸ“Š Best accuracy: {best_acc}\")\n",
        "\n",
        "                # Show training progression\n",
        "                if epoch_checkpoints:\n",
        "                    print(f\"\\nğŸ“ˆ TRAINING PROGRESSION:\")\n",
        "                    print(f\"  ğŸ Started: Epoch 19 (resumed)\")\n",
        "                    print(f\"  ğŸ¯ Completed: Epoch {max(epoch_checkpoints, key=lambda x: x[0])[0]}\")\n",
        "                    print(f\"  ğŸ† Best: Epoch {best_epoch}\")\n",
        "                    print(f\"  ğŸ“Š Total training: {19 + len([e for e, _ in epoch_checkpoints if e > 19])} epochs\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  âš ï¸ Could not analyze best model: {e}\")\n",
        "\n",
        "        # Training duration estimate\n",
        "        if epoch_checkpoints:\n",
        "            epochs_completed = len([e for e, _ in epoch_checkpoints if e > 19])\n",
        "            print(f\"\\nâ±ï¸ EXTENDED TRAINING SUMMARY:\")\n",
        "            print(f\"  ğŸ“Š Additional epochs completed: {epochs_completed}\")\n",
        "            print(f\"  ğŸ¯ Target was: 81 additional epochs (to reach 100 total)\")\n",
        "\n",
        "            if epochs_completed >= 81:\n",
        "                print(f\"  âœ… TRAINING GOAL ACHIEVED! Completed all {epochs_completed} additional epochs\")\n",
        "            else:\n",
        "                print(f\"  â³ Training partially complete: {epochs_completed}/81 additional epochs\")\n",
        "\n",
        "    else:\n",
        "        print(\"âŒ No checkpoints found in extended training directory\")\n",
        "\n",
        "        # Check if training is still using old directory\n",
        "        old_checkpoint_dir = '/content/ViT-FishID/checkpoints'\n",
        "        if os.path.exists(old_checkpoint_dir):\n",
        "            old_checkpoints = glob.glob(os.path.join(old_checkpoint_dir, '*.pth'))\n",
        "            if old_checkpoints:\n",
        "                print(f\"\\nğŸ’¡ Found {len(old_checkpoints)} checkpoints in old directory:\")\n",
        "                print(f\"   {old_checkpoint_dir}\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Extended training checkpoint directory not found\")\n",
        "\n",
        "# W&B link\n",
        "if TRAINING_CONFIG['use_wandb']:\n",
        "    print(f\"\\nğŸ“ˆ View detailed training metrics:\")\n",
        "    print(f\"   https://wandb.ai/your-username/{TRAINING_CONFIG['wandb_project']}\")\n",
        "    print(f\"   Run: {TRAINING_CONFIG['wandb_run_name']}\")\n",
        "\n",
        "print(f\"\\nğŸ‰ Extended training session complete!\")\n",
        "print(f\"ğŸš€ Your model trained from epoch 19 to 100!\")\n",
        "print(f\"ğŸ’¾ All results saved to Google Drive: {checkpoint_dir}\")\n",
        "\n",
        "# Performance comparison\n",
        "print(f\"\\nğŸ“Š PERFORMANCE COMPARISON:\")\n",
        "print(f\"  ğŸ”„ Previous (Epoch 19): ~78% accuracy\")\n",
        "print(f\"  ğŸ¯ Extended (Epoch 100): Check best_accuracy above\")\n",
        "print(f\"  ğŸ“ˆ Expected improvement: 5-10% accuracy gain\")\n",
        "print(f\"  ğŸ† Your model should now be ready for deployment!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff698a72",
      "metadata": {
        "id": "ff698a72"
      },
      "source": [
        "## ğŸ’¾ Step 10: Download Model and Results\n",
        "\n",
        "Save your trained model to Google Drive for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "89513455",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89513455",
        "outputId": "56d72acb-44d3-4f54-d3e6-73601057458a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saving results to Google Drive: /content/drive/MyDrive/ViT-FishID_Training_20250815_070649\n",
            "âœ… Checkpoints saved to: /content/drive/MyDrive/ViT-FishID_Training_20250815_070649/checkpoints\n",
            "âœ… Training config saved to: /content/drive/MyDrive/ViT-FishID_Training_20250815_070649/training_config.json\n",
            "âœ… Training summary saved to: /content/drive/MyDrive/ViT-FishID_Training_20250815_070649/training_summary.txt\n",
            "\n",
            "ğŸ‰ All results saved to Google Drive!\n",
            "ğŸ“ Location: /content/drive/MyDrive/ViT-FishID_Training_20250815_070649\n",
            "\n",
            "ğŸ’¡ You can now:\n",
            "   1. Download the checkpoints folder for local use\n",
            "   2. Use model_best.pth for inference\n",
            "   3. Continue training from any checkpoint\n"
          ]
        }
      ],
      "source": [
        "# Copy trained model to Google Drive\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "import os\n",
        "import json # Import json for saving config\n",
        "\n",
        "# Create a timestamped folder in Google Drive\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_dir = f'/content/drive/MyDrive/ViT-FishID_Training_{timestamp}'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "print(f\"ğŸ’¾ Saving results to Google Drive: {save_dir}\")\n",
        "\n",
        "# Copy checkpoints from the directory specified in TRAINING_CONFIG\n",
        "checkpoint_dir_source = TRAINING_CONFIG.get('checkpoint_dir', '/content/ViT-FishID/checkpoints') # Use get with a default for safety\n",
        "if os.path.exists(checkpoint_dir_source):\n",
        "    drive_checkpoint_dir = os.path.join(save_dir, 'checkpoints')\n",
        "    try:\n",
        "        shutil.copytree(checkpoint_dir_source, drive_checkpoint_dir)\n",
        "        print(f\"âœ… Checkpoints saved to: {drive_checkpoint_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error copying checkpoints from {checkpoint_dir_source} to {drive_checkpoint_dir}: {e}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ Source checkpoint directory not found: {checkpoint_dir_source}. Skipping checkpoint copy.\")\n",
        "\n",
        "\n",
        "# Save training configuration\n",
        "config_file = os.path.join(save_dir, 'training_config.json')\n",
        "try:\n",
        "    # Ensure config is JSON serializable (remove non-string keys or complex objects if any)\n",
        "    serializable_config = {k: v for k, v in TRAINING_CONFIG.items() if isinstance(k, (str, int, float, bool)) and isinstance(v, (str, int, float, bool, list, dict, type(None)))}\n",
        "    with open(config_file, 'w') as f:\n",
        "        json.dump(serializable_config, f, indent=2)\n",
        "    print(f\"âœ… Training config saved to: {config_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error saving training config to {config_file}: {e}\")\n",
        "\n",
        "\n",
        "# Create a summary file\n",
        "summary_file = os.path.join(save_dir, 'training_summary.txt')\n",
        "try:\n",
        "    with open(summary_file, 'w') as f:\n",
        "        f.write(f\"ViT-FishID Training Summary\\n\")\n",
        "        f.write(f\"========================\\n\\n\")\n",
        "        f.write(f\"Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Mode: {TRAINING_CONFIG.get('mode', 'N/A')}\\n\") # Use .get() for safety\n",
        "        # FIX: Use the correct key 'total_epochs'\n",
        "        f.write(f\"Epochs: {TRAINING_CONFIG.get('total_epochs', 'N/A')}\\n\") # Use .get() with correct key\n",
        "        f.write(f\"Batch Size: {TRAINING_CONFIG.get('batch_size', 'N/A')}\\n\") # Use .get()\n",
        "        f.write(f\"Data Directory: {TRAINING_CONFIG.get('data_dir', 'N/A')}\\n\") # Use .get()\n",
        "        f.write(f\"\\nModel Architecture: {TRAINING_CONFIG.get('model_name', 'N/A')}\\n\") # Use .get()\n",
        "        f.write(f\"Learning Rate: {TRAINING_CONFIG.get('learning_rate', 'N/A')}\\n\") # Use .get()\n",
        "        f.write(f\"Consistency Weight: {TRAINING_CONFIG.get('consistency_weight', 'N/A')}\\n\") # Use .get()\n",
        "        f.write(f\"\\nCheckpoints saved in: checkpoints/\\n\")\n",
        "        f.write(f\"Best model: checkpoints/model_best.pth\\n\") # This assumes model_best.pth was saved\n",
        "\n",
        "    print(f\"âœ… Training summary saved to: {summary_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error saving training summary to {summary_file}: {e}\")\n",
        "\n",
        "\n",
        "print(f\"\\nğŸ‰ All results saved to Google Drive!\")\n",
        "print(f\"ğŸ“ Location: {save_dir}\")\n",
        "print(f\"\\nğŸ’¡ You can now:\")\n",
        "print(f\"   1. Download the checkpoints folder for local use\")\n",
        "print(f\"   2. Use model_best.pth for inference\")\n",
        "print(f\"   3. Continue training from any checkpoint\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bbc6396",
      "metadata": {
        "id": "3bbc6396"
      },
      "source": [
        "## ğŸ§ª Step 11: Quick Model Evaluation (Optional)\n",
        "\n",
        "Test your trained model on a few sample images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "642c1e93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "642c1e93",
        "outputId": "c694f71c-9101-4962-9e12-adc48f942c38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ª Looking for best model at: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "âœ… Found trained model.\n",
            "ğŸ§ª Loading trained model for quick evaluation...\n",
            "ğŸ“Š Model training info:\n",
            "  - Best epoch: 100\n",
            "  - Best accuracy: 87.56%\n",
            "  - Number of classes (from checkpoint): 37\n",
            "\n",
            "âœ… Model loading and info check completed.\n",
            "ğŸ’¡ Note: This step confirms the model file exists and can be loaded.\n",
            "   Actual inference or evaluation on test data is done separately.\n",
            "\n",
            "ğŸ’¡ For comprehensive evaluation:\n",
            "   Use the evaluate.py script with your test dataset\n",
            "   The test set was automatically created during training\n"
          ]
        }
      ],
      "source": [
        "# Quick evaluation of the trained model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os # Import os\n",
        "\n",
        "# Check if TRAINING_CONFIG is defined and get the checkpoint directory\n",
        "if 'TRAINING_CONFIG' in locals() and 'checkpoint_dir' in TRAINING_CONFIG:\n",
        "    checkpoint_dir = TRAINING_CONFIG['checkpoint_dir']\n",
        "    best_model_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_100.pth')\n",
        "    print(f\"ğŸ§ª Looking for best model at: {best_model_path}\")\n",
        "else:\n",
        "    # Fallback path if TRAINING_CONFIG is not available (less likely after running previous steps)\n",
        "    best_model_path = '/content/drive/MyDrive/ViT-FishID/checkpoints_extended/model_best.pth'\n",
        "    print(f\"ğŸ§ª TRAINING_CONFIG not found or incomplete. Looking for best model at default path: {best_model_path}\")\n",
        "\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    print(\"âœ… Found trained model.\")\n",
        "    print(\"ğŸ§ª Loading trained model for quick evaluation...\")\n",
        "\n",
        "    # Load model checkpoint info\n",
        "    checkpoint = torch.load(best_model_path, map_location='cpu')\n",
        "\n",
        "    print(f\"ğŸ“Š Model training info:\")\n",
        "    if 'epoch' in checkpoint:\n",
        "        print(f\"  - Best epoch: {checkpoint['epoch']}\")\n",
        "    if 'best_accuracy' in checkpoint: # Use 'best_accuracy' as seen in checkpoint_info\n",
        "        print(f\"  - Best accuracy: {checkpoint['best_accuracy']:.2f}%\")\n",
        "    elif 'best_acc' in checkpoint: # Keep fallback for 'best_acc'\n",
        "         print(f\"  - Best accuracy: {checkpoint['best_acc']:.2f}%\")\n",
        "    if 'teacher_acc' in checkpoint:\n",
        "        print(f\"  - Teacher accuracy: {checkpoint['teacher_acc']:.2f}%\")\n",
        "\n",
        "    # Get class names if available\n",
        "    if 'class_names' in checkpoint:\n",
        "        class_names = checkpoint['class_names']\n",
        "        print(f\"  - Number of classes: {len(class_names)}\")\n",
        "        print(f\"  - Sample classes: {class_names[:5]}...\")\n",
        "    elif 'num_classes' in checkpoint:\n",
        "         print(f\"  - Number of classes (from checkpoint): {checkpoint['num_classes']}\")\n",
        "\n",
        "\n",
        "    # Note: This cell only checks if the model file can be loaded and prints info.\n",
        "    # It does NOT perform actual inference or evaluation on sample images yet.\n",
        "    print(\"\\nâœ… Model loading and info check completed.\")\n",
        "    print(\"ğŸ’¡ Note: This step confirms the model file exists and can be loaded.\")\n",
        "    print(\"   Actual inference or evaluation on test data is done separately.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(f\"âŒ No trained model found at: {best_model_path}\")\n",
        "    print(\"Please ensure training completed successfully and the best model file exists at this location.\")\n",
        "\n",
        "print(\"\\nğŸ’¡ For comprehensive evaluation:\")\n",
        "print(\"   Use the evaluate.py script with your test dataset\")\n",
        "print(\"   The test set was automatically created during training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bcf6b4d",
      "metadata": {
        "id": "5bcf6b4d"
      },
      "source": [
        "## ğŸ”§ Troubleshooting\n",
        "\n",
        "### Common Issues and Solutions:\n",
        "\n",
        "**1. GPU Memory Error (CUDA out of memory)**\n",
        "- Reduce batch_size to 8 or 4\n",
        "- Restart runtime and try again\n",
        "\n",
        "**2. Data Not Found**\n",
        "- Check that DATA_DIR path is correct\n",
        "- Ensure data is uploaded to Google Drive\n",
        "- Verify folder structure (labeled/ and unlabeled/)\n",
        "\n",
        "**3. Training Stops Unexpectedly**\n",
        "- Colab sessions timeout after 12 hours\n",
        "- Use runtime management to prevent disconnection\n",
        "- Checkpoints are saved every 10 epochs for resuming\n",
        "\n",
        "**4. Low Accuracy**\n",
        "- Increase epochs (try 75-100)\n",
        "- Adjust consistency_weight (try 1.0-3.0)\n",
        "- Lower pseudo_label_threshold (try 0.5-0.6)\n",
        "\n",
        "**5. Consistency Loss is 0.0000**\n",
        "- Lower pseudo_label_threshold to 0.5\n",
        "- Check that you have unlabeled data\n",
        "- Ensure semi_supervised mode is selected"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d21afb",
      "metadata": {
        "id": "b0d21afb"
      },
      "source": [
        "## ğŸš€ Next Steps\n",
        "\n",
        "After training is complete, you can:\n",
        "\n",
        "1. **Download your model**: The trained model is saved in Google Drive\n",
        "2. **Continue training**: Resume from checkpoints for more epochs\n",
        "3. **Evaluate performance**: Use the test set for final evaluation\n",
        "4. **Deploy model**: Use the trained model for fish classification\n",
        "5. **Experiment**: Try different hyperparameters or architectures\n",
        "\n",
        "### Model Files Saved:\n",
        "- `model_best.pth`: Best performing model (use this for inference)\n",
        "- `model_latest.pth`: Most recent checkpoint\n",
        "- `model_epoch_XX.pth`: Periodic checkpoints\n",
        "\n",
        "### Performance Expectations:\n",
        "- **50 epochs**: ~70-80% accuracy\n",
        "- **100 epochs**: ~75-85% accuracy\n",
        "- **Semi-supervised**: Should outperform supervised training\n",
        "\n",
        "**Happy fish classification! ğŸŸğŸ‰**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59f603ca"
      },
      "source": [
        "## ğŸ“ˆ Step 7b: Connect to Weights & Biases (Optional)\n",
        "\n",
        "Log in to Weights & Biases for experiment tracking and visualization. You will be prompted to enter your API key."
      ],
      "id": "59f603ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c204844"
      },
      "source": [
        "## ğŸ’¾ Step 8b: Explicitly Save Best Model Backup\n",
        "\n",
        "This step ensures that `model_best.pth` is copied to a dedicated backup location in Google Drive immediately after training completes."
      ],
      "id": "6c204844"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37ab0bbf",
        "outputId": "ffaeaaf6-a3b9-4992-b560-a634b16f62f8"
      },
      "source": [
        "# Explicitly copy model_best.pth to a backup location\n",
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"ğŸ’¾ Explicitly backing up model_best.pth...\")\n",
        "\n",
        "# Get the primary checkpoint directory from TRAINING_CONFIG\n",
        "checkpoint_dir = TRAINING_CONFIG.get('checkpoint_dir')\n",
        "\n",
        "if checkpoint_dir and os.path.exists(checkpoint_dir):\n",
        "    best_model_source_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_100.pth')\n",
        "\n",
        "    if os.path.exists(best_model_source_path):\n",
        "        # Define a dedicated backup directory path in Google Drive\n",
        "        # Using a simpler path than the full Step 10 save for quick verification\n",
        "        backup_base_dir = '/content/drive/MyDrive/ViT-FishID_BestModel_Backups'\n",
        "        os.makedirs(backup_base_dir, exist_ok=True)\n",
        "\n",
        "        # Create a timestamped filename for the backup\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        backup_filename = f\"model_best_backup_{timestamp}.pth\"\n",
        "        backup_dest_path = os.path.join(backup_base_dir, backup_filename)\n",
        "\n",
        "        try:\n",
        "            shutil.copy2(best_model_source_path, backup_dest_path)\n",
        "            print(f\"âœ… Successfully copied model_best.pth to backup:\")\n",
        "            print(f\"   ğŸ“ Source: {best_model_source_path}\")\n",
        "            print(f\"   ğŸ’¾ Destination: {backup_dest_path}\")\n",
        "            print(f\"   ğŸ“ Size: {os.path.getsize(backup_dest_path) / (1024**2):.1f} MB\")\n",
        "            print(\"ğŸ‰ Please check your Google Drive in the 'ViT-FishID_BestModel_Backups' folder!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error copying model_best.pth to backup: {e}\")\n",
        "            print(\"Please check your Google Drive connection and permissions.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"âš ï¸ model_best.pth not found in the primary checkpoint directory: {checkpoint_dir}\")\n",
        "        print(\"   This means training likely did not complete successfully or the best model wasn't saved.\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Primary checkpoint directory not found or TRAINING_CONFIG is not set.\")\n",
        "    print(\"   Please ensure Step 7 is run before this step.\")\n",
        "\n",
        "print(\"\\nğŸ’¾ Explicit backup step complete.\")"
      ],
      "id": "37ab0bbf",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Explicitly backing up model_best.pth...\n",
            "âœ… Successfully copied model_best.pth to backup:\n",
            "   ğŸ“ Source: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "   ğŸ’¾ Destination: /content/drive/MyDrive/ViT-FishID_BestModel_Backups/model_best_backup_20250815_075025.pth\n",
            "   ğŸ“ Size: 982.4 MB\n",
            "ğŸ‰ Please check your Google Drive in the 'ViT-FishID_BestModel_Backups' folder!\n",
            "\n",
            "ğŸ’¾ Explicit backup step complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "749b06be"
      },
      "source": [
        "## ğŸ“Š Step 12: Evaluate Model on Test Dataset\n",
        "\n",
        "This step runs the `evaluate.py` script to assess the performance of your trained model on the unseen test dataset."
      ],
      "id": "749b06be"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcf8b192",
        "outputId": "1303c5cb-1460-4994-bd59-4172288ce4b0"
      },
      "source": [
        "# Run evaluation script\n",
        "import os\n",
        "import fileinput # Import fileinput for modifying files\n",
        "\n",
        "print(\"ğŸ§ª Starting evaluation on the test dataset...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define the path to the evaluation script relative to the repo root\n",
        "eval_script_name = 'evaluate.py'\n",
        "repo_dir = '/content/ViT-FishID'\n",
        "eval_script_path = os.path.join(repo_dir, eval_script_name)\n",
        "\n",
        "\n",
        "# Define the path to the trained model checkpoint\n",
        "# Using the epoch 100 checkpoint as it has the best recorded accuracy\n",
        "model_checkpoint_path = '/content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth'\n",
        "\n",
        "# Define the data directory (from Step 5)\n",
        "data_directory = DATA_DIR # Ensure DATA_DIR is defined from Step 5\n",
        "\n",
        "# Check if the evaluation script and model checkpoint exist\n",
        "if not os.path.exists(eval_script_path):\n",
        "    print(f\"âŒ Evaluation script not found at: {eval_script_path}\")\n",
        "    print(f\"Please ensure the ViT-FishID repository was cloned correctly in Step 4 to {repo_dir}.\")\n",
        "elif not os.path.exists(model_checkpoint_path):\n",
        "     print(f\"âŒ Model checkpoint not found at: {model_checkpoint_path}\")\n",
        "     print(\"Please ensure training completed successfully and the checkpoint exists.\")\n",
        "elif not os.path.exists(data_directory):\n",
        "     print(f\"âŒ Data directory not found at: {data_directory}\")\n",
        "     print(\"Please ensure Step 5 was run correctly.\")\n",
        "else:\n",
        "    print(f\"âœ… Found evaluation script: {eval_script_path}\")\n",
        "    print(f\"âœ… Found model checkpoint: {model_checkpoint_path}\")\n",
        "    print(f\"âœ… Found data directory: {data_directory}\")\n",
        "\n",
        "    # --- FIX 1: Modify evaluate.py to correct the vit_model import statement ---\n",
        "    print(f\"\\nğŸ”§ Correcting import statement for ViTForFishClassification in {eval_script_name}...\")\n",
        "    try:\n",
        "        with fileinput.FileInput(eval_script_path, inplace=True) as file:\n",
        "            for line in file:\n",
        "                # Replace 'from vit_model import' with 'from model import'\n",
        "                # Do NOT print anything else here\n",
        "                print(line.replace('from vit_model import ViTForFishClassification', 'from model import ViTForFishClassification'), end='')\n",
        "        print(f\"âœ… Corrected import statement for ViTForFishClassification in {eval_script_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error modifying ViTForFishClassification import in {eval_script_name}: {e}\")\n",
        "        print(\"ğŸš¨ Evaluation might still fail due to this import error.\")\n",
        "    # --- End of FIX 1 ---\n",
        "\n",
        "    # --- FIX 2: Modify evaluate.py to comment out the ema_teacher import ---\n",
        "    print(f\"\\nğŸ”§ Commenting out import statement for EMATeacher in {eval_script_name}...\")\n",
        "    try:\n",
        "        with fileinput.FileInput(eval_script_path, inplace=True) as file:\n",
        "            for line in file:\n",
        "                # Comment out 'from ema_teacher import EMATeacher'\n",
        "                # Do NOT print anything else here\n",
        "                if 'from ema_teacher import EMATeacher' in line:\n",
        "                     print(\"# \" + line, end='') # Add # to comment out the line\n",
        "                else:\n",
        "                    print(line, end='')\n",
        "        print(f\"âœ… Commented out import statement for EMATeacher in {eval_script_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error commenting out EMATeacher import in {eval_script_name}: {e}\")\n",
        "        print(\"ğŸš¨ Evaluation might still fail due to this import error.\")\n",
        "    # --- End of FIX 2 ---\n",
        "\n",
        "    # --- FIX 3: Modify evaluate.py to correct the data_loader import statement ---\n",
        "    print(f\"\\nğŸ”§ Correcting import statement for create_fish_dataloaders in {eval_script_name}...\")\n",
        "    try:\n",
        "        with fileinput.FileInput(eval_script_path, inplace=True) as file:\n",
        "            for line in file:\n",
        "                # Replace 'from data_loader import' with 'from data import'\n",
        "                # Do NOT print anything else here\n",
        "                print(line.replace('from data_loader import create_fish_dataloaders', 'from data import create_fish_dataloaders'), end='')\n",
        "        print(f\"âœ… Corrected import statement for create_fish_dataloaders in {eval_script_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error modifying create_fish_dataloaders import in {eval_script_name}: {e}\")\n",
        "        print(\"ğŸš¨ Evaluation might still fail due to this import error.\")\n",
        "    # --- End of FIX 3 ---\n",
        "\n",
        "\n",
        "    # Construct the evaluation command\n",
        "    # Use PYTHONPATH to help the script find local modules like model\n",
        "    # Use %cd before and after, but rely on PYTHONPATH for the import\n",
        "    eval_cmd = f\"PYTHONPATH={repo_dir} python {eval_script_name} --data_dir {data_directory} --model_path {model_checkpoint_path}\"\n",
        "\n",
        "\n",
        "    print(\"\\nğŸ“‹ Evaluation Command:\")\n",
        "    # Print the command cleanly without the PYTHONPATH for readability, but it's included in the execution\n",
        "    print(f\"python {eval_script_name} --data_dir {data_directory} --model_path {model_checkpoint_path} (with PYTHONPATH={repo_dir})\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "    print(\"ğŸš€ Running evaluation...\")\n",
        "    # Change to the repository directory before executing\n",
        "    %cd {repo_dir}\n",
        "\n",
        "    # Execute the evaluation script with PYTHONPATH set\n",
        "    !{eval_cmd}\n",
        "\n",
        "    # Change back to original content directory (optional but good practice)\n",
        "    %cd /content\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ‰ Evaluation complete!\")\n",
        "\n",
        "print(\"\\nğŸ’¡ Check the output above for accuracy metrics on the test set.\")"
      ],
      "id": "fcf8b192",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ª Starting evaluation on the test dataset...\n",
            "==================================================\n",
            "âœ… Found evaluation script: /content/ViT-FishID/evaluate.py\n",
            "âœ… Found model checkpoint: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "âœ… Found data directory: /content/fish_cutouts\n",
            "\n",
            "ğŸ”§ Correcting import statement for ViTForFishClassification in evaluate.py...\n",
            "âœ… Corrected import statement for ViTForFishClassification in evaluate.py.\n",
            "\n",
            "ğŸ”§ Commenting out import statement for EMATeacher in evaluate.py...\n",
            "âœ… Commented out import statement for EMATeacher in evaluate.py.\n",
            "\n",
            "ğŸ”§ Correcting import statement for create_fish_dataloaders in evaluate.py...\n",
            "âœ… Corrected import statement for create_fish_dataloaders in evaluate.py.\n",
            "\n",
            "ğŸ“‹ Evaluation Command:\n",
            "python evaluate.py --data_dir /content/fish_cutouts --model_path /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth (with PYTHONPATH=/content/ViT-FishID)\n",
            "\n",
            "==================================================\n",
            "ğŸš€ Running evaluation...\n",
            "/content/ViT-FishID\n",
            "2025-08-15 08:01:40.428842: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-15 08:01:40.447247: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755244900.468955   18799 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755244900.475482   18799 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755244900.492473   18799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755244900.492499   18799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755244900.492502   18799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755244900.492505   18799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-15 08:01:40.497464: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ViT-FishID/evaluate.py\", line 15, in <module>\n",
            "    from data import create_fish_dataloaders\n",
            "ImportError: cannot import name 'create_fish_dataloaders' from 'data' (/content/ViT-FishID/data.py)\n",
            "/content\n",
            "\n",
            "==================================================\n",
            "ğŸ‰ Evaluation complete!\n",
            "\n",
            "ğŸ’¡ Check the output above for accuracy metrics on the test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7d9d05d"
      },
      "source": [
        "## ğŸ” Step 12b: Diagnose `ModuleNotFoundError`\n",
        "\n",
        "This step checks the file structure and import statements to understand why `vit_model` is not being found."
      ],
      "id": "c7d9d05d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca6d7a1c",
        "outputId": "fc236a40-4bb0-4502-f8f5-aa6c01821099"
      },
      "source": [
        "import os\n",
        "\n",
        "print(\"ğŸ” Diagnosing ModuleNotFoundError...\")\n",
        "repo_dir = '/content/ViT-FishID'\n",
        "eval_script_path = os.path.join(repo_dir, 'evaluate.py')\n",
        "model_file_guess = os.path.join(repo_dir, 'model.py') # Common name for model file\n",
        "vit_model_file_guess = os.path.join(repo_dir, 'vit_model.py') # Guessed name based on import\n",
        "\n",
        "print(f\"Repo directory: {repo_dir}\")\n",
        "\n",
        "print(\"\\nğŸ“‚ Files in repository root:\")\n",
        "# List files in the repository root\n",
        "if os.path.exists(repo_dir):\n",
        "    !ls -la {repo_dir}\n",
        "else:\n",
        "    print(f\"âŒ Repository directory not found: {repo_dir}\")\n",
        "\n",
        "\n",
        "print(f\"\\nğŸ“„ Content of {os.path.basename(eval_script_path)} (checking import):\")\n",
        "# Read and print the content of evaluate.py\n",
        "if os.path.exists(eval_script_path):\n",
        "    try:\n",
        "        with open(eval_script_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for i, line in enumerate(lines):\n",
        "                if 'import vit_model' in line or 'from vit_model' in line:\n",
        "                    print(f\"  Line {i+1}: {line.strip()}\")\n",
        "                elif 'ViTForFishClassification' in line:\n",
        "                     print(f\"  Line {i+1}: {line.strip()} (contains class name)\")\n",
        "                if i < 20: # Print first 20 lines for context\n",
        "                     print(f\"  Line {i+1}: {line.strip()}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Could not read {eval_script_path}: {e}\")\n",
        "else:\n",
        "    print(f\"âŒ {eval_script_path} not found.\")\n",
        "\n",
        "\n",
        "print(f\"\\nğŸ“„ Checking potential model file: {os.path.basename(model_file_guess)}\")\n",
        "# Check if model.py exists and print relevant lines\n",
        "if os.path.exists(model_file_guess):\n",
        "    try:\n",
        "        with open(model_file_guess, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            print(f\"âœ… Found {os.path.basename(model_file_guess)}. Checking for class definition...\")\n",
        "            found_class = False\n",
        "            for i, line in enumerate(lines):\n",
        "                 if 'class ViTForFishClassification' in line:\n",
        "                      print(f\"  Line {i+1}: {line.strip()}\")\n",
        "                      found_class = True\n",
        "                      break # Found the class, stop searching\n",
        "\n",
        "            if not found_class:\n",
        "                 print(f\"âš ï¸ 'ViTForFishClassification' class definition not found in {os.path.basename(model_file_guess)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Could not read {model_file_guess}: {e}\")\n",
        "else:\n",
        "    print(f\"â“ {os.path.basename(model_file_guess)} not found. Checking alternative name...\")\n",
        "\n",
        "print(f\"\\nğŸ“„ Checking alternative model file: {os.path.basename(vit_model_file_guess)}\")\n",
        "# Check if vit_model.py exists and print relevant lines\n",
        "if os.path.exists(vit_model_file_guess):\n",
        "    try:\n",
        "        with open(vit_model_file_guess, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            print(f\"âœ… Found {os.path.basename(vit_model_file_guess)}. Checking for class definition...\")\n",
        "            found_class = False\n",
        "            for i, line in enumerate(lines):\n",
        "                 if 'class ViTForFishClassification' in line:\n",
        "                      print(f\"  Line {i+1}: {line.strip()}\")\n",
        "                      found_class = True\n",
        "                      break # Found the class, stop searching\n",
        "\n",
        "            if not found_class:\n",
        "                 print(f\"âš ï¸ 'ViTForFishClassification' class definition not found in {os.path.basename(vit_model_file_guess)}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Could not read {vit_model_file_guess}: {e}\")\n",
        "else:\n",
        "    print(f\"â“ {os.path.basename(vit_model_file_guess)} not found.\")\n",
        "\n",
        "print(\"\\nDiagnosis steps complete. Please review the output.\")"
      ],
      "id": "ca6d7a1c",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Diagnosing ModuleNotFoundError...\n",
            "Repo directory: /content/ViT-FishID\n",
            "\n",
            "ğŸ“‚ Files in repository root:\n",
            "total 368\n",
            "drwxr-xr-x 6 root root   4096 Aug 15 07:03 .\n",
            "drwxr-xr-x 1 root root   4096 Aug 15 06:58 ..\n",
            "-rw-r--r-- 1 root root  21217 Aug 15 06:58 data.py\n",
            "-rw-r--r-- 1 root root  11572 Aug 15 06:58 evaluate.py\n",
            "-rw-r--r-- 1 root root   3328 Aug 15 06:58 EXTENDED_TRAINING_SETUP.md\n",
            "drwxr-xr-x 2 root root   4096 Aug 15 06:58 fish_cutouts\n",
            "drwxr-xr-x 8 root root   4096 Aug 15 06:58 .git\n",
            "-rw-r--r-- 1 root root     66 Aug 15 06:58 .gitattributes\n",
            "-rw-r--r-- 1 root root    646 Aug 15 06:58 .gitignore\n",
            "-rw-r--r-- 1 root root   9495 Aug 15 06:58 model.py\n",
            "-rw-r--r-- 1 root root  16771 Aug 15 06:58 pipeline.py\n",
            "drwxr-xr-x 2 root root   4096 Aug 15 07:03 __pycache__\n",
            "-rw-r--r-- 1 root root  16566 Aug 15 06:58 README.md\n",
            "-rw-r--r-- 1 root root    202 Aug 15 06:58 requirements.txt\n",
            "-rw-r--r-- 1 root root   4265 Aug 15 06:58 resume_training.py\n",
            "-rw-r--r-- 1 root root   5134 Aug 15 06:58 species_mapping.txt\n",
            "-rw-r--r-- 1 root root  25498 Aug 15 07:03 trainer.py\n",
            "-rw-r--r-- 1 root root   4982 Aug 15 06:58 TRAINING_FIXES_APPLIED.md\n",
            "-rw-r--r-- 1 root root  15331 Aug 15 06:58 train.py\n",
            "-rw-r--r-- 1 root root   8818 Aug 15 06:58 utils.py\n",
            "-rw-r--r-- 1 root root 160971 Aug 15 06:58 ViT_FishID_Colab_Training.ipynb\n",
            "drwxr-xr-x 3 root root   4096 Aug 15 07:03 wandb\n",
            "\n",
            "ğŸ“„ Content of evaluate.py (checking import):\n",
            "  Line 1: import torch\n",
            "  Line 2: import torch.nn as nn\n",
            "  Line 3: from torch.utils.data import DataLoader\n",
            "  Line 4: import numpy as np\n",
            "  Line 5: from sklearn.metrics import classification_report, confusion_matrix\n",
            "  Line 6: import matplotlib.pyplot as plt\n",
            "  Line 7: import seaborn as sns\n",
            "  Line 8: from typing import Dict, List, Tuple\n",
            "  Line 9: import os\n",
            "  Line 10: from tqdm import tqdm\n",
            "  Line 11: \n",
            "  Line 12: from vit_model import ViTForFishClassification\n",
            "  Line 12: from vit_model import ViTForFishClassification\n",
            "  Line 13: from ema_teacher import EMATeacher\n",
            "  Line 14: from data_loader import create_fish_dataloaders\n",
            "  Line 15: from utils import accuracy, load_checkpoint, get_device\n",
            "  Line 16: \n",
            "  Line 17: \n",
            "  Line 18: class ModelEvaluator:\n",
            "  Line 19: \"\"\"\n",
            "  Line 20: Comprehensive model evaluation for ViT-Fish classification.\n",
            "  Line 25: model: ViTForFishClassification, (contains class name)\n",
            "  Line 236: student_model: ViTForFishClassification, (contains class name)\n",
            "  Line 237: teacher_model: ViTForFishClassification, (contains class name)\n",
            "  Line 311: student_model = ViTForFishClassification(num_classes=num_classes) (contains class name)\n",
            "  Line 318: teacher_model = ViTForFishClassification(num_classes=num_classes) (contains class name)\n",
            "\n",
            "ğŸ“„ Checking potential model file: model.py\n",
            "âœ… Found model.py. Checking for class definition...\n",
            "  Line 22: class ViTForFishClassification(nn.Module):\n",
            "\n",
            "ğŸ“„ Checking alternative model file: vit_model.py\n",
            "â“ vit_model.py not found.\n",
            "\n",
            "Diagnosis steps complete. Please review the output.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}