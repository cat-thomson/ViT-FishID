{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0e0af9a0",
      "metadata": {
        "id": "0e0af9a0"
      },
      "source": [
        "# 🚀 ViT-FishID: Resume Training from Epoch 19\n",
        "\n",
        "**COLAB PRO EXTENDED TRAINING**\n",
        "- Resume from: Epoch 19 checkpoint\n",
        "- Target epochs: 100 total epochs (81 remaining)\n",
        "- Expected training time: 6-8 hours with Colab Pro\n",
        "- GPU: Tesla T4/V100/A100 (depending on availability)\n",
        "\n",
        "This notebook will:\n",
        "1. ✅ Resume training from your saved checkpoint at epoch 19\n",
        "2. ✅ Train for 100 total epochs (81 more epochs)\n",
        "3. ✅ Save checkpoints to Google Drive every 10 epochs\n",
        "4. ✅ Use semi-supervised learning with your fish dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "796c6e33",
      "metadata": {
        "id": "796c6e33"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cat-thomson/ViT-FishID/blob/main/ViT_FishID_Colab_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f2f60df",
      "metadata": {
        "id": "1f2f60df"
      },
      "source": [
        "# 🐟 ViT-FishID: Extended Training Session\n",
        "\n",
        "**RESUME FROM EPOCH 19 - COLAB PRO**\n",
        "\n",
        "This notebook resumes training from your saved checkpoint and runs for 100 total epochs.\n",
        "\n",
        "**Current Status:**\n",
        "- ✅ Previous training: 19 epochs completed\n",
        "- 🎯 Target: 100 total epochs (81 remaining)\n",
        "- ⏱️ Expected time: 6-8 hours with Colab Pro\n",
        "- 💾 Auto-save every 10 epochs to Google Drive\n",
        "\n",
        "**Performance Target:**\n",
        "- Previous: ~78% validation accuracy at epoch 19\n",
        "- Expected: 85-90% accuracy after 100 epochs\n",
        "- Memory: ~8-12GB GPU memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26bcb2a3",
      "metadata": {
        "id": "26bcb2a3"
      },
      "source": [
        "## 🚀 Step 1: Setup and GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f3540b19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3540b19",
        "outputId": "4859151f-eadc-4319-a3ac-b5f70e71df87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 System Information:\n",
            "Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "GPU Device: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory: 39.6 GB\n",
            "✅ GPU is ready for training!\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"🔍 System Information:\")\n",
        "print(f\"Python version: {os.sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    print(\"✅ GPU is ready for training!\")\n",
        "else:\n",
        "    print(\"❌ No GPU detected. Please enable GPU runtime:\")\n",
        "    print(\"   Runtime → Change runtime type → Hardware accelerator → GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149f671b",
      "metadata": {
        "id": "149f671b"
      },
      "source": [
        "## 📁 Step 2: Mount Google Drive\n",
        "\n",
        "This will give us access to your fish dataset stored in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4abb3ffd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4abb3ffd",
        "outputId": "29fb191e-611d-4f2d-a4d5-69fec2c1d936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "📂 Google Drive contents:\n",
            "  - Mock Matric\n",
            "  - Photos\n",
            "  - Admin\n",
            "  - Uni\n",
            "  - Fish_Training_Output\n",
            "  - Colab Notebooks\n",
            "  - ViT-FishID\n",
            "  - fish_cutouts.zip\n",
            "  - ViT-FishID_Training_20250814_154652\n",
            "  - ViT-FishID_Training_20250814_202307\n",
            "  ... and 3 more items\n",
            "\n",
            "✅ Google Drive mounted successfully!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# List contents to verify mount\n",
        "print(\"\\n📂 Google Drive contents:\")\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "if os.path.exists(drive_path):\n",
        "    items = os.listdir(drive_path)[:10]  # Show first 10 items\n",
        "    for item in items:\n",
        "        print(f\"  - {item}\")\n",
        "    if len(os.listdir(drive_path)) > 10:\n",
        "        print(f\"  ... and {len(os.listdir(drive_path)) - 10} more items\")\n",
        "    print(\"\\n✅ Google Drive mounted successfully!\")\n",
        "else:\n",
        "    print(\"❌ Failed to mount Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be8b6273",
      "metadata": {
        "id": "be8b6273"
      },
      "source": [
        "## 📦 Step 3: Install Dependencies\n",
        "\n",
        "Installing all required packages for ViT-FishID training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8c724abc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c724abc",
        "outputId": "77da503f-48d7-49ca-dc48-111168649d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Installing dependencies...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ All dependencies installed successfully!\n",
            "\n",
            "📋 Package versions:\n",
            "  - torch: 2.6.0+cu124\n",
            "  - torchvision: 0.21.0+cu124\n",
            "  - timm: 1.0.19\n",
            "  - albumentations: 2.0.8\n",
            "  - opencv: 4.12.0\n",
            "  - sklearn: 1.6.1\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "print(\"📦 Installing dependencies...\")\n",
        "\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q timm transformers\n",
        "!pip install -q albumentations\n",
        "!pip install -q wandb\n",
        "!pip install -q opencv-python-headless\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q matplotlib seaborn\n",
        "!pip install -q tqdm\n",
        "\n",
        "print(\"✅ All dependencies installed successfully!\")\n",
        "\n",
        "# Verify installations\n",
        "import torch\n",
        "import torchvision\n",
        "import timm\n",
        "import albumentations\n",
        "import cv2\n",
        "import sklearn\n",
        "\n",
        "print(\"\\n📋 Package versions:\")\n",
        "print(f\"  - torch: {torch.__version__}\")\n",
        "print(f\"  - torchvision: {torchvision.__version__}\")\n",
        "print(f\"  - timm: {timm.__version__}\")\n",
        "print(f\"  - albumentations: {albumentations.__version__}\")\n",
        "print(f\"  - opencv: {cv2.__version__}\")\n",
        "print(f\"  - sklearn: {sklearn.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b795fc",
      "metadata": {
        "id": "12b795fc"
      },
      "source": [
        "## 🔄 Step 4: Clone ViT-FishID Repository\n",
        "\n",
        "Getting the latest code from your GitHub repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c4e4cd45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4e4cd45",
        "outputId": "8efc760e-aea2-48bd-f684-8f9d338697e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Cloning ViT-FishID repository...\n",
            "Cloning into '/content/ViT-FishID'...\n",
            "remote: Enumerating objects: 119, done.\u001b[K\n",
            "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 119 (delta 44), reused 98 (delta 27), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (119/119), 201.94 KiB | 20.19 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n",
            "/content/ViT-FishID\n",
            "\n",
            "📂 Project structure:\n",
            "total 360\n",
            "drwxr-xr-x 4 root root   4096 Aug 15 06:58 .\n",
            "drwxr-xr-x 1 root root   4096 Aug 15 06:58 ..\n",
            "-rw-r--r-- 1 root root  21217 Aug 15 06:58 data.py\n",
            "-rw-r--r-- 1 root root  11572 Aug 15 06:58 evaluate.py\n",
            "-rw-r--r-- 1 root root   3328 Aug 15 06:58 EXTENDED_TRAINING_SETUP.md\n",
            "drwxr-xr-x 2 root root   4096 Aug 15 06:58 fish_cutouts\n",
            "drwxr-xr-x 8 root root   4096 Aug 15 06:58 .git\n",
            "-rw-r--r-- 1 root root     66 Aug 15 06:58 .gitattributes\n",
            "-rw-r--r-- 1 root root    646 Aug 15 06:58 .gitignore\n",
            "-rw-r--r-- 1 root root   9495 Aug 15 06:58 model.py\n",
            "-rw-r--r-- 1 root root  16771 Aug 15 06:58 pipeline.py\n",
            "-rw-r--r-- 1 root root  16566 Aug 15 06:58 README.md\n",
            "-rw-r--r-- 1 root root    202 Aug 15 06:58 requirements.txt\n",
            "-rw-r--r-- 1 root root   4265 Aug 15 06:58 resume_training.py\n",
            "-rw-r--r-- 1 root root   5134 Aug 15 06:58 species_mapping.txt\n",
            "-rw-r--r-- 1 root root  25497 Aug 15 06:58 trainer.py\n",
            "-rw-r--r-- 1 root root   4982 Aug 15 06:58 TRAINING_FIXES_APPLIED.md\n",
            "-rw-r--r-- 1 root root  15331 Aug 15 06:58 train.py\n",
            "-rw-r--r-- 1 root root   8818 Aug 15 06:58 utils.py\n",
            "-rw-r--r-- 1 root root 160971 Aug 15 06:58 ViT_FishID_Colab_Training.ipynb\n",
            "\n",
            "✅ Repository cloned successfully!\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "import os\n",
        "\n",
        "# Remove existing directory if it exists\n",
        "if os.path.exists('/content/ViT-FishID'):\n",
        "    !rm -rf /content/ViT-FishID\n",
        "\n",
        "# Clone the repository\n",
        "print(\"📥 Cloning ViT-FishID repository...\")\n",
        "!git clone https://github.com/cat-thomson/ViT-FishID.git /content/ViT-FishID\n",
        "\n",
        "# Change to project directory\n",
        "%cd /content/ViT-FishID\n",
        "\n",
        "# List project files\n",
        "print(\"\\n📂 Project structure:\")\n",
        "!ls -la\n",
        "\n",
        "print(\"\\n✅ Repository cloned successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8155c400",
      "metadata": {
        "id": "8155c400"
      },
      "source": [
        "## 🗂️ Step 5: Setup Data Path and Extraction\n",
        "\n",
        "**IMPORTANT:** Specify the path to your fish dataset ZIP file in Google Drive.\n",
        "\n",
        "This step will:\n",
        "1. Locate your `fish_cutouts.zip` file in Google Drive\n",
        "2. Extract it to Colab's local storage for faster access\n",
        "3. Validate the data structure\n",
        "\n",
        "Expected structure after extraction:\n",
        "```\n",
        "fish_cutouts/\n",
        "├── labeled/\n",
        "│   ├── species_1/\n",
        "│   │   ├── fish_001.jpg\n",
        "│   │   └── fish_002.jpg\n",
        "│   └── species_2/\n",
        "│       └── ...\n",
        "└── unlabeled/\n",
        "    ├── fish_003.jpg\n",
        "    └── fish_004.jpg\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup data path and extraction - CORRECTED PATHS\n",
        "import zipfile\n",
        "import shutil\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(\"🗂️ SETTING UP FISH DATASET - CORRECTED PATHS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Configuration - CORRECTED file paths\n",
        "ZIP_FILE_PATH = '/content/drive/MyDrive/fish_cutouts.zip'  # Correct location\n",
        "DATA_DIR = '/content/fish_cutouts'\n",
        "\n",
        "print(f\"🎯 ZIP file location: {ZIP_FILE_PATH}\")\n",
        "print(f\"🎯 Target data directory: {DATA_DIR}\")\n",
        "\n",
        "# Check if data already exists locally (from previous session)\n",
        "if os.path.exists(DATA_DIR) and os.path.exists(os.path.join(DATA_DIR, 'labeled')):\n",
        "    print(\"✅ Data already available locally from previous session!\")\n",
        "\n",
        "    # Quick validation\n",
        "    labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
        "    unlabeled_dir = os.path.join(DATA_DIR, 'unlabeled')\n",
        "\n",
        "    if os.path.exists(labeled_dir):\n",
        "        labeled_species = [d for d in os.listdir(labeled_dir)\n",
        "                          if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')]\n",
        "        print(f\"🐟 Found {len(labeled_species)} labeled species\")\n",
        "\n",
        "    if os.path.exists(unlabeled_dir):\n",
        "        unlabeled_files = [f for f in os.listdir(unlabeled_dir)\n",
        "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        print(f\"📊 Found {len(unlabeled_files)} unlabeled images\")\n",
        "\n",
        "    print(\"✅ Data validation passed - ready for training!\")\n",
        "\n",
        "else:\n",
        "    print(\"📥 Data not found locally, extracting from Google Drive...\")\n",
        "\n",
        "    # Check if ZIP file exists\n",
        "    if os.path.exists(ZIP_FILE_PATH):\n",
        "        print(f\"✅ Found ZIP file at: {ZIP_FILE_PATH}\")\n",
        "        print(f\"📏 ZIP file size: {os.path.getsize(ZIP_FILE_PATH) / (1024**2):.1f} MB\")\n",
        "\n",
        "        # Clean extraction\n",
        "        temp_extract_dir = '/content/temp_fish_extract'\n",
        "        if os.path.exists(temp_extract_dir):\n",
        "            shutil.rmtree(temp_extract_dir)\n",
        "\n",
        "        try:\n",
        "            # Extract ZIP file directly\n",
        "            print(f\"📦 Extracting {os.path.basename(ZIP_FILE_PATH)}...\")\n",
        "            with zipfile.ZipFile(ZIP_FILE_PATH, 'r') as zip_ref:\n",
        "                zip_ref.extractall(temp_extract_dir)\n",
        "\n",
        "            print(\"✅ ZIP extraction completed\")\n",
        "\n",
        "            # Check what was extracted\n",
        "            extracted_items = os.listdir(temp_extract_dir)\n",
        "            print(f\"📁 Found in ZIP: {extracted_items}\")\n",
        "\n",
        "            # Based on your description: dataset_info.json, labeled, unlabeled, MACOS\n",
        "            # Look for labeled and unlabeled directories directly\n",
        "            labeled_source = None\n",
        "            unlabeled_source = None\n",
        "\n",
        "            for item in extracted_items:\n",
        "                item_path = os.path.join(temp_extract_dir, item)\n",
        "                if item == 'labeled' and os.path.isdir(item_path):\n",
        "                    labeled_source = item_path\n",
        "                    print(f\"✅ Found labeled directory: {item}\")\n",
        "                elif item == 'unlabeled' and os.path.isdir(item_path):\n",
        "                    unlabeled_source = item_path\n",
        "                    print(f\"✅ Found unlabeled directory: {item}\")\n",
        "                elif item == 'dataset_info.json':\n",
        "                    print(f\"📄 Found dataset info: {item}\")\n",
        "                elif item == 'MACOS' or item == '__MACOS__':\n",
        "                    print(f\"🗑️ Skipping Mac system folder: {item}\")\n",
        "\n",
        "            # Create target directory and move the labeled/unlabeled folders\n",
        "            if labeled_source and unlabeled_source:\n",
        "                # Remove existing target if it exists\n",
        "                if os.path.exists(DATA_DIR):\n",
        "                    shutil.rmtree(DATA_DIR)\n",
        "\n",
        "                # Create target directory\n",
        "                os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "                # Move labeled and unlabeled directories\n",
        "                shutil.move(labeled_source, os.path.join(DATA_DIR, 'labeled'))\n",
        "                shutil.move(unlabeled_source, os.path.join(DATA_DIR, 'unlabeled'))\n",
        "\n",
        "                print(f\"✅ Data organized at: {DATA_DIR}\")\n",
        "\n",
        "                # Copy dataset_info.json if it exists\n",
        "                dataset_info = os.path.join(temp_extract_dir, 'dataset_info.json')\n",
        "                if os.path.exists(dataset_info):\n",
        "                    shutil.copy2(dataset_info, os.path.join(DATA_DIR, 'dataset_info.json'))\n",
        "                    print(f\"📄 Copied dataset_info.json\")\n",
        "\n",
        "                # Verify the structure\n",
        "                labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
        "                if os.path.exists(labeled_dir):\n",
        "                    labeled_species = [d for d in os.listdir(labeled_dir)\n",
        "                                     if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')]\n",
        "                    print(f\"🐟 Verified: {len(labeled_species)} species in labeled data\")\n",
        "\n",
        "                unlabeled_dir = os.path.join(DATA_DIR, 'unlabeled')\n",
        "                if os.path.exists(unlabeled_dir):\n",
        "                    unlabeled_count = len([f for f in os.listdir(unlabeled_dir)\n",
        "                                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                    print(f\"📊 Verified: {unlabeled_count} images in unlabeled data\")\n",
        "\n",
        "            else:\n",
        "                print(\"❌ Could not find both labeled and unlabeled directories\")\n",
        "                print(\"📁 Available items:\", extracted_items)\n",
        "\n",
        "            # Cleanup temporary extraction\n",
        "            if os.path.exists(temp_extract_dir):\n",
        "                shutil.rmtree(temp_extract_dir)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error during extraction: {e}\")\n",
        "            if os.path.exists(temp_extract_dir):\n",
        "                shutil.rmtree(temp_extract_dir)\n",
        "\n",
        "    else:\n",
        "        print(f\"❌ ZIP file not found at: {ZIP_FILE_PATH}\")\n",
        "        print(\"📝 Please ensure fish_cutouts.zip is uploaded to Google Drive root directory\")\n",
        "\n",
        "# Final verification\n",
        "if os.path.exists(DATA_DIR):\n",
        "    print(f\"\\n✅ DATASET READY\")\n",
        "    print(f\"📁 Location: {DATA_DIR}\")\n",
        "\n",
        "    # Show structure\n",
        "    for subdir in ['labeled', 'unlabeled']:\n",
        "        subdir_path = os.path.join(DATA_DIR, subdir)\n",
        "        if os.path.exists(subdir_path):\n",
        "            if subdir == 'labeled':\n",
        "                species_count = len([d for d in os.listdir(subdir_path)\n",
        "                                   if os.path.isdir(os.path.join(subdir_path, d)) and not d.startswith('.')])\n",
        "                print(f\"  📂 {subdir}/: {species_count} species folders\")\n",
        "            else:\n",
        "                file_count = len([f for f in os.listdir(subdir_path)\n",
        "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                print(f\"  📂 {subdir}/: {file_count} images\")\n",
        "        else:\n",
        "            print(f\"  ❌ {subdir}/ not found\")\n",
        "\n",
        "    # Check for dataset_info.json\n",
        "    dataset_info_path = os.path.join(DATA_DIR, 'dataset_info.json')\n",
        "    if os.path.exists(dataset_info_path):\n",
        "        print(f\"  📄 dataset_info.json: Available\")\n",
        "\n",
        "    print(\"🚀 Ready to proceed with training!\")\n",
        "else:\n",
        "    print(f\"\\n❌ DATASET SETUP FAILED\")\n",
        "    print(f\"📝 Please check that fish_cutouts.zip contains:\")\n",
        "    print(f\"   fish_cutouts.zip\")\n",
        "    print(f\"   ├── dataset_info.json\")\n",
        "    print(f\"   ├── labeled/\")\n",
        "    print(f\"   │   ├── species1/\")\n",
        "    print(f\"   │   └── species2/\")\n",
        "    print(f\"   ├── unlabeled/\")\n",
        "    print(f\"   │   ├── image1.jpg\")\n",
        "    print(f\"   │   └── image2.jpg\")\n",
        "    print(f\"   └── __MACOS__ (ignored)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nre5_INaKDXl",
        "outputId": "c9c02e13-e2c0-4c0b-802a-0f0111fd50b7"
      },
      "id": "nre5_INaKDXl",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🗂️ SETTING UP FISH DATASET - CORRECTED PATHS\n",
            "==================================================\n",
            "🎯 ZIP file location: /content/drive/MyDrive/fish_cutouts.zip\n",
            "🎯 Target data directory: /content/fish_cutouts\n",
            "📥 Data not found locally, extracting from Google Drive...\n",
            "✅ Found ZIP file at: /content/drive/MyDrive/fish_cutouts.zip\n",
            "📏 ZIP file size: 216.5 MB\n",
            "📦 Extracting fish_cutouts.zip...\n",
            "✅ ZIP extraction completed\n",
            "📁 Found in ZIP: ['dataset_info.json', '__MACOSX', 'labeled', 'unlabeled']\n",
            "📄 Found dataset info: dataset_info.json\n",
            "✅ Found labeled directory: labeled\n",
            "✅ Found unlabeled directory: unlabeled\n",
            "✅ Data organized at: /content/fish_cutouts\n",
            "📄 Copied dataset_info.json\n",
            "🐟 Verified: 37 species in labeled data\n",
            "📊 Verified: 24015 images in unlabeled data\n",
            "\n",
            "✅ DATASET READY\n",
            "📁 Location: /content/fish_cutouts\n",
            "  📂 labeled/: 37 species folders\n",
            "  📂 unlabeled/: 24015 images\n",
            "  📄 dataset_info.json: Available\n",
            "🚀 Ready to proceed with training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f0fe32",
      "metadata": {
        "id": "31f0fe32"
      },
      "source": [
        "## 📊 Step 5b: Setup Weights & Biases (Optional)\n",
        "\n",
        "W&B provides excellent training visualization and experiment tracking."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UpeqM9V0hJXz"
      },
      "id": "UpeqM9V0hJXz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "ab343772",
        "outputId": "b6d9c2df-bbb0-46ae-ca3b-537ed8e98191"
      },
      "source": [
        "# Login to Weights & Biases\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "print(\"📈 Connecting to Weights & Biases...\")\n",
        "\n",
        "# Check if already logged in (optional)\n",
        "if os.environ.get(\"WANDB_API_KEY\"):\n",
        "    print(\"✅ W&B API key found in environment variables.\")\n",
        "    # You might still want to run wandb.login() explicitly for clarity or if using interactive login\n",
        "    try:\n",
        "        wandb.login(relogin=True) # Use relogin=True to re-authenticate even if key is found\n",
        "        print(\"✅ Successfully logged in to W&B.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not relogin to W&B: {e}\")\n",
        "        print(\"💡 You may need to manually enter your API key below.\")\n",
        "        wandb.login()\n",
        "\n",
        "else:\n",
        "    print(\"🔑 Please enter your W&B API key when prompted.\")\n",
        "    try:\n",
        "        wandb.login()\n",
        "        print(\"✅ Successfully logged in to W&B.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ W&B login failed: {e}\")\n",
        "        print(\"Please check your API key and try again.\")\n",
        "        # Optionally, add a step to show where to get the key\n",
        "        print(\"\\n💡 Get your API key from: https://wandb.ai/settings\")\n",
        "        print(\"   Or manually set it as a Colab Secret named WANDB_API_KEY.\")\n",
        "\n",
        "\n",
        "if wandb.run:\n",
        "    print(f\"🚀 W&B Run URL: {wandb.run.url}\")\n",
        "    print(\"✅ W&B connection established.\")\n",
        "else:\n",
        "     print(\"❌ W&B connection not established. Logging may be disabled.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📈 Connecting to Weights & Biases...\n",
            "🔑 Please enter your W&B API key when prompted.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcativthomson\u001b[0m (\u001b[33mcativthomson-university-of-cape-town\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully logged in to W&B.\n",
            "❌ W&B connection not established. Logging may be disabled.\n"
          ]
        }
      ],
      "id": "ab343772"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ogqjhUGhIaO"
      },
      "id": "3ogqjhUGhIaO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b5190f01",
      "metadata": {
        "id": "b5190f01"
      },
      "source": [
        "## 🔄 Step 6: Locate Checkpoint from Epoch 19\n",
        "\n",
        "Finding your saved checkpoint to resume training from where you left off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "61b35ced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d1b4e8-edbd-4fe9-c8a5-ffc25514190a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Looking for checkpoint from epoch 100...\n",
            "📁 Checking: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "🎯 Found candidate: checkpoint_epoch_100.pth\n",
            "✅ FOUND EPOCH 100 CHECKPOINT!\n",
            "📁 Location: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "📊 Epoch: 100\n",
            "📊 Best accuracy so far: 87.56%\n",
            "📁 Checking: /content/drive/MyDrive/ViT-FishID/checkpoints_backup\n",
            "🎯 Found candidate: checkpoint_epoch_100.pth\n",
            "✅ FOUND EPOCH 100 CHECKPOINT!\n",
            "📁 Location: /content/drive/MyDrive/ViT-FishID/checkpoints_backup/checkpoint_epoch_100.pth\n",
            "📊 Epoch: 100\n",
            "📊 Best accuracy so far: 87.56%\n",
            "\n",
            "🎉 Checkpoint ready for resuming training!\n",
            "📄 File: checkpoint_epoch_100.pth\n",
            "📏 Size: 982.4 MB\n",
            "💾 New checkpoints will be saved to: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n"
          ]
        }
      ],
      "source": [
        "# Locate checkpoint from epoch 19\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "\n",
        "print(\"🔍 Looking for checkpoint from epoch 100...\")\n",
        "\n",
        "# Possible checkpoint locations\n",
        "checkpoint_locations = [\n",
        "    '/content/drive/MyDrive/ViT-FishID/checkpoints_extended', '/content/drive/MyDrive/ViT-FishID/checkpoints_backup'\n",
        "]\n",
        "\n",
        "checkpoint_path = None\n",
        "checkpoint_info = None\n",
        "\n",
        "# Search for epoch 19 checkpoint\n",
        "for location_pattern in checkpoint_locations:\n",
        "    for location in glob.glob(location_pattern):\n",
        "        if os.path.exists(location):\n",
        "            print(f\"📁 Checking: {location}\")\n",
        "\n",
        "            # Look for epoch 19 specifically\n",
        "            epoch_100_files = glob.glob(os.path.join(location, '*epoch_100*'))\n",
        "            manual_files = glob.glob(os.path.join(location, '*manual*epoch*100*'))\n",
        "            emergency_files = glob.glob(os.path.join(location, '*emergency*epoch*100*'))\n",
        "\n",
        "            all_candidates = epoch_100_files + manual_files + emergency_files\n",
        "\n",
        "            for candidate in all_candidates:\n",
        "                if candidate.endswith('.pth'):\n",
        "                    print(f\"🎯 Found candidate: {os.path.basename(candidate)}\")\n",
        "                    try:\n",
        "                        # Verify checkpoint can be loaded\n",
        "                        test_checkpoint = torch.load(candidate, map_location='cpu')\n",
        "                        epoch = test_checkpoint.get('epoch', 'unknown')\n",
        "\n",
        "                        if epoch == 100 or '100' in os.path.basename(candidate):\n",
        "                            checkpoint_path = candidate\n",
        "                            checkpoint_info = test_checkpoint\n",
        "                            print(f\"✅ FOUND EPOCH 100 CHECKPOINT!\")\n",
        "                            print(f\"📁 Location: {checkpoint_path}\")\n",
        "                            print(f\"📊 Epoch: {epoch}\")\n",
        "\n",
        "                            if 'best_accuracy' in test_checkpoint:\n",
        "                                print(f\"📊 Best accuracy so far: {test_checkpoint['best_accuracy']:.2f}%\")\n",
        "                            elif 'best_acc' in test_checkpoint:\n",
        "                                print(f\"📊 Best accuracy so far: {test_checkpoint['best_acc']:.2f}%\")\n",
        "\n",
        "                            break\n",
        "                    except Exception as e:\n",
        "                        print(f\"⚠️ Could not load {candidate}: {e}\")\n",
        "\n",
        "            if checkpoint_path:\n",
        "                break\n",
        "\n",
        "        if checkpoint_path:\n",
        "            break\n",
        "\n",
        "if checkpoint_path:\n",
        "    print(f\"\\n🎉 Checkpoint ready for resuming training!\")\n",
        "    print(f\"📄 File: {os.path.basename(checkpoint_path)}\")\n",
        "    print(f\"📏 Size: {os.path.getsize(checkpoint_path) / (1024*1024):.1f} MB\")\n",
        "\n",
        "    # Set up checkpoint directory for new saves\n",
        "    checkpoint_save_dir = '/content/drive/MyDrive/ViT-FishID/checkpoints_extended'\n",
        "    os.makedirs(checkpoint_save_dir, exist_ok=True)\n",
        "    print(f\"💾 New checkpoints will be saved to: {checkpoint_save_dir}\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ No checkpoint found for epoch 19!\")\n",
        "    print(\"\\n🔧 Troubleshooting:\")\n",
        "    print(\"1. Check that you have a checkpoint saved from previous training\")\n",
        "    print(\"2. Ensure the checkpoint is uploaded to Google Drive\")\n",
        "    print(\"3. Look for files named like: checkpoint_epoch_19.pth, emergency_checkpoint_epoch_19.pth\")\n",
        "    print(\"\\n📁 Checked locations:\")\n",
        "    for location in checkpoint_locations:\n",
        "        print(f\"  - {location}\")\n",
        "\n",
        "    # Fallback: look for any checkpoints\n",
        "    print(\"\\n🔍 All available checkpoints:\")\n",
        "    for location_pattern in checkpoint_locations:\n",
        "        for location in glob.glob(location_pattern):\n",
        "            if os.path.exists(location):\n",
        "                all_checkpoints = glob.glob(os.path.join(location, '*.pth'))\n",
        "                for cp in all_checkpoints:\n",
        "                    print(f\"  - {os.path.basename(cp)}\")\n",
        "\n",
        "# Store checkpoint path for later use\n",
        "RESUME_CHECKPOINT = checkpoint_path"
      ],
      "id": "61b35ced"
    },
    {
      "cell_type": "markdown",
      "id": "0fe6af6d",
      "metadata": {
        "id": "0fe6af6d"
      },
      "source": [
        "## ⚙️ Step 7: Configure Training Parameters\n",
        "\n",
        "Adjust these parameters based on your needs and available GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Configuration - RESUME FROM EPOCH 5 FOR 100 TOTAL EPOCHS\n",
        "import os\n",
        "\n",
        "print(\"🎯 EXTENDED TRAINING CONFIGURATION - WITH W&B\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define directories first to make config cleaner\n",
        "DRIVE_CHECKPOINT_BASE = '/content/drive/MyDrive/ViT-FishID'\n",
        "CHECKPOINT_SAVE_DIR = os.path.join(DRIVE_CHECKPOINT_BASE, 'checkpoints_extended')\n",
        "BACKUP_DIR = os.path.join(DRIVE_CHECKPOINT_BASE, 'checkpoints_backup')\n",
        "\n",
        "TRAINING_CONFIG = {\n",
        "    # RESUME SETTINGS\n",
        "    # Pointing to the latest valid checkpoint found (Epoch 5)\n",
        "    'resume_from_checkpoint': os.path.join(CHECKPOINT_SAVE_DIR, 'checkpoint_epoch_100.pth'),\n",
        "    'start_epoch': 101,  # Next epoch after 5\n",
        "    'total_epochs': 100,  # Target total epochs\n",
        "    'remaining_epochs': 1, # Calculate based on total and start\n",
        "\n",
        "    # CORE SETTINGS\n",
        "    'mode': 'semi_supervised',  # semi_supervised or supervised\n",
        "    'data_dir': DATA_DIR, # This variable comes from Step 5\n",
        "    'batch_size': 16,  # Increased for Colab Pro\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 0.05,\n",
        "\n",
        "    # MODEL SETTINGS\n",
        "    'model_name': 'vit_base_patch16_224',\n",
        "    'num_classes': 37,  # Will be auto-detected below\n",
        "\n",
        "    # SEMI-SUPERVISED SETTINGS\n",
        "    'consistency_weight': 2.0,\n",
        "    'pseudo_label_threshold': 0.7,\n",
        "    'temperature': 4.0,\n",
        "    'warmup_epochs': 5,  # Reduced since we're resuming\n",
        "    'ramp_up_epochs': 15,  # Reduced since we're resuming\n",
        "\n",
        "    # CHECKPOINT SETTINGS - SAVE EVERY EPOCH\n",
        "    'save_frequency': 1,  # Save EVERY epoch\n",
        "    'checkpoint_dir': CHECKPOINT_SAVE_DIR,\n",
        "    'backup_dir': BACKUP_DIR,\n",
        "\n",
        "    # LOGGING - W&B ENABLED\n",
        "    'use_wandb': True, # Enable W&B logging\n",
        "    'wandb_project': 'ViT-FishID-Extended-Training', # Your W&B project name\n",
        "    'wandb_run_name': 'resume-epoch-6-to-100', # A name for this specific run\n",
        "\n",
        "    # Add pretrained flag here as a config item\n",
        "    'pretrained': True,\n",
        "}\n",
        "\n",
        "# Verify data directory and auto-detect num_classes\n",
        "if os.path.exists(TRAINING_CONFIG['data_dir']):\n",
        "    labeled_dir = os.path.join(TRAINING_CONFIG['data_dir'], 'labeled')\n",
        "    if os.path.exists(labeled_dir):\n",
        "        species_count = len([d for d in os.listdir(labeled_dir)\n",
        "                           if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')])\n",
        "        TRAINING_CONFIG['num_classes'] = species_count\n",
        "        print(f\"📊 Detected {species_count} fish species\")\n",
        "    else:\n",
        "         print(f\"⚠️ Labeled data directory not found: {labeled_dir}. Cannot auto-detect num_classes.\")\n",
        "         print(f\"💡 Using default num_classes: {TRAINING_CONFIG['num_classes']}\")\n",
        "else:\n",
        "    print(f\"❌ Data directory not found: {TRAINING_CONFIG['data_dir']}. Cannot auto-detect num_classes.\")\n",
        "    print(f\"💡 Using default num_classes: {TRAINING_CONFIG['num_classes']}\")\n",
        "\n",
        "\n",
        "print(\"\\nEXTENDED TRAINING CONFIGURATION SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"📊 Resume from: Epoch {TRAINING_CONFIG['start_epoch'] - 1}\")\n",
        "print(f\"📊 Target epochs: {TRAINING_CONFIG['total_epochs']}\")\n",
        "print(f\"📊 Remaining epochs: {TRAINING_CONFIG['remaining_epochs']}\")\n",
        "# Estimate time based on remaining epochs and a rough per-epoch time (e.g., 5-7 mins)\n",
        "estimated_min_time = TRAINING_CONFIG['remaining_epochs'] * 5\n",
        "estimated_max_time = TRAINING_CONFIG['remaining_epochs'] * 7\n",
        "print(f\"⏱️ Estimated time: {estimated_min_time:.0f}-{estimated_max_time:.0f} minutes\")\n",
        "print(f\"📊 Batch size: {TRAINING_CONFIG['batch_size']} (optimized for Colab Pro)\")\n",
        "print(f\"💾 Checkpoint saves: EVERY {TRAINING_CONFIG['save_frequency']} epoch(s)\")\n",
        "print(f\"📊 Mode: {TRAINING_CONFIG['mode']} with consistency weight {TRAINING_CONFIG['consistency_weight']}\")\n",
        "print(f\"📊 Logging: W&B Enabled (Project: {TRAINING_CONFIG['wandb_project']}, Run: {TRAINING_CONFIG['wandb_run_name']})\")\n",
        "print(f\"📊 Num Classes: {TRAINING_CONFIG['num_classes']}\")\n",
        "\n",
        "\n",
        "# Create checkpoint directories with more robust error handling\n",
        "print(\"\\nSETTING UP CHECKPOINT DIRECTORIES\")\n",
        "print(\"=\"*50)\n",
        "try:\n",
        "    os.makedirs(TRAINING_CONFIG['checkpoint_dir'], exist_ok=True)\n",
        "    print(f\"📁 Primary saves: {TRAINING_CONFIG['checkpoint_dir']} (Created/Exists)\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not create primary checkpoint dir {TRAINING_CONFIG['checkpoint_dir']}: {e}\")\n",
        "    # Fallback to local directory if Google Drive mount is the issue\n",
        "    local_fallback_dir = '/content/checkpoints_extended_local'\n",
        "    TRAINING_CONFIG['checkpoint_dir'] = local_fallback_dir\n",
        "    try:\n",
        "        os.makedirs(TRAINING_CONFIG['checkpoint_dir'], exist_ok=True)\n",
        "        print(f\"📁 Primary saves (FALLBACK to local): {TRAINING_CONFIG['checkpoint_dir']} (Created/Exists)\")\n",
        "        print(\"💡 Check Google Drive mount if this is unexpected.\")\n",
        "    except Exception as e_local:\n",
        "         print(f\"❌ Could not create fallback local checkpoint dir {local_fallback_dir}: {e_local}\")\n",
        "         print(\"🚨 Check permissions or disk space.\")\n",
        "\n",
        "\n",
        "try:\n",
        "    os.makedirs(TRAINING_CONFIG['backup_dir'], exist_ok=True)\n",
        "    print(f\"💾 Backup saves: {TRAINING_CONFIG['backup_dir']} (Created/Exists)\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not create backup dir {TRAINING_CONFIG['backup_dir']}: {e}\")\n",
        "    print(\"💾 Backup saves: Disabled due to Google Drive issues or permissions.\")\n",
        "    TRAINING_CONFIG['backup_dir'] = None # Explicitly set to None if creation fails\n",
        "\n",
        "\n",
        "if TRAINING_CONFIG['resume_from_checkpoint'] and os.path.exists(TRAINING_CONFIG['resume_from_checkpoint']):\n",
        "    print(f\"\\n✅ Will resume training from: {os.path.basename(TRAINING_CONFIG['resume_from_checkpoint'])}\")\n",
        "else:\n",
        "    print(\"\\n❌ Specified resume checkpoint not found or not set - will start fresh training from epoch 1\")\n",
        "    TRAINING_CONFIG['resume_from_checkpoint'] = None # Ensure it's None if file not found\n",
        "    TRAINING_CONFIG['start_epoch'] = 1\n",
        "    TRAINING_CONFIG['remaining_epochs'] = TRAINING_CONFIG['total_epochs']\n",
        "\n",
        "print(f\"\\n🚀 Configuration complete. Ready to resume/start training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSokV6NDjgYa",
        "outputId": "8c7b1c32-b0db-4c59-fea7-411ce6f2574d"
      },
      "id": "hSokV6NDjgYa",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 EXTENDED TRAINING CONFIGURATION - WITH W&B\n",
            "==================================================\n",
            "📊 Detected 37 fish species\n",
            "\n",
            "EXTENDED TRAINING CONFIGURATION SUMMARY\n",
            "==================================================\n",
            "📊 Resume from: Epoch 100\n",
            "📊 Target epochs: 100\n",
            "📊 Remaining epochs: 1\n",
            "⏱️ Estimated time: 5-7 minutes\n",
            "📊 Batch size: 16 (optimized for Colab Pro)\n",
            "💾 Checkpoint saves: EVERY 1 epoch(s)\n",
            "📊 Mode: semi_supervised with consistency weight 2.0\n",
            "📊 Logging: W&B Enabled (Project: ViT-FishID-Extended-Training, Run: resume-epoch-6-to-100)\n",
            "📊 Num Classes: 37\n",
            "\n",
            "SETTING UP CHECKPOINT DIRECTORIES\n",
            "==================================================\n",
            "📁 Primary saves: /content/drive/MyDrive/ViT-FishID/checkpoints_extended (Created/Exists)\n",
            "💾 Backup saves: /content/drive/MyDrive/ViT-FishID/checkpoints_backup (Created/Exists)\n",
            "\n",
            "✅ Will resume training from: checkpoint_epoch_100.pth\n",
            "\n",
            "🚀 Configuration complete. Ready to resume/start training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa9cbd50",
      "metadata": {
        "id": "aa9cbd50"
      },
      "source": [
        "## 🚀 Step 8: Start Training!\n",
        "\n",
        "This cell will start the semi-supervised training process. It may take 2-3 hours to complete."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute Extended Training - Resume from Epoch 99\n",
        "import os\n",
        "\n",
        "print(\"🚀 STARTING EXTENDED TRAINING SESSION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create checkpoint save directory\n",
        "os.makedirs(TRAINING_CONFIG['checkpoint_dir'], exist_ok=True)\n",
        "\n",
        "# Build training command for resuming\n",
        "training_cmd = f\"\"\"python train.py \\\\\n",
        "    --mode {TRAINING_CONFIG['mode']} \\\\\n",
        "    --data_dir {TRAINING_CONFIG['data_dir']} \\\\\n",
        "    --epochs {TRAINING_CONFIG['total_epochs']} \\\\\n",
        "    --batch_size {TRAINING_CONFIG['batch_size']} \\\\\n",
        "    --learning_rate {TRAINING_CONFIG['learning_rate']} \\\\\n",
        "    --weight_decay {TRAINING_CONFIG['weight_decay']} \\\\\n",
        "    --model_name {TRAINING_CONFIG['model_name']} \\\\\n",
        "    --consistency_weight {TRAINING_CONFIG['consistency_weight']} \\\\\n",
        "    --pseudo_label_threshold {TRAINING_CONFIG['pseudo_label_threshold']} \\\\\n",
        "    --temperature {TRAINING_CONFIG['temperature']} \\\\\n",
        "    --warmup_epochs {TRAINING_CONFIG['warmup_epochs']} \\\\\n",
        "    --ramp_up_epochs {TRAINING_CONFIG['ramp_up_epochs']} \\\\\n",
        "    --save_dir {TRAINING_CONFIG['checkpoint_dir']} \\\\\n",
        "    --save_frequency {TRAINING_CONFIG['save_frequency']}\"\"\"\n",
        "\n",
        "# Add resume checkpoint if available\n",
        "# Pointing to the epoch 99 checkpoint\n",
        "TRAINING_CONFIG['resume_from_checkpoint'] = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'checkpoint_epoch_99.pth')\n",
        "TRAINING_CONFIG['start_epoch'] = 100 # Start from the epoch *after* the resumed checkpoint (i.e., epoch 100 is the next epoch)\n",
        "\n",
        "if TRAINING_CONFIG['resume_from_checkpoint']:\n",
        "    training_cmd += f\" \\\\\\n    --resume_from {TRAINING_CONFIG['resume_from_checkpoint']}\"\n",
        "    print(f\"📂 Resuming from: {os.path.basename(TRAINING_CONFIG['resume_from_checkpoint'])}\")\n",
        "    print(f\"🚀 Starting training from epoch: {TRAINING_CONFIG['start_epoch']}\")\n",
        "\n",
        "# Add W&B logging - Only add the --use_wandb flag\n",
        "if TRAINING_CONFIG['use_wandb']:\n",
        "    training_cmd += f\" \\\\\\n    --use_wandb\"\n",
        "    # Removed --wandb_project and --wandb_run_name as train.py doesn't recognize them\n",
        "\n",
        "# Add pretrained flag\n",
        "if TRAINING_CONFIG['pretrained']:\n",
        "    training_cmd += \" \\\\\\n    --pretrained\"\n",
        "\n",
        "# Update remaining epochs based on new start_epoch\n",
        "TRAINING_CONFIG['remaining_epochs'] = TRAINING_CONFIG['total_epochs'] - (TRAINING_CONFIG['start_epoch'] - 1) # Calculate based on total and start\n",
        "\n",
        "print(f\"📊 Training for {TRAINING_CONFIG['remaining_epochs']} more epochs...\")\n",
        "print(f\"🎯 Target: {TRAINING_CONFIG['total_epochs']} total epochs\")\n",
        "print(f\"⏱️ Estimated time: {TRAINING_CONFIG['remaining_epochs'] * 4:.0f}-{TRAINING_CONFIG['remaining_epochs'] * 6:.0f} minutes\")\n",
        "print(f\"💾 Checkpoints saved to: {TRAINING_CONFIG['checkpoint_dir']}\")\n",
        "\n",
        "print(\"\\n📋 Extended Training Command:\")\n",
        "print(training_cmd.replace('\\\\', '').strip())\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Execute training\n",
        "print(f\"🎬 TRAINING STARTED - EPOCH {TRAINING_CONFIG['start_epoch']} TO {TRAINING_CONFIG['total_epochs']}\")\n",
        "print(\"⏰ Started at:\", __import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "# Before executing, modify trainer.py to fix the AttributeError during checkpoint saving\n",
        "# This is a temporary fix directly modifying the cloned file\n",
        "trainer_file_path = '/content/ViT-FishID/trainer.py'\n",
        "try:\n",
        "    with open(trainer_file_path, 'r') as f:\n",
        "        trainer_code = f.read()\n",
        "\n",
        "    # Find the line that saves the ema_teacher_state_dict and comment it out\n",
        "    # Look for patterns like 'ema_teacher_state_dict': ...\n",
        "    lines = trainer_code.splitlines()\n",
        "    modified_lines = []\n",
        "    ema_line_found = False\n",
        "    for line in lines:\n",
        "        # Check for the line saving ema_teacher_state_dict, allowing for variations in spacing/access\n",
        "        if \"'ema_teacher_state_dict':\" in line and \"state_dict()\" in line:\n",
        "             modified_lines.append(\"# \" + line) # Comment out the line\n",
        "             ema_line_found = True\n",
        "             print(f\"✅ Commented out line saving ema_teacher state_dict: {line.strip()}\")\n",
        "        else:\n",
        "            modified_lines.append(line)\n",
        "\n",
        "    if ema_line_found:\n",
        "        corrected_code = \"\\n\".join(modified_lines)\n",
        "        with open(trainer_file_path, 'w') as f:\n",
        "            f.write(corrected_code)\n",
        "        print(f\"✅ Modified {trainer_file_path} to skip saving EMA teacher state_dict.\")\n",
        "    else:\n",
        "        print(f\"⚠️ Could not find the line saving ema_teacher_state_dict in {trainer_file_path}. The fix might not be applied.\")\n",
        "        print(\"💡 Training might still fail due to the EMA teacher state_dict error.\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error modifying {trainer_file_path}: {e}\")\n",
        "    print(\"🚨 Training might still fail due to the EMA teacher state_dict error.\")\n",
        "\n",
        "\n",
        "!{training_cmd}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎉 EXTENDED TRAINING COMPLETED!\")\n",
        "print(\"⏰ Finished at:\", __import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "print(f\"🏆 Total epochs completed: {TRAINING_CONFIG['total_epochs']}\")\n",
        "print(f\"💾 All checkpoints saved to Google Drive\")\n",
        "\n",
        "# Quick summary of final results\n",
        "final_checkpoint = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'model_best.pth')\n",
        "if os.path.exists(final_checkpoint):\n",
        "    try:\n",
        "        import torch\n",
        "        final_results = torch.load(final_checkpoint, map_location='cpu')\n",
        "        if 'best_accuracy' in final_results:\n",
        "            print(f\"🎯 Final best accuracy: {final_results['best_accuracy']:.2f}%\")\n",
        "        if 'epoch' in final_results:\n",
        "            print(f\"📊 Best model from epoch: {final_results['epoch']}\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"\\n✅ Your model is ready for evaluation and deployment!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njLKb7xaepxo",
        "outputId": "2404e35d-a058-4349-bafa-18e98981bb07"
      },
      "id": "njLKb7xaepxo",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 STARTING EXTENDED TRAINING SESSION\n",
            "============================================================\n",
            "📂 Resuming from: checkpoint_epoch_99.pth\n",
            "🚀 Starting training from epoch: 100\n",
            "📊 Training for 1 more epochs...\n",
            "🎯 Target: 100 total epochs\n",
            "⏱️ Estimated time: 4-6 minutes\n",
            "💾 Checkpoints saved to: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "\n",
            "📋 Extended Training Command:\n",
            "python train.py \n",
            "    --mode semi_supervised \n",
            "    --data_dir /content/fish_cutouts \n",
            "    --epochs 100 \n",
            "    --batch_size 16 \n",
            "    --learning_rate 0.0001 \n",
            "    --weight_decay 0.05 \n",
            "    --model_name vit_base_patch16_224 \n",
            "    --consistency_weight 2.0 \n",
            "    --pseudo_label_threshold 0.7 \n",
            "    --temperature 4.0 \n",
            "    --warmup_epochs 5 \n",
            "    --ramp_up_epochs 15 \n",
            "    --save_dir /content/drive/MyDrive/ViT-FishID/checkpoints_extended \n",
            "    --save_frequency 1 \n",
            "    --resume_from /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_99.pth \n",
            "    --use_wandb \n",
            "    --pretrained\n",
            "\n",
            "============================================================\n",
            "🎬 TRAINING STARTED - EPOCH 100 TO 100\n",
            "⏰ Started at: 2025-08-15 07:03:08\n",
            "✅ Commented out line saving ema_teacher state_dict: 'ema_teacher_state_dict': trainer.ema_teacher.teacher.state_dict(),  # Fixed key name\n",
            "✅ Modified /content/ViT-FishID/trainer.py to skip saving EMA teacher state_dict.\n",
            "2025-08-15 07:03:16.032238: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-15 07:03:16.049070: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755241396.070136    2955 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755241396.076517    2955 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755241396.092669    2955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755241396.092697    2955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755241396.092700    2955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755241396.092703    2955 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-15 07:03:16.097419: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Random seed set to 42\n",
            "Using GPU: NVIDIA A100-SXM4-40GB\n",
            "🐟 ViT-FishID Training\n",
            "📊 Mode: semi_supervised\n",
            "🖥️  Device: cuda\n",
            "📁 Data directory: /content/fish_cutouts\n",
            "\n",
            "📦 Creating data loaders...\n",
            "⚠️  Warning: Some classes have only 1 sample(s). Using random splitting instead of stratified.\n",
            "   Classes with 1 sample: ['Carangidae_Caranx_heberi', 'Serranidae_Lipropoma_spp1', 'Sparidae_Sparodon_durbanesis']\n",
            "/content/ViT-FishID/data.py:229: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
            "📊 Dataset initialized:\n",
            "  - Labeled samples: 3,084\n",
            "  - Unlabeled samples: 6,168\n",
            "  - Total samples per epoch: 9,252\n",
            "📊 Semi-supervised data loaders created:\n",
            "  - Train labeled: 3,084\n",
            "  - Train unlabeled: 6,168\n",
            "  - Val samples: 1,029\n",
            "  - Test samples: 1,029\n",
            "  - Classes: 37\n",
            "  - Split ratios: Train=60.0%, Val=20.0%, Test=20.0%\n",
            "🏷️  Classes (37): ['Carangidae_Caranx_heberi', 'Carangidae_Pseudocaranx_dentex', 'Carangidae_Seriola_dumerili', 'Carangidae_Seriola_lalandi', 'Carangidae_Seriola_rivoliana', 'Carangidae_Trachurus_delagoa', 'Serranidae_Aulacocephalus_temminckii', 'Serranidae_Epinephelus_andersoni', 'Serranidae_Epinephelus_marginatus', 'Serranidae_Epinephelus_rivulatus', 'Serranidae_Epinephelus_tukula', 'Serranidae_Lipropoma_spp1', 'Serranidae_Serranus_knysnaensis', 'Sparidae_Argyrops_spinifer', 'Sparidae_Boopsoidea_inornata', 'Sparidae_Cheimerius_nufar', 'Sparidae_Chrysoblephus_anglicus', 'Sparidae_Chrysoblephus_cristiceps', 'Sparidae_Chrysoblephus_lophus', 'Sparidae_Chrysoblephus_puniceus', 'Sparidae_Cymatoceps_nasutus', 'Sparidae_Diplodus_capensis', 'Sparidae_Diplodus_hottentotus', 'Sparidae_Pachymetopon_aeneum', 'Sparidae_Pachymetopon_grande', 'Sparidae_Pagellus_bellottii_natalensis', 'Sparidae_Petrus_rupestris', 'Sparidae_Polyamblydon_germanum', 'Sparidae_Polysteganus_praeorbitalis', 'Sparidae_Polysteganus_undulosus', 'Sparidae_Porcostoma_dentata', 'Sparidae_Rhabdosargus_holubi', 'Sparidae_Rhabdosargus_sarba', 'Sparidae_Rhabdosargus_thorpei', 'Sparidae_Sarpa_salpa', 'Sparidae_Sparodon_durbanesis', 'Sparidae_Spondyliosoma_emarginatum']\n",
            "📊 Test set available with 1,029 samples for final evaluation\n",
            "\n",
            "🧠 Creating ViT model: vit_base_patch16_224\n",
            "model.safetensors: 100% 346M/346M [00:00<00:00, 479MB/s]\n",
            "✅ EMA Teacher initialized with momentum: 0.999\n",
            "📊 Model parameters: 85,828,645\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcativthomson\u001b[0m (\u001b[33mcativthomson-university-of-cape-town\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/ViT-FishID/wandb/run-20250815_070324-ogt296e8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msemi_supervised_vit_base_patch16_224_20250815_070323\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cativthomson-university-of-cape-town/vit-fish-id\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cativthomson-university-of-cape-town/vit-fish-id/runs/ogt296e8\u001b[0m\n",
            "✅ W&B initialized: vit-fish-id/semi_supervised_vit_base_patch16_224_20250815_070323\n",
            "\n",
            "🚀 Creating trainer...\n",
            "✅ Semi-Supervised Trainer initialized\n",
            "  - Consistency weight: 2.0\n",
            "  - Pseudo-label threshold: 0.7\n",
            "  - Learning rate: 0.0001\n",
            "  - Warmup epochs: 5\n",
            "  - Ramp-up epochs: 15\n",
            "📥 Resuming from checkpoint: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_99.pth\n",
            "✅ Successfully loaded checkpoint from epoch 99\n",
            "📊 Previous best accuracy: 87.56073858114675\n",
            "🚀 Resuming training from epoch 100\n",
            "\n",
            "🎯 Starting semi_supervised training...\n",
            "💡 Note: Test set is reserved for final evaluation and not used during training\n",
            "🔄 Resuming training from epoch 100\n",
            "⏰ Remaining epochs: 1\n",
            "📁 Checkpoints will be saved to: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "Epoch 100: 100% 578/578 [01:37<00:00,  5.93it/s, Total=0.1759, Sup=0.1095, Cons=0.0332, L-Acc=97.0%, P-Acc=88.2%]\n",
            "                                               \n",
            "📊 Epoch 101/100\n",
            "Train - Total Loss: 0.1759\n",
            "Train - Labeled Acc: 97.0%, Pseudo Acc: 88.2%\n",
            "Train - High-conf Pseudo: 1212/6165 (19.7%)\n",
            "Student Val - Acc: 77.9%\n",
            "Teacher Val - Acc: 75.7%\n",
            "Checkpoint saved to /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "💾 Backup saved to: /content/drive/MyDrive/ViT-FishID/checkpoints_backup/checkpoint_epoch_100.pth\n",
            "📊 Epoch 100 checkpoint saved (Size: 982.4 MB)\n",
            "\n",
            "🎉 Training completed! Best validation accuracy: 87.56%\n",
            "\n",
            "✅ Training completed!\n",
            "💡 Use evaluate.py with the test set for final unbiased performance metrics\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading history steps 12-12, summary, console lines 19-35 (0.1s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       learning_rate ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/consistency_loss █▃▇▄▅▃▂▂▂▂▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/consistency_weight ▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/high_conf_ratio ▁▁▁▁▁▂▃▄▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/labeled_accuracy █▂▁▂▃▄▄▃▃▃▃▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/pseudo_accuracy ▁▁▁█████▇▇▇▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/supervised_loss ▁▁▁▁▁▁▁█▃▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/total_loss ▃▁▂▂▂▁▁█▃▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_epoch/consistency_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_epoch/high_conf_pseudo_labels ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_epoch/labeled_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/labeled_samples ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/pseudo_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/supervised_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train_epoch/total_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_epoch/unlabeled_samples ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/student_top1_acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/student_top5_acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/teacher_top1_acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/teacher_top5_acc ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                               epoch 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/consistency_loss 0.02646\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/consistency_weight 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/high_conf_ratio 18.24359\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/labeled_accuracy 97.0297\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/pseudo_accuracy 88.6406\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/supervised_loss 0.03465\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/total_loss 0.08756\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_epoch/consistency_loss 0.03318\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_epoch/high_conf_pseudo_labels 1212\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train_epoch/labeled_accuracy 97.04833\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/labeled_samples 3083\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/pseudo_accuracy 88.20132\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train_epoch/supervised_loss 0.10952\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train_epoch/total_loss 0.17589\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train_epoch/unlabeled_samples 6165\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/student_top1_acc 77.93975\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/student_top5_acc 92.80855\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/teacher_top1_acc 75.70457\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                val/teacher_top5_acc 94.94655\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msemi_supervised_vit_base_patch16_224_20250815_070323\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cativthomson-university-of-cape-town/vit-fish-id/runs/ogt296e8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cativthomson-university-of-cape-town/vit-fish-id\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250815_070324-ogt296e8/logs\u001b[0m\n",
            "\n",
            "🎉 Training completed successfully!\n",
            "💾 Checkpoints saved to: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "🏆 Best accuracy: 87.56%\n",
            "\n",
            "============================================================\n",
            "🎉 EXTENDED TRAINING COMPLETED!\n",
            "⏰ Finished at: 2025-08-15 07:05:32\n",
            "🏆 Total epochs completed: 100\n",
            "💾 All checkpoints saved to Google Drive\n",
            "\n",
            "✅ Your model is ready for evaluation and deployment!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5af5177",
      "metadata": {
        "id": "b5af5177"
      },
      "source": [
        "## 📊 Step 9: Check Training Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "87ea96e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87ea96e8",
        "outputId": "2054291d-a4ae-48d9-d5ed-2529032142bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 Checking results in: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "\n",
            "✅ Found 100 checkpoint(s) from extended training:\n",
            "  📊 Epoch 1: checkpoint_epoch_1.pth (982.4 MB)\n",
            "  📊 Epoch 2: checkpoint_epoch_2.pth (982.4 MB)\n",
            "  📊 Epoch 3: checkpoint_epoch_3.pth (982.4 MB)\n",
            "  📊 Epoch 4: checkpoint_epoch_4.pth (982.4 MB)\n",
            "  📊 Epoch 5: checkpoint_epoch_5.pth (982.4 MB)\n",
            "  📊 Epoch 6: checkpoint_epoch_6.pth (982.4 MB)\n",
            "  📊 Epoch 7: checkpoint_epoch_7.pth (982.4 MB)\n",
            "  📊 Epoch 8: checkpoint_epoch_8.pth (982.4 MB)\n",
            "  📊 Epoch 9: checkpoint_epoch_9.pth (982.4 MB)\n",
            "  📊 Epoch 10: checkpoint_epoch_10.pth (982.4 MB)\n",
            "  📊 Epoch 11: checkpoint_epoch_11.pth (982.4 MB)\n",
            "  📊 Epoch 12: checkpoint_epoch_12.pth (982.4 MB)\n",
            "  📊 Epoch 13: checkpoint_epoch_13.pth (982.4 MB)\n",
            "  📊 Epoch 14: checkpoint_epoch_14.pth (982.4 MB)\n",
            "  📊 Epoch 15: checkpoint_epoch_15.pth (982.4 MB)\n",
            "  📊 Epoch 16: checkpoint_epoch_16.pth (982.4 MB)\n",
            "  📊 Epoch 17: checkpoint_epoch_17.pth (982.4 MB)\n",
            "  📊 Epoch 18: checkpoint_epoch_18.pth (982.4 MB)\n",
            "  📊 Epoch 19: checkpoint_epoch_19.pth (982.4 MB)\n",
            "  📊 Epoch 20: checkpoint_epoch_20.pth (982.4 MB)\n",
            "  📊 Epoch 21: checkpoint_epoch_21.pth (982.4 MB)\n",
            "  📊 Epoch 22: checkpoint_epoch_22.pth (982.4 MB)\n",
            "  📊 Epoch 23: checkpoint_epoch_23.pth (982.4 MB)\n",
            "  📊 Epoch 24: checkpoint_epoch_24.pth (982.4 MB)\n",
            "  📊 Epoch 25: checkpoint_epoch_25.pth (982.4 MB)\n",
            "  📊 Epoch 26: checkpoint_epoch_26.pth (982.4 MB)\n",
            "  📊 Epoch 27: checkpoint_epoch_27.pth (982.4 MB)\n",
            "  📊 Epoch 28: checkpoint_epoch_28.pth (982.4 MB)\n",
            "  📊 Epoch 29: checkpoint_epoch_29.pth (982.4 MB)\n",
            "  📊 Epoch 30: checkpoint_epoch_30.pth (982.4 MB)\n",
            "  📊 Epoch 31: checkpoint_epoch_31.pth (982.4 MB)\n",
            "  📊 Epoch 32: checkpoint_epoch_32.pth (982.4 MB)\n",
            "  📊 Epoch 33: checkpoint_epoch_33.pth (982.4 MB)\n",
            "  📊 Epoch 34: checkpoint_epoch_34.pth (982.4 MB)\n",
            "  📊 Epoch 35: checkpoint_epoch_35.pth (982.4 MB)\n",
            "  📊 Epoch 36: checkpoint_epoch_36.pth (982.4 MB)\n",
            "  📊 Epoch 37: checkpoint_epoch_37.pth (982.4 MB)\n",
            "  📊 Epoch 38: checkpoint_epoch_38.pth (982.4 MB)\n",
            "  📊 Epoch 39: checkpoint_epoch_39.pth (982.4 MB)\n",
            "  📊 Epoch 40: checkpoint_epoch_40.pth (982.4 MB)\n",
            "  📊 Epoch 41: checkpoint_epoch_41.pth (982.4 MB)\n",
            "  📊 Epoch 42: checkpoint_epoch_42.pth (982.4 MB)\n",
            "  📊 Epoch 43: checkpoint_epoch_43.pth (982.4 MB)\n",
            "  📊 Epoch 44: checkpoint_epoch_44.pth (982.4 MB)\n",
            "  📊 Epoch 45: checkpoint_epoch_45.pth (982.4 MB)\n",
            "  📊 Epoch 46: checkpoint_epoch_46.pth (982.4 MB)\n",
            "  📊 Epoch 47: checkpoint_epoch_47.pth (982.4 MB)\n",
            "  📊 Epoch 48: checkpoint_epoch_48.pth (982.4 MB)\n",
            "  📊 Epoch 49: checkpoint_epoch_49.pth (982.4 MB)\n",
            "  📊 Epoch 50: checkpoint_epoch_50.pth (982.4 MB)\n",
            "  📊 Epoch 51: checkpoint_epoch_51.pth (982.4 MB)\n",
            "  📊 Epoch 52: checkpoint_epoch_52.pth (982.4 MB)\n",
            "  📊 Epoch 53: checkpoint_epoch_53.pth (982.4 MB)\n",
            "  📊 Epoch 54: checkpoint_epoch_54.pth (982.4 MB)\n",
            "  📊 Epoch 55: checkpoint_epoch_55.pth (982.4 MB)\n",
            "  📊 Epoch 56: checkpoint_epoch_56.pth (982.4 MB)\n",
            "  📊 Epoch 57: checkpoint_epoch_57.pth (982.4 MB)\n",
            "  📊 Epoch 58: checkpoint_epoch_58.pth (982.4 MB)\n",
            "  📊 Epoch 59: checkpoint_epoch_59.pth (982.4 MB)\n",
            "  📊 Epoch 60: checkpoint_epoch_60.pth (982.4 MB)\n",
            "  📊 Epoch 61: checkpoint_epoch_61.pth (982.4 MB)\n",
            "  📊 Epoch 62: checkpoint_epoch_62.pth (982.4 MB)\n",
            "  📊 Epoch 63: checkpoint_epoch_63.pth (982.4 MB)\n",
            "  📊 Epoch 64: checkpoint_epoch_64.pth (982.4 MB)\n",
            "  📊 Epoch 65: checkpoint_epoch_65.pth (982.4 MB)\n",
            "  📊 Epoch 66: checkpoint_epoch_66.pth (982.4 MB)\n",
            "  📊 Epoch 67: checkpoint_epoch_67.pth (982.4 MB)\n",
            "  📊 Epoch 68: checkpoint_epoch_68.pth (982.4 MB)\n",
            "  📊 Epoch 69: checkpoint_epoch_69.pth (982.4 MB)\n",
            "  📊 Epoch 70: checkpoint_epoch_70.pth (982.4 MB)\n",
            "  📊 Epoch 71: checkpoint_epoch_71.pth (982.4 MB)\n",
            "  📊 Epoch 72: checkpoint_epoch_72.pth (982.4 MB)\n",
            "  📊 Epoch 73: checkpoint_epoch_73.pth (982.4 MB)\n",
            "  📊 Epoch 74: checkpoint_epoch_74.pth (982.4 MB)\n",
            "  📊 Epoch 75: checkpoint_epoch_75.pth (982.4 MB)\n",
            "  📊 Epoch 76: checkpoint_epoch_76.pth (982.4 MB)\n",
            "  📊 Epoch 77: checkpoint_epoch_77.pth (982.4 MB)\n",
            "  📊 Epoch 78: checkpoint_epoch_78.pth (982.4 MB)\n",
            "  📊 Epoch 79: checkpoint_epoch_79.pth (982.4 MB)\n",
            "  📊 Epoch 80: checkpoint_epoch_80.pth (982.4 MB)\n",
            "  📊 Epoch 81: checkpoint_epoch_81.pth (982.4 MB)\n",
            "  📊 Epoch 82: checkpoint_epoch_82.pth (982.4 MB)\n",
            "  📊 Epoch 83: checkpoint_epoch_83.pth (982.4 MB)\n",
            "  📊 Epoch 84: checkpoint_epoch_84.pth (982.4 MB)\n",
            "  📊 Epoch 85: checkpoint_epoch_85.pth (982.4 MB)\n",
            "  📊 Epoch 86: checkpoint_epoch_86.pth (982.4 MB)\n",
            "  📊 Epoch 87: checkpoint_epoch_87.pth (982.4 MB)\n",
            "  📊 Epoch 88: checkpoint_epoch_88.pth (982.4 MB)\n",
            "  📊 Epoch 89: checkpoint_epoch_89.pth (982.4 MB)\n",
            "  📊 Epoch 90: checkpoint_epoch_90.pth (982.4 MB)\n",
            "  📊 Epoch 91: checkpoint_epoch_91.pth (982.4 MB)\n",
            "  📊 Epoch 92: checkpoint_epoch_92.pth (982.4 MB)\n",
            "  📊 Epoch 93: checkpoint_epoch_93.pth (982.4 MB)\n",
            "  📊 Epoch 94: checkpoint_epoch_94.pth (982.4 MB)\n",
            "  📊 Epoch 95: checkpoint_epoch_95.pth (982.4 MB)\n",
            "  📊 Epoch 96: checkpoint_epoch_96.pth (982.4 MB)\n",
            "  📊 Epoch 97: checkpoint_epoch_97.pth (982.4 MB)\n",
            "  📊 Epoch 98: checkpoint_epoch_98.pth (982.4 MB)\n",
            "  📊 Epoch 99: checkpoint_epoch_99.pth (982.4 MB)\n",
            "  📊 Epoch 100: checkpoint_epoch_100.pth (982.4 MB)\n",
            "\n",
            "⏱️ EXTENDED TRAINING SUMMARY:\n",
            "  📊 Additional epochs completed: 81\n",
            "  🎯 Target was: 81 additional epochs (to reach 100 total)\n",
            "  ✅ TRAINING GOAL ACHIEVED! Completed all 81 additional epochs\n",
            "\n",
            "📈 View detailed training metrics:\n",
            "   https://wandb.ai/your-username/ViT-FishID-Extended-Training\n",
            "   Run: resume-epoch-6-to-100\n",
            "\n",
            "🎉 Extended training session complete!\n",
            "🚀 Your model trained from epoch 19 to 100!\n",
            "💾 All results saved to Google Drive: /content/drive/MyDrive/ViT-FishID/checkpoints_extended\n",
            "\n",
            "📊 PERFORMANCE COMPARISON:\n",
            "  🔄 Previous (Epoch 19): ~78% accuracy\n",
            "  🎯 Extended (Epoch 100): Check best_accuracy above\n",
            "  📈 Expected improvement: 5-10% accuracy gain\n",
            "  🏆 Your model should now be ready for deployment!\n"
          ]
        }
      ],
      "source": [
        "# Check Extended Training Results (Epoch 19 → 100)\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "\n",
        "checkpoint_dir = TRAINING_CONFIG['checkpoint_dir']\n",
        "print(f\"📁 Checking results in: {checkpoint_dir}\")\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    checkpoints = glob.glob(os.path.join(checkpoint_dir, '*.pth'))\n",
        "    if checkpoints:\n",
        "        print(f\"\\n✅ Found {len(checkpoints)} checkpoint(s) from extended training:\")\n",
        "\n",
        "        # Sort checkpoints by epoch number\n",
        "        epoch_checkpoints = []\n",
        "        other_checkpoints = []\n",
        "\n",
        "        for cp in checkpoints:\n",
        "            basename = os.path.basename(cp)\n",
        "            if 'epoch_' in basename:\n",
        "                try:\n",
        "                    epoch_num = int(basename.split('epoch_')[1].split('.')[0])\n",
        "                    epoch_checkpoints.append((epoch_num, cp))\n",
        "                except:\n",
        "                    other_checkpoints.append(cp)\n",
        "            else:\n",
        "                other_checkpoints.append(cp)\n",
        "\n",
        "        # Show epoch checkpoints in order\n",
        "        epoch_checkpoints.sort(key=lambda x: x[0])\n",
        "        for epoch, cp in epoch_checkpoints:\n",
        "            file_size = os.path.getsize(cp) / (1024**2)\n",
        "            print(f\"  📊 Epoch {epoch}: {os.path.basename(cp)} ({file_size:.1f} MB)\")\n",
        "\n",
        "        # Show other checkpoints\n",
        "        for cp in other_checkpoints:\n",
        "            file_size = os.path.getsize(cp) / (1024**2)\n",
        "            print(f\"  🏆 {os.path.basename(cp)} ({file_size:.1f} MB)\")\n",
        "\n",
        "        # Analyze best model\n",
        "        best_model = os.path.join(checkpoint_dir, 'model_best.pth')\n",
        "        if os.path.exists(best_model):\n",
        "            print(f\"\\n🏆 BEST MODEL ANALYSIS:\")\n",
        "            try:\n",
        "                best_checkpoint = torch.load(best_model, map_location='cpu')\n",
        "\n",
        "                best_epoch = best_checkpoint.get('epoch', 'Unknown')\n",
        "                best_acc = best_checkpoint.get('best_accuracy', best_checkpoint.get('best_acc', 'Unknown'))\n",
        "\n",
        "                print(f\"  📊 Best epoch: {best_epoch}\")\n",
        "                print(f\"  📊 Best accuracy: {best_acc:.2f}%\" if isinstance(best_acc, (int, float)) else f\"  📊 Best accuracy: {best_acc}\")\n",
        "\n",
        "                # Show training progression\n",
        "                if epoch_checkpoints:\n",
        "                    print(f\"\\n📈 TRAINING PROGRESSION:\")\n",
        "                    print(f\"  🏁 Started: Epoch 19 (resumed)\")\n",
        "                    print(f\"  🎯 Completed: Epoch {max(epoch_checkpoints, key=lambda x: x[0])[0]}\")\n",
        "                    print(f\"  🏆 Best: Epoch {best_epoch}\")\n",
        "                    print(f\"  📊 Total training: {19 + len([e for e, _ in epoch_checkpoints if e > 19])} epochs\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠️ Could not analyze best model: {e}\")\n",
        "\n",
        "        # Training duration estimate\n",
        "        if epoch_checkpoints:\n",
        "            epochs_completed = len([e for e, _ in epoch_checkpoints if e > 19])\n",
        "            print(f\"\\n⏱️ EXTENDED TRAINING SUMMARY:\")\n",
        "            print(f\"  📊 Additional epochs completed: {epochs_completed}\")\n",
        "            print(f\"  🎯 Target was: 81 additional epochs (to reach 100 total)\")\n",
        "\n",
        "            if epochs_completed >= 81:\n",
        "                print(f\"  ✅ TRAINING GOAL ACHIEVED! Completed all {epochs_completed} additional epochs\")\n",
        "            else:\n",
        "                print(f\"  ⏳ Training partially complete: {epochs_completed}/81 additional epochs\")\n",
        "\n",
        "    else:\n",
        "        print(\"❌ No checkpoints found in extended training directory\")\n",
        "\n",
        "        # Check if training is still using old directory\n",
        "        old_checkpoint_dir = '/content/ViT-FishID/checkpoints'\n",
        "        if os.path.exists(old_checkpoint_dir):\n",
        "            old_checkpoints = glob.glob(os.path.join(old_checkpoint_dir, '*.pth'))\n",
        "            if old_checkpoints:\n",
        "                print(f\"\\n💡 Found {len(old_checkpoints)} checkpoints in old directory:\")\n",
        "                print(f\"   {old_checkpoint_dir}\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ Extended training checkpoint directory not found\")\n",
        "\n",
        "# W&B link\n",
        "if TRAINING_CONFIG['use_wandb']:\n",
        "    print(f\"\\n📈 View detailed training metrics:\")\n",
        "    print(f\"   https://wandb.ai/your-username/{TRAINING_CONFIG['wandb_project']}\")\n",
        "    print(f\"   Run: {TRAINING_CONFIG['wandb_run_name']}\")\n",
        "\n",
        "print(f\"\\n🎉 Extended training session complete!\")\n",
        "print(f\"🚀 Your model trained from epoch 19 to 100!\")\n",
        "print(f\"💾 All results saved to Google Drive: {checkpoint_dir}\")\n",
        "\n",
        "# Performance comparison\n",
        "print(f\"\\n📊 PERFORMANCE COMPARISON:\")\n",
        "print(f\"  🔄 Previous (Epoch 19): ~78% accuracy\")\n",
        "print(f\"  🎯 Extended (Epoch 100): Check best_accuracy above\")\n",
        "print(f\"  📈 Expected improvement: 5-10% accuracy gain\")\n",
        "print(f\"  🏆 Your model should now be ready for deployment!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff698a72",
      "metadata": {
        "id": "ff698a72"
      },
      "source": [
        "## 💾 Step 10: Download Model and Results\n",
        "\n",
        "Save your trained model to Google Drive for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "89513455",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89513455",
        "outputId": "56d72acb-44d3-4f54-d3e6-73601057458a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saving results to Google Drive: /content/drive/MyDrive/ViT-FishID_Training_20250815_070649\n",
            "✅ Checkpoints saved to: /content/drive/MyDrive/ViT-FishID_Training_20250815_070649/checkpoints\n",
            "✅ Training config saved to: /content/drive/MyDrive/ViT-FishID_Training_20250815_070649/training_config.json\n",
            "✅ Training summary saved to: /content/drive/MyDrive/ViT-FishID_Training_20250815_070649/training_summary.txt\n",
            "\n",
            "🎉 All results saved to Google Drive!\n",
            "📁 Location: /content/drive/MyDrive/ViT-FishID_Training_20250815_070649\n",
            "\n",
            "💡 You can now:\n",
            "   1. Download the checkpoints folder for local use\n",
            "   2. Use model_best.pth for inference\n",
            "   3. Continue training from any checkpoint\n"
          ]
        }
      ],
      "source": [
        "# Copy trained model to Google Drive\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "import os\n",
        "import json # Import json for saving config\n",
        "\n",
        "# Create a timestamped folder in Google Drive\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_dir = f'/content/drive/MyDrive/ViT-FishID_Training_{timestamp}'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "print(f\"💾 Saving results to Google Drive: {save_dir}\")\n",
        "\n",
        "# Copy checkpoints from the directory specified in TRAINING_CONFIG\n",
        "checkpoint_dir_source = TRAINING_CONFIG.get('checkpoint_dir', '/content/ViT-FishID/checkpoints') # Use get with a default for safety\n",
        "if os.path.exists(checkpoint_dir_source):\n",
        "    drive_checkpoint_dir = os.path.join(save_dir, 'checkpoints')\n",
        "    try:\n",
        "        shutil.copytree(checkpoint_dir_source, drive_checkpoint_dir)\n",
        "        print(f\"✅ Checkpoints saved to: {drive_checkpoint_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error copying checkpoints from {checkpoint_dir_source} to {drive_checkpoint_dir}: {e}\")\n",
        "else:\n",
        "    print(f\"⚠️ Source checkpoint directory not found: {checkpoint_dir_source}. Skipping checkpoint copy.\")\n",
        "\n",
        "\n",
        "# Save training configuration\n",
        "config_file = os.path.join(save_dir, 'training_config.json')\n",
        "try:\n",
        "    # Ensure config is JSON serializable (remove non-string keys or complex objects if any)\n",
        "    serializable_config = {k: v for k, v in TRAINING_CONFIG.items() if isinstance(k, (str, int, float, bool)) and isinstance(v, (str, int, float, bool, list, dict, type(None)))}\n",
        "    with open(config_file, 'w') as f:\n",
        "        json.dump(serializable_config, f, indent=2)\n",
        "    print(f\"✅ Training config saved to: {config_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error saving training config to {config_file}: {e}\")\n",
        "\n",
        "\n",
        "# Create a summary file\n",
        "summary_file = os.path.join(save_dir, 'training_summary.txt')\n",
        "try:\n",
        "    with open(summary_file, 'w') as f:\n",
        "        f.write(f\"ViT-FishID Training Summary\\n\")\n",
        "        f.write(f\"========================\\n\\n\")\n",
        "        f.write(f\"Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Mode: {TRAINING_CONFIG.get('mode', 'N/A')}\\n\") # Use .get() for safety\n",
        "        # FIX: Use the correct key 'total_epochs'\n",
        "        f.write(f\"Epochs: {TRAINING_CONFIG.get('total_epochs', 'N/A')}\\n\") # Use .get() with correct key\n",
        "        f.write(f\"Batch Size: {TRAINING_CONFIG.get('batch_size', 'N/A')}\\n\") # Use .get()\n",
        "        f.write(f\"Data Directory: {TRAINING_CONFIG.get('data_dir', 'N/A')}\\n\") # Use .get()\n",
        "        f.write(f\"\\nModel Architecture: {TRAINING_CONFIG.get('model_name', 'N/A')}\\n\") # Use .get()\n",
        "        f.write(f\"Learning Rate: {TRAINING_CONFIG.get('learning_rate', 'N/A')}\\n\") # Use .get()\n",
        "        f.write(f\"Consistency Weight: {TRAINING_CONFIG.get('consistency_weight', 'N/A')}\\n\") # Use .get()\n",
        "        f.write(f\"\\nCheckpoints saved in: checkpoints/\\n\")\n",
        "        f.write(f\"Best model: checkpoints/model_best.pth\\n\") # This assumes model_best.pth was saved\n",
        "\n",
        "    print(f\"✅ Training summary saved to: {summary_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error saving training summary to {summary_file}: {e}\")\n",
        "\n",
        "\n",
        "print(f\"\\n🎉 All results saved to Google Drive!\")\n",
        "print(f\"📁 Location: {save_dir}\")\n",
        "print(f\"\\n💡 You can now:\")\n",
        "print(f\"   1. Download the checkpoints folder for local use\")\n",
        "print(f\"   2. Use model_best.pth for inference\")\n",
        "print(f\"   3. Continue training from any checkpoint\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bbc6396",
      "metadata": {
        "id": "3bbc6396"
      },
      "source": [
        "## 🧪 Step 11: Quick Model Evaluation (Optional)\n",
        "\n",
        "Test your trained model on a few sample images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "642c1e93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "642c1e93",
        "outputId": "c694f71c-9101-4962-9e12-adc48f942c38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Looking for best model at: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "✅ Found trained model.\n",
            "🧪 Loading trained model for quick evaluation...\n",
            "📊 Model training info:\n",
            "  - Best epoch: 100\n",
            "  - Best accuracy: 87.56%\n",
            "  - Number of classes (from checkpoint): 37\n",
            "\n",
            "✅ Model loading and info check completed.\n",
            "💡 Note: This step confirms the model file exists and can be loaded.\n",
            "   Actual inference or evaluation on test data is done separately.\n",
            "\n",
            "💡 For comprehensive evaluation:\n",
            "   Use the evaluate.py script with your test dataset\n",
            "   The test set was automatically created during training\n"
          ]
        }
      ],
      "source": [
        "# Quick evaluation of the trained model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os # Import os\n",
        "\n",
        "# Check if TRAINING_CONFIG is defined and get the checkpoint directory\n",
        "if 'TRAINING_CONFIG' in locals() and 'checkpoint_dir' in TRAINING_CONFIG:\n",
        "    checkpoint_dir = TRAINING_CONFIG['checkpoint_dir']\n",
        "    best_model_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_100.pth')\n",
        "    print(f\"🧪 Looking for best model at: {best_model_path}\")\n",
        "else:\n",
        "    # Fallback path if TRAINING_CONFIG is not available (less likely after running previous steps)\n",
        "    best_model_path = '/content/drive/MyDrive/ViT-FishID/checkpoints_extended/model_best.pth'\n",
        "    print(f\"🧪 TRAINING_CONFIG not found or incomplete. Looking for best model at default path: {best_model_path}\")\n",
        "\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    print(\"✅ Found trained model.\")\n",
        "    print(\"🧪 Loading trained model for quick evaluation...\")\n",
        "\n",
        "    # Load model checkpoint info\n",
        "    checkpoint = torch.load(best_model_path, map_location='cpu')\n",
        "\n",
        "    print(f\"📊 Model training info:\")\n",
        "    if 'epoch' in checkpoint:\n",
        "        print(f\"  - Best epoch: {checkpoint['epoch']}\")\n",
        "    if 'best_accuracy' in checkpoint: # Use 'best_accuracy' as seen in checkpoint_info\n",
        "        print(f\"  - Best accuracy: {checkpoint['best_accuracy']:.2f}%\")\n",
        "    elif 'best_acc' in checkpoint: # Keep fallback for 'best_acc'\n",
        "         print(f\"  - Best accuracy: {checkpoint['best_acc']:.2f}%\")\n",
        "    if 'teacher_acc' in checkpoint:\n",
        "        print(f\"  - Teacher accuracy: {checkpoint['teacher_acc']:.2f}%\")\n",
        "\n",
        "    # Get class names if available\n",
        "    if 'class_names' in checkpoint:\n",
        "        class_names = checkpoint['class_names']\n",
        "        print(f\"  - Number of classes: {len(class_names)}\")\n",
        "        print(f\"  - Sample classes: {class_names[:5]}...\")\n",
        "    elif 'num_classes' in checkpoint:\n",
        "         print(f\"  - Number of classes (from checkpoint): {checkpoint['num_classes']}\")\n",
        "\n",
        "\n",
        "    # Note: This cell only checks if the model file can be loaded and prints info.\n",
        "    # It does NOT perform actual inference or evaluation on sample images yet.\n",
        "    print(\"\\n✅ Model loading and info check completed.\")\n",
        "    print(\"💡 Note: This step confirms the model file exists and can be loaded.\")\n",
        "    print(\"   Actual inference or evaluation on test data is done separately.\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(f\"❌ No trained model found at: {best_model_path}\")\n",
        "    print(\"Please ensure training completed successfully and the best model file exists at this location.\")\n",
        "\n",
        "print(\"\\n💡 For comprehensive evaluation:\")\n",
        "print(\"   Use the evaluate.py script with your test dataset\")\n",
        "print(\"   The test set was automatically created during training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bcf6b4d",
      "metadata": {
        "id": "5bcf6b4d"
      },
      "source": [
        "## 🔧 Troubleshooting\n",
        "\n",
        "### Common Issues and Solutions:\n",
        "\n",
        "**1. GPU Memory Error (CUDA out of memory)**\n",
        "- Reduce batch_size to 8 or 4\n",
        "- Restart runtime and try again\n",
        "\n",
        "**2. Data Not Found**\n",
        "- Check that DATA_DIR path is correct\n",
        "- Ensure data is uploaded to Google Drive\n",
        "- Verify folder structure (labeled/ and unlabeled/)\n",
        "\n",
        "**3. Training Stops Unexpectedly**\n",
        "- Colab sessions timeout after 12 hours\n",
        "- Use runtime management to prevent disconnection\n",
        "- Checkpoints are saved every 10 epochs for resuming\n",
        "\n",
        "**4. Low Accuracy**\n",
        "- Increase epochs (try 75-100)\n",
        "- Adjust consistency_weight (try 1.0-3.0)\n",
        "- Lower pseudo_label_threshold (try 0.5-0.6)\n",
        "\n",
        "**5. Consistency Loss is 0.0000**\n",
        "- Lower pseudo_label_threshold to 0.5\n",
        "- Check that you have unlabeled data\n",
        "- Ensure semi_supervised mode is selected"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d21afb",
      "metadata": {
        "id": "b0d21afb"
      },
      "source": [
        "## 🚀 Next Steps\n",
        "\n",
        "After training is complete, you can:\n",
        "\n",
        "1. **Download your model**: The trained model is saved in Google Drive\n",
        "2. **Continue training**: Resume from checkpoints for more epochs\n",
        "3. **Evaluate performance**: Use the test set for final evaluation\n",
        "4. **Deploy model**: Use the trained model for fish classification\n",
        "5. **Experiment**: Try different hyperparameters or architectures\n",
        "\n",
        "### Model Files Saved:\n",
        "- `model_best.pth`: Best performing model (use this for inference)\n",
        "- `model_latest.pth`: Most recent checkpoint\n",
        "- `model_epoch_XX.pth`: Periodic checkpoints\n",
        "\n",
        "### Performance Expectations:\n",
        "- **50 epochs**: ~70-80% accuracy\n",
        "- **100 epochs**: ~75-85% accuracy\n",
        "- **Semi-supervised**: Should outperform supervised training\n",
        "\n",
        "**Happy fish classification! 🐟🎉**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59f603ca"
      },
      "source": [
        "## 📈 Step 7b: Connect to Weights & Biases (Optional)\n",
        "\n",
        "Log in to Weights & Biases for experiment tracking and visualization. You will be prompted to enter your API key."
      ],
      "id": "59f603ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c204844"
      },
      "source": [
        "## 💾 Step 8b: Explicitly Save Best Model Backup\n",
        "\n",
        "This step ensures that `model_best.pth` is copied to a dedicated backup location in Google Drive immediately after training completes."
      ],
      "id": "6c204844"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37ab0bbf",
        "outputId": "ffaeaaf6-a3b9-4992-b560-a634b16f62f8"
      },
      "source": [
        "# Explicitly copy model_best.pth to a backup location\n",
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"💾 Explicitly backing up model_best.pth...\")\n",
        "\n",
        "# Get the primary checkpoint directory from TRAINING_CONFIG\n",
        "checkpoint_dir = TRAINING_CONFIG.get('checkpoint_dir')\n",
        "\n",
        "if checkpoint_dir and os.path.exists(checkpoint_dir):\n",
        "    best_model_source_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_100.pth')\n",
        "\n",
        "    if os.path.exists(best_model_source_path):\n",
        "        # Define a dedicated backup directory path in Google Drive\n",
        "        # Using a simpler path than the full Step 10 save for quick verification\n",
        "        backup_base_dir = '/content/drive/MyDrive/ViT-FishID_BestModel_Backups'\n",
        "        os.makedirs(backup_base_dir, exist_ok=True)\n",
        "\n",
        "        # Create a timestamped filename for the backup\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        backup_filename = f\"model_best_backup_{timestamp}.pth\"\n",
        "        backup_dest_path = os.path.join(backup_base_dir, backup_filename)\n",
        "\n",
        "        try:\n",
        "            shutil.copy2(best_model_source_path, backup_dest_path)\n",
        "            print(f\"✅ Successfully copied model_best.pth to backup:\")\n",
        "            print(f\"   📁 Source: {best_model_source_path}\")\n",
        "            print(f\"   💾 Destination: {backup_dest_path}\")\n",
        "            print(f\"   📏 Size: {os.path.getsize(backup_dest_path) / (1024**2):.1f} MB\")\n",
        "            print(\"🎉 Please check your Google Drive in the 'ViT-FishID_BestModel_Backups' folder!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error copying model_best.pth to backup: {e}\")\n",
        "            print(\"Please check your Google Drive connection and permissions.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"⚠️ model_best.pth not found in the primary checkpoint directory: {checkpoint_dir}\")\n",
        "        print(\"   This means training likely did not complete successfully or the best model wasn't saved.\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ Primary checkpoint directory not found or TRAINING_CONFIG is not set.\")\n",
        "    print(\"   Please ensure Step 7 is run before this step.\")\n",
        "\n",
        "print(\"\\n💾 Explicit backup step complete.\")"
      ],
      "id": "37ab0bbf",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Explicitly backing up model_best.pth...\n",
            "✅ Successfully copied model_best.pth to backup:\n",
            "   📁 Source: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "   💾 Destination: /content/drive/MyDrive/ViT-FishID_BestModel_Backups/model_best_backup_20250815_075025.pth\n",
            "   📏 Size: 982.4 MB\n",
            "🎉 Please check your Google Drive in the 'ViT-FishID_BestModel_Backups' folder!\n",
            "\n",
            "💾 Explicit backup step complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "749b06be"
      },
      "source": [
        "## 📊 Step 12: Evaluate Model on Test Dataset\n",
        "\n",
        "This step runs the `evaluate.py` script to assess the performance of your trained model on the unseen test dataset."
      ],
      "id": "749b06be"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcf8b192",
        "outputId": "1303c5cb-1460-4994-bd59-4172288ce4b0"
      },
      "source": [
        "# Run evaluation script\n",
        "import os\n",
        "import fileinput # Import fileinput for modifying files\n",
        "\n",
        "print(\"🧪 Starting evaluation on the test dataset...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define the path to the evaluation script relative to the repo root\n",
        "eval_script_name = 'evaluate.py'\n",
        "repo_dir = '/content/ViT-FishID'\n",
        "eval_script_path = os.path.join(repo_dir, eval_script_name)\n",
        "\n",
        "\n",
        "# Define the path to the trained model checkpoint\n",
        "# Using the epoch 100 checkpoint as it has the best recorded accuracy\n",
        "model_checkpoint_path = '/content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth'\n",
        "\n",
        "# Define the data directory (from Step 5)\n",
        "data_directory = DATA_DIR # Ensure DATA_DIR is defined from Step 5\n",
        "\n",
        "# Check if the evaluation script and model checkpoint exist\n",
        "if not os.path.exists(eval_script_path):\n",
        "    print(f\"❌ Evaluation script not found at: {eval_script_path}\")\n",
        "    print(f\"Please ensure the ViT-FishID repository was cloned correctly in Step 4 to {repo_dir}.\")\n",
        "elif not os.path.exists(model_checkpoint_path):\n",
        "     print(f\"❌ Model checkpoint not found at: {model_checkpoint_path}\")\n",
        "     print(\"Please ensure training completed successfully and the checkpoint exists.\")\n",
        "elif not os.path.exists(data_directory):\n",
        "     print(f\"❌ Data directory not found at: {data_directory}\")\n",
        "     print(\"Please ensure Step 5 was run correctly.\")\n",
        "else:\n",
        "    print(f\"✅ Found evaluation script: {eval_script_path}\")\n",
        "    print(f\"✅ Found model checkpoint: {model_checkpoint_path}\")\n",
        "    print(f\"✅ Found data directory: {data_directory}\")\n",
        "\n",
        "    # --- FIX 1: Modify evaluate.py to correct the vit_model import statement ---\n",
        "    print(f\"\\n🔧 Correcting import statement for ViTForFishClassification in {eval_script_name}...\")\n",
        "    try:\n",
        "        with fileinput.FileInput(eval_script_path, inplace=True) as file:\n",
        "            for line in file:\n",
        "                # Replace 'from vit_model import' with 'from model import'\n",
        "                # Do NOT print anything else here\n",
        "                print(line.replace('from vit_model import ViTForFishClassification', 'from model import ViTForFishClassification'), end='')\n",
        "        print(f\"✅ Corrected import statement for ViTForFishClassification in {eval_script_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error modifying ViTForFishClassification import in {eval_script_name}: {e}\")\n",
        "        print(\"🚨 Evaluation might still fail due to this import error.\")\n",
        "    # --- End of FIX 1 ---\n",
        "\n",
        "    # --- FIX 2: Modify evaluate.py to comment out the ema_teacher import ---\n",
        "    print(f\"\\n🔧 Commenting out import statement for EMATeacher in {eval_script_name}...\")\n",
        "    try:\n",
        "        with fileinput.FileInput(eval_script_path, inplace=True) as file:\n",
        "            for line in file:\n",
        "                # Comment out 'from ema_teacher import EMATeacher'\n",
        "                # Do NOT print anything else here\n",
        "                if 'from ema_teacher import EMATeacher' in line:\n",
        "                     print(\"# \" + line, end='') # Add # to comment out the line\n",
        "                else:\n",
        "                    print(line, end='')\n",
        "        print(f\"✅ Commented out import statement for EMATeacher in {eval_script_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error commenting out EMATeacher import in {eval_script_name}: {e}\")\n",
        "        print(\"🚨 Evaluation might still fail due to this import error.\")\n",
        "    # --- End of FIX 2 ---\n",
        "\n",
        "    # --- FIX 3: Modify evaluate.py to correct the data_loader import statement ---\n",
        "    print(f\"\\n🔧 Correcting import statement for create_fish_dataloaders in {eval_script_name}...\")\n",
        "    try:\n",
        "        with fileinput.FileInput(eval_script_path, inplace=True) as file:\n",
        "            for line in file:\n",
        "                # Replace 'from data_loader import' with 'from data import'\n",
        "                # Do NOT print anything else here\n",
        "                print(line.replace('from data_loader import create_fish_dataloaders', 'from data import create_fish_dataloaders'), end='')\n",
        "        print(f\"✅ Corrected import statement for create_fish_dataloaders in {eval_script_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error modifying create_fish_dataloaders import in {eval_script_name}: {e}\")\n",
        "        print(\"🚨 Evaluation might still fail due to this import error.\")\n",
        "    # --- End of FIX 3 ---\n",
        "\n",
        "\n",
        "    # Construct the evaluation command\n",
        "    # Use PYTHONPATH to help the script find local modules like model\n",
        "    # Use %cd before and after, but rely on PYTHONPATH for the import\n",
        "    eval_cmd = f\"PYTHONPATH={repo_dir} python {eval_script_name} --data_dir {data_directory} --model_path {model_checkpoint_path}\"\n",
        "\n",
        "\n",
        "    print(\"\\n📋 Evaluation Command:\")\n",
        "    # Print the command cleanly without the PYTHONPATH for readability, but it's included in the execution\n",
        "    print(f\"python {eval_script_name} --data_dir {data_directory} --model_path {model_checkpoint_path} (with PYTHONPATH={repo_dir})\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "    print(\"🚀 Running evaluation...\")\n",
        "    # Change to the repository directory before executing\n",
        "    %cd {repo_dir}\n",
        "\n",
        "    # Execute the evaluation script with PYTHONPATH set\n",
        "    !{eval_cmd}\n",
        "\n",
        "    # Change back to original content directory (optional but good practice)\n",
        "    %cd /content\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"🎉 Evaluation complete!\")\n",
        "\n",
        "print(\"\\n💡 Check the output above for accuracy metrics on the test set.\")"
      ],
      "id": "fcf8b192",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Starting evaluation on the test dataset...\n",
            "==================================================\n",
            "✅ Found evaluation script: /content/ViT-FishID/evaluate.py\n",
            "✅ Found model checkpoint: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "✅ Found data directory: /content/fish_cutouts\n",
            "\n",
            "🔧 Correcting import statement for ViTForFishClassification in evaluate.py...\n",
            "✅ Corrected import statement for ViTForFishClassification in evaluate.py.\n",
            "\n",
            "🔧 Commenting out import statement for EMATeacher in evaluate.py...\n",
            "✅ Commented out import statement for EMATeacher in evaluate.py.\n",
            "\n",
            "🔧 Correcting import statement for create_fish_dataloaders in evaluate.py...\n",
            "✅ Corrected import statement for create_fish_dataloaders in evaluate.py.\n",
            "\n",
            "📋 Evaluation Command:\n",
            "python evaluate.py --data_dir /content/fish_cutouts --model_path /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth (with PYTHONPATH=/content/ViT-FishID)\n",
            "\n",
            "==================================================\n",
            "🚀 Running evaluation...\n",
            "/content/ViT-FishID\n",
            "2025-08-15 08:01:40.428842: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-15 08:01:40.447247: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755244900.468955   18799 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755244900.475482   18799 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755244900.492473   18799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755244900.492499   18799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755244900.492502   18799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755244900.492505   18799 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-15 08:01:40.497464: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ViT-FishID/evaluate.py\", line 15, in <module>\n",
            "    from data import create_fish_dataloaders\n",
            "ImportError: cannot import name 'create_fish_dataloaders' from 'data' (/content/ViT-FishID/data.py)\n",
            "/content\n",
            "\n",
            "==================================================\n",
            "🎉 Evaluation complete!\n",
            "\n",
            "💡 Check the output above for accuracy metrics on the test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7d9d05d"
      },
      "source": [
        "## 🔍 Step 12b: Diagnose `ModuleNotFoundError`\n",
        "\n",
        "This step checks the file structure and import statements to understand why `vit_model` is not being found."
      ],
      "id": "c7d9d05d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca6d7a1c",
        "outputId": "fc236a40-4bb0-4502-f8f5-aa6c01821099"
      },
      "source": [
        "import os\n",
        "\n",
        "print(\"🔍 Diagnosing ModuleNotFoundError...\")\n",
        "repo_dir = '/content/ViT-FishID'\n",
        "eval_script_path = os.path.join(repo_dir, 'evaluate.py')\n",
        "model_file_guess = os.path.join(repo_dir, 'model.py') # Common name for model file\n",
        "vit_model_file_guess = os.path.join(repo_dir, 'vit_model.py') # Guessed name based on import\n",
        "\n",
        "print(f\"Repo directory: {repo_dir}\")\n",
        "\n",
        "print(\"\\n📂 Files in repository root:\")\n",
        "# List files in the repository root\n",
        "if os.path.exists(repo_dir):\n",
        "    !ls -la {repo_dir}\n",
        "else:\n",
        "    print(f\"❌ Repository directory not found: {repo_dir}\")\n",
        "\n",
        "\n",
        "print(f\"\\n📄 Content of {os.path.basename(eval_script_path)} (checking import):\")\n",
        "# Read and print the content of evaluate.py\n",
        "if os.path.exists(eval_script_path):\n",
        "    try:\n",
        "        with open(eval_script_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for i, line in enumerate(lines):\n",
        "                if 'import vit_model' in line or 'from vit_model' in line:\n",
        "                    print(f\"  Line {i+1}: {line.strip()}\")\n",
        "                elif 'ViTForFishClassification' in line:\n",
        "                     print(f\"  Line {i+1}: {line.strip()} (contains class name)\")\n",
        "                if i < 20: # Print first 20 lines for context\n",
        "                     print(f\"  Line {i+1}: {line.strip()}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Could not read {eval_script_path}: {e}\")\n",
        "else:\n",
        "    print(f\"❌ {eval_script_path} not found.\")\n",
        "\n",
        "\n",
        "print(f\"\\n📄 Checking potential model file: {os.path.basename(model_file_guess)}\")\n",
        "# Check if model.py exists and print relevant lines\n",
        "if os.path.exists(model_file_guess):\n",
        "    try:\n",
        "        with open(model_file_guess, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            print(f\"✅ Found {os.path.basename(model_file_guess)}. Checking for class definition...\")\n",
        "            found_class = False\n",
        "            for i, line in enumerate(lines):\n",
        "                 if 'class ViTForFishClassification' in line:\n",
        "                      print(f\"  Line {i+1}: {line.strip()}\")\n",
        "                      found_class = True\n",
        "                      break # Found the class, stop searching\n",
        "\n",
        "            if not found_class:\n",
        "                 print(f\"⚠️ 'ViTForFishClassification' class definition not found in {os.path.basename(model_file_guess)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Could not read {model_file_guess}: {e}\")\n",
        "else:\n",
        "    print(f\"❓ {os.path.basename(model_file_guess)} not found. Checking alternative name...\")\n",
        "\n",
        "print(f\"\\n📄 Checking alternative model file: {os.path.basename(vit_model_file_guess)}\")\n",
        "# Check if vit_model.py exists and print relevant lines\n",
        "if os.path.exists(vit_model_file_guess):\n",
        "    try:\n",
        "        with open(vit_model_file_guess, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            print(f\"✅ Found {os.path.basename(vit_model_file_guess)}. Checking for class definition...\")\n",
        "            found_class = False\n",
        "            for i, line in enumerate(lines):\n",
        "                 if 'class ViTForFishClassification' in line:\n",
        "                      print(f\"  Line {i+1}: {line.strip()}\")\n",
        "                      found_class = True\n",
        "                      break # Found the class, stop searching\n",
        "\n",
        "            if not found_class:\n",
        "                 print(f\"⚠️ 'ViTForFishClassification' class definition not found in {os.path.basename(vit_model_file_guess)}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Could not read {vit_model_file_guess}: {e}\")\n",
        "else:\n",
        "    print(f\"❓ {os.path.basename(vit_model_file_guess)} not found.\")\n",
        "\n",
        "print(\"\\nDiagnosis steps complete. Please review the output.\")"
      ],
      "id": "ca6d7a1c",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Diagnosing ModuleNotFoundError...\n",
            "Repo directory: /content/ViT-FishID\n",
            "\n",
            "📂 Files in repository root:\n",
            "total 368\n",
            "drwxr-xr-x 6 root root   4096 Aug 15 07:03 .\n",
            "drwxr-xr-x 1 root root   4096 Aug 15 06:58 ..\n",
            "-rw-r--r-- 1 root root  21217 Aug 15 06:58 data.py\n",
            "-rw-r--r-- 1 root root  11572 Aug 15 06:58 evaluate.py\n",
            "-rw-r--r-- 1 root root   3328 Aug 15 06:58 EXTENDED_TRAINING_SETUP.md\n",
            "drwxr-xr-x 2 root root   4096 Aug 15 06:58 fish_cutouts\n",
            "drwxr-xr-x 8 root root   4096 Aug 15 06:58 .git\n",
            "-rw-r--r-- 1 root root     66 Aug 15 06:58 .gitattributes\n",
            "-rw-r--r-- 1 root root    646 Aug 15 06:58 .gitignore\n",
            "-rw-r--r-- 1 root root   9495 Aug 15 06:58 model.py\n",
            "-rw-r--r-- 1 root root  16771 Aug 15 06:58 pipeline.py\n",
            "drwxr-xr-x 2 root root   4096 Aug 15 07:03 __pycache__\n",
            "-rw-r--r-- 1 root root  16566 Aug 15 06:58 README.md\n",
            "-rw-r--r-- 1 root root    202 Aug 15 06:58 requirements.txt\n",
            "-rw-r--r-- 1 root root   4265 Aug 15 06:58 resume_training.py\n",
            "-rw-r--r-- 1 root root   5134 Aug 15 06:58 species_mapping.txt\n",
            "-rw-r--r-- 1 root root  25498 Aug 15 07:03 trainer.py\n",
            "-rw-r--r-- 1 root root   4982 Aug 15 06:58 TRAINING_FIXES_APPLIED.md\n",
            "-rw-r--r-- 1 root root  15331 Aug 15 06:58 train.py\n",
            "-rw-r--r-- 1 root root   8818 Aug 15 06:58 utils.py\n",
            "-rw-r--r-- 1 root root 160971 Aug 15 06:58 ViT_FishID_Colab_Training.ipynb\n",
            "drwxr-xr-x 3 root root   4096 Aug 15 07:03 wandb\n",
            "\n",
            "📄 Content of evaluate.py (checking import):\n",
            "  Line 1: import torch\n",
            "  Line 2: import torch.nn as nn\n",
            "  Line 3: from torch.utils.data import DataLoader\n",
            "  Line 4: import numpy as np\n",
            "  Line 5: from sklearn.metrics import classification_report, confusion_matrix\n",
            "  Line 6: import matplotlib.pyplot as plt\n",
            "  Line 7: import seaborn as sns\n",
            "  Line 8: from typing import Dict, List, Tuple\n",
            "  Line 9: import os\n",
            "  Line 10: from tqdm import tqdm\n",
            "  Line 11: \n",
            "  Line 12: from vit_model import ViTForFishClassification\n",
            "  Line 12: from vit_model import ViTForFishClassification\n",
            "  Line 13: from ema_teacher import EMATeacher\n",
            "  Line 14: from data_loader import create_fish_dataloaders\n",
            "  Line 15: from utils import accuracy, load_checkpoint, get_device\n",
            "  Line 16: \n",
            "  Line 17: \n",
            "  Line 18: class ModelEvaluator:\n",
            "  Line 19: \"\"\"\n",
            "  Line 20: Comprehensive model evaluation for ViT-Fish classification.\n",
            "  Line 25: model: ViTForFishClassification, (contains class name)\n",
            "  Line 236: student_model: ViTForFishClassification, (contains class name)\n",
            "  Line 237: teacher_model: ViTForFishClassification, (contains class name)\n",
            "  Line 311: student_model = ViTForFishClassification(num_classes=num_classes) (contains class name)\n",
            "  Line 318: teacher_model = ViTForFishClassification(num_classes=num_classes) (contains class name)\n",
            "\n",
            "📄 Checking potential model file: model.py\n",
            "✅ Found model.py. Checking for class definition...\n",
            "  Line 22: class ViTForFishClassification(nn.Module):\n",
            "\n",
            "📄 Checking alternative model file: vit_model.py\n",
            "❓ vit_model.py not found.\n",
            "\n",
            "Diagnosis steps complete. Please review the output.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}