{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0e0af9a0",
      "metadata": {
        "id": "0e0af9a0"
      },
      "source": [
        "# üêü ViT-FishID: Semi-Supervised Fish Classification\n",
        "\n",
        "**COMPLETE TRAINING PIPELINE WITH GOOGLE COLAB**\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/cat-thomson/ViT-FishID/blob/main/ViT_FishID_Colab_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "## üéØ What This Notebook Does\n",
        "\n",
        "This notebook implements a **complete semi-supervised learning pipeline** for fish species classification using:\n",
        "\n",
        "**ü§ñ Vision Transformer (ViT)**: State-of-the-art transformer architecture for image classification\n",
        "**üìä Semi-Supervised Learning**: Leverages both labeled and unlabeled fish images\n",
        "**üéì EMA Teacher-Student Framework**: Uses exponential moving averages for consistency training\n",
        "**‚òÅÔ∏è Google Colab**: Cloud-based training with GPU acceleration\n",
        "\n",
        "## üìä Expected Performance\n",
        "\n",
        "- **Training Time**: 4-6 hours for 100 epochs\n",
        "- **GPU Requirements**: T4/V100/A100 (Colab Pro recommended)\n",
        "- **Expected Accuracy**: 80-90% on fish species classification\n",
        "- **Data Efficiency**: Works well with limited labeled data\n",
        "\n",
        "## üõ†Ô∏è What You Need\n",
        "\n",
        "1. **Fish Dataset**: Labeled and unlabeled fish images (upload to Google Drive)\n",
        "2. **Google Colab Pro**: Recommended for longer training sessions\n",
        "3. **Weights & Biases Account**: Optional for experiment tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26bcb2a3",
      "metadata": {
        "id": "26bcb2a3"
      },
      "source": [
        "## üîß Step 1: Environment Setup and GPU Check\n",
        "\n",
        "First, let's verify that we have GPU access and set up the optimal environment for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f3540b19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3540b19",
        "outputId": "23cef4f3-72a1-4e63-de50-2c68d478e6b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç SYSTEM INFORMATION\n",
            "==================================================\n",
            "Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "GPU Device: Tesla T4\n",
            "GPU Memory: 14.7 GB\n",
            "‚úÖ GPU is ready for training!\n",
            "üöÄ GPU optimized for training\n",
            "\n",
            "üéØ Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability and system information\n",
        "import torch\n",
        "import os\n",
        "import gc\n",
        "\n",
        "print(\"üîç SYSTEM INFORMATION\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Python version: {os.sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    device_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f\"GPU Device: {device_name}\")\n",
        "    print(f\"GPU Memory: {device_memory:.1f} GB\")\n",
        "    print(\"‚úÖ GPU is ready for training!\")\n",
        "\n",
        "    # Set optimal GPU settings\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "\n",
        "    # Clear GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"üöÄ GPU optimized for training\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No GPU detected!\")\n",
        "    print(\"üìù To enable GPU in Colab:\")\n",
        "    print(\"   Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")\n",
        "    print(\"   Then restart this notebook\")\n",
        "\n",
        "# Set device for later use\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nüéØ Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149f671b",
      "metadata": {
        "id": "149f671b"
      },
      "source": [
        "## üìÅ Step 2: Mount Google Drive\n",
        "\n",
        "This will give us access to your fish dataset stored in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4abb3ffd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4abb3ffd",
        "outputId": "8788f96b-035e-431d-85cf-aac9ad80ffad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to mount Google Drive...\n",
            "Clearing contents of mount point: /content/drive\n",
            "‚úÖ Mount point cleared.\n",
            "Mounted at /content/drive\n",
            "\n",
            "üìÇ Google Drive contents:\n",
            "  - Mock Matric\n",
            "  - Photos\n",
            "  - Admin\n",
            "  - Uni\n",
            "  - Fish_Training_Output\n",
            "  - Colab Notebooks\n",
            "  - ViT-FishID\n",
            "  - ViT-FishID_Training_20250814_154652\n",
            "  - ViT-FishID_Training_20250814_202307\n",
            "  - ViT-FishID_Training_20250814_205442\n",
            "  ... and 6 more items\n",
            "\n",
            "‚úÖ Google Drive mounted successfully!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Attempting to mount Google Drive...\")\n",
        "\n",
        "# Ensure the mount point is clean before mounting\n",
        "mount_point = '/content/drive'\n",
        "if os.path.exists(mount_point) and os.path.isdir(mount_point):\n",
        "    print(f\"Clearing contents of mount point: {mount_point}\")\n",
        "    try:\n",
        "        # Use `rm -rf` via shell command for robustness in Colab environment\n",
        "        !rm -rf {mount_point}/*\n",
        "        # Recreate the directory structure if it was completely removed\n",
        "        if not os.path.exists(mount_point):\n",
        "             os.makedirs(mount_point)\n",
        "        print(\"‚úÖ Mount point cleared.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error clearing mount point: {e}\")\n",
        "        print(\"Attempting to proceed with mount anyway...\")\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# List contents to verify mount\n",
        "print(\"\\nüìÇ Google Drive contents:\")\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "if os.path.exists(drive_path):\n",
        "    items = os.listdir(drive_path)[:10]  # Show first 10 items\n",
        "    for item in items:\n",
        "        print(f\"  - {item}\")\n",
        "    if len(os.listdir(drive_path)) > 10:\n",
        "        print(f\"  ... and {len(os.listdir(drive_path)) - 10} more items\")\n",
        "    print(\"\\n‚úÖ Google Drive mounted successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to mount Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be8b6273",
      "metadata": {
        "id": "be8b6273"
      },
      "source": [
        "## üì¶ Step 3: Install Dependencies\n",
        "\n",
        "Installing all required packages for ViT-FishID training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8c724abc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c724abc",
        "outputId": "ee0d1d80-c1a2-43a0-97e5-f8bd530daaf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Installing dependencies...\n",
            "‚úÖ All dependencies installed successfully!\n",
            "\n",
            "üìã Package versions:\n",
            "  - torch: 2.6.0+cu124\n",
            "  - torchvision: 0.21.0+cu124\n",
            "  - timm: 1.0.19\n",
            "  - albumentations: 2.0.8\n",
            "  - opencv: 4.12.0\n",
            "  - sklearn: 1.6.1\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q timm transformers\n",
        "!pip install -q albumentations\n",
        "!pip install -q wandb\n",
        "!pip install -q opencv-python-headless\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q matplotlib seaborn\n",
        "!pip install -q tqdm\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully!\")\n",
        "\n",
        "# Verify installations\n",
        "import torch\n",
        "import torchvision\n",
        "import timm\n",
        "import albumentations\n",
        "import cv2\n",
        "import sklearn\n",
        "\n",
        "print(\"\\nüìã Package versions:\")\n",
        "print(f\"  - torch: {torch.__version__}\")\n",
        "print(f\"  - torchvision: {torchvision.__version__}\")\n",
        "print(f\"  - timm: {timm.__version__}\")\n",
        "print(f\"  - albumentations: {albumentations.__version__}\")\n",
        "print(f\"  - opencv: {cv2.__version__}\")\n",
        "print(f\"  - sklearn: {sklearn.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b795fc",
      "metadata": {
        "id": "12b795fc"
      },
      "source": [
        "## üîÑ Step 4: Clone ViT-FishID Repository\n",
        "\n",
        "Getting the latest code from your GitHub repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c4e4cd45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4e4cd45",
        "outputId": "86085e4c-db2e-498e-8baa-f7c49ef7021b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Cloning ViT-FishID repository...\n",
            "Cloning into '/content/ViT-FishID'...\n",
            "remote: Enumerating objects: 164, done.\u001b[K\n",
            "remote: Counting objects: 100% (164/164), done.\u001b[K\n",
            "remote: Compressing objects: 100% (122/122), done.\u001b[K\n",
            "remote: Total 164 (delta 69), reused 124 (delta 35), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (164/164), 322.97 KiB | 1.32 MiB/s, done.\n",
            "Resolving deltas: 100% (69/69), done.\n",
            "/content/ViT-FishID\n",
            "\n",
            "üìÇ Project structure:\n",
            "total 612\n",
            "drwxr-xr-x 5 root root   4096 Aug 18 09:21 .\n",
            "drwxr-xr-x 1 root root   4096 Aug 18 09:21 ..\n",
            "-rw-r--r-- 1 root root   4182 Aug 18 09:21 COLAB_CRASH_FIXES.md\n",
            "-rw-r--r-- 1 root root  21217 Aug 18 09:21 data.py\n",
            "-rw-r--r-- 1 root root  11572 Aug 18 09:21 evaluate.py\n",
            "-rw-r--r-- 1 root root   3328 Aug 18 09:21 EXTENDED_TRAINING_SETUP.md\n",
            "drwxr-xr-x 3 root root   4096 Aug 18 09:21 fish_cutouts\n",
            "drwxr-xr-x 8 root root   4096 Aug 18 09:21 .git\n",
            "-rw-r--r-- 1 root root     66 Aug 18 09:21 .gitattributes\n",
            "-rw-r--r-- 1 root root    646 Aug 18 09:21 .gitignore\n",
            "drwxr-xr-x 2 root root   4096 Aug 18 09:21 local_checkpoints\n",
            "-rw-r--r-- 1 root root  13100 Aug 18 09:21 local_resume_training.py\n",
            "-rw-r--r-- 1 root root      0 Aug 18 09:21 MAE_INTEGRATION_GUIDE.md\n",
            "-rw-r--r-- 1 root root   9495 Aug 18 09:21 model.py\n",
            "-rw-r--r-- 1 root root  16771 Aug 18 09:21 pipeline.py\n",
            "-rw-r--r-- 1 root root  16566 Aug 18 09:21 README.md\n",
            "-rw-r--r-- 1 root root    202 Aug 18 09:21 requirements.txt\n",
            "-rw-r--r-- 1 root root   4271 Aug 18 09:21 resume_training.py\n",
            "-rw-r--r-- 1 root root   5134 Aug 18 09:21 species_mapping.txt\n",
            "-rw-r--r-- 1 root root  25503 Aug 18 09:21 trainer.py\n",
            "-rw-r--r-- 1 root root   4982 Aug 18 09:21 TRAINING_FIXES_APPLIED.md\n",
            "-rw-r--r-- 1 root root  15343 Aug 18 09:21 train.py\n",
            "-rw-r--r-- 1 root root   8818 Aug 18 09:21 utils.py\n",
            "-rw-r--r-- 1 root root 153962 Aug 18 09:21 ViT_FishID_Colab_Training.ipynb\n",
            "-rw-r--r-- 1 root root 234973 Aug 18 09:21 ViT_FishID_MAE_EMA_Training.ipynb\n",
            "\n",
            "‚úÖ Repository cloned successfully!\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "import os\n",
        "\n",
        "# Remove existing directory if it exists\n",
        "if os.path.exists('/content/ViT-FishID'):\n",
        "    !rm -rf /content/ViT-FishID\n",
        "\n",
        "# Clone the repository\n",
        "print(\"üì• Cloning ViT-FishID repository...\")\n",
        "!git clone https://github.com/cat-thomson/ViT-FishID.git /content/ViT-FishID\n",
        "\n",
        "# Change to project directory\n",
        "%cd /content/ViT-FishID\n",
        "\n",
        "# List project files\n",
        "print(\"\\nüìÇ Project structure:\")\n",
        "!ls -la\n",
        "\n",
        "print(\"\\n‚úÖ Repository cloned successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8155c400",
      "metadata": {
        "id": "8155c400"
      },
      "source": [
        "## üê† Step 5: Setup Fish Dataset\n",
        "\n",
        "**Important**: Upload your `fish_cutouts.zip` file to Google Drive before running this step.\n",
        "\n",
        "Expected dataset structure:\n",
        "```\n",
        "fish_cutouts/\n",
        "‚îú‚îÄ‚îÄ labeled/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ species_1/\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fish_001.jpg\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fish_002.jpg\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ species_2/\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ ...\n",
        "‚îî‚îÄ‚îÄ unlabeled/\n",
        "    ‚îú‚îÄ‚îÄ fish_003.jpg\n",
        "    ‚îî‚îÄ‚îÄ fish_004.jpg\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "nre5_INaKDXl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nre5_INaKDXl",
        "outputId": "f9e30750-e5ec-4cbe-8048-1cf712706717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üê† SETTING UP FISH DATASET\n",
            "==================================================\n",
            "üìÇ Looking for dataset: /content/drive/MyDrive/fish_cutouts.zip\n",
            "üéØ Target directory: /content/fish_cutouts\n",
            "üì• Extracting dataset from Google Drive...\n",
            "‚úÖ Found dataset: 216.5 MB\n",
            "üìÅ Extracted: ['dataset_info.json', 'unlabeled', '__MACOSX', 'labeled']\n",
            "‚úÖ Dataset organized successfully!\n",
            "üêü Verified: 37 species\n",
            "üìä Verified: 24015 unlabeled images\n",
            "\n",
            "‚úÖ DATASET READY\n",
            "üìÅ Location: /content/fish_cutouts\n",
            "üöÄ Ready for training!\n"
          ]
        }
      ],
      "source": [
        "# Setup fish dataset from Google Drive\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print(\"üê† SETTING UP FISH DATASET\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Configuration\n",
        "ZIP_FILE_PATH = '/content/drive/MyDrive/fish_cutouts.zip'\n",
        "DATA_DIR = '/content/fish_cutouts'\n",
        "\n",
        "print(f\"üìÇ Looking for dataset: {ZIP_FILE_PATH}\")\n",
        "print(f\"üéØ Target directory: {DATA_DIR}\")\n",
        "\n",
        "# Check if data already exists locally\n",
        "if os.path.exists(DATA_DIR) and os.path.exists(os.path.join(DATA_DIR, 'labeled')):\n",
        "    print(\"‚úÖ Dataset already available locally!\")\n",
        "\n",
        "    # Quick validation\n",
        "    labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
        "    unlabeled_dir = os.path.join(DATA_DIR, 'unlabeled')\n",
        "\n",
        "    if os.path.exists(labeled_dir):\n",
        "        species_count = len([d for d in os.listdir(labeled_dir)\n",
        "                           if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')])\n",
        "        print(f\"üêü Found {species_count} labeled species\")\n",
        "\n",
        "    if os.path.exists(unlabeled_dir):\n",
        "        unlabeled_count = len([f for f in os.listdir(unlabeled_dir)\n",
        "                             if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "        print(f\"üìä Found {unlabeled_count} unlabeled images\")\n",
        "\n",
        "else:\n",
        "    print(\"üì• Extracting dataset from Google Drive...\")\n",
        "\n",
        "    # Check if ZIP file exists\n",
        "    if not os.path.exists(ZIP_FILE_PATH):\n",
        "        print(f\"‚ùå Dataset not found at: {ZIP_FILE_PATH}\")\n",
        "        print(\"üìù Please upload fish_cutouts.zip to Google Drive root directory\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Found dataset: {os.path.getsize(ZIP_FILE_PATH) / (1024**2):.1f} MB\")\n",
        "\n",
        "        try:\n",
        "            # Extract to temporary directory\n",
        "            temp_dir = '/content/temp_extract'\n",
        "            if os.path.exists(temp_dir):\n",
        "                shutil.rmtree(temp_dir)\n",
        "\n",
        "            with zipfile.ZipFile(ZIP_FILE_PATH, 'r') as zip_ref:\n",
        "                zip_ref.extractall(temp_dir)\n",
        "\n",
        "            # Find and organize data\n",
        "            extracted_items = os.listdir(temp_dir)\n",
        "            print(f\"üìÅ Extracted: {extracted_items}\")\n",
        "\n",
        "            # Look for labeled and unlabeled directories\n",
        "            labeled_source = None\n",
        "            unlabeled_source = None\n",
        "\n",
        "            for item in extracted_items:\n",
        "                item_path = os.path.join(temp_dir, item)\n",
        "                if item == 'labeled' and os.path.isdir(item_path):\n",
        "                    labeled_source = item_path\n",
        "                elif item == 'unlabeled' and os.path.isdir(item_path):\n",
        "                    unlabeled_source = item_path\n",
        "\n",
        "            if labeled_source and unlabeled_source:\n",
        "                # Create target directory\n",
        "                if os.path.exists(DATA_DIR):\n",
        "                    shutil.rmtree(DATA_DIR)\n",
        "                os.makedirs(DATA_DIR)\n",
        "\n",
        "                # Move directories\n",
        "                shutil.move(labeled_source, os.path.join(DATA_DIR, 'labeled'))\n",
        "                shutil.move(unlabeled_source, os.path.join(DATA_DIR, 'unlabeled'))\n",
        "\n",
        "                print(\"‚úÖ Dataset organized successfully!\")\n",
        "\n",
        "                # Verify structure\n",
        "                labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
        "                species_count = len([d for d in os.listdir(labeled_dir)\n",
        "                                   if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')])\n",
        "\n",
        "                unlabeled_dir = os.path.join(DATA_DIR, 'unlabeled')\n",
        "                unlabeled_count = len([f for f in os.listdir(unlabeled_dir)\n",
        "                                     if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "\n",
        "                print(f\"üêü Verified: {species_count} species\")\n",
        "                print(f\"üìä Verified: {unlabeled_count} unlabeled images\")\n",
        "\n",
        "            else:\n",
        "                print(\"‚ùå Could not find labeled and unlabeled directories\")\n",
        "\n",
        "            # Cleanup\n",
        "            if os.path.exists(temp_dir):\n",
        "                shutil.rmtree(temp_dir)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error extracting dataset: {e}\")\n",
        "\n",
        "# Final verification\n",
        "if os.path.exists(DATA_DIR):\n",
        "    print(f\"\\n‚úÖ DATASET READY\")\n",
        "    print(f\"üìÅ Location: {DATA_DIR}\")\n",
        "    print(\"üöÄ Ready for training!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå DATASET SETUP FAILED\")\n",
        "    print(\"Please check that fish_cutouts.zip is uploaded to Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f0fe32",
      "metadata": {
        "id": "31f0fe32"
      },
      "source": [
        "## üìà Step 6: Setup Weights & Biases (Optional)\n",
        "\n",
        "Weights & Biases provides excellent training visualization and experiment tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ab343772",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab343772",
        "outputId": "5ecc5316-eef3-4af2-e01e-01987bf9c2a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìà SETTING UP WEIGHTS & BIASES\n",
            "========================================\n",
            "üîë Please enter your W&B API key when prompted\n",
            "üí° Get your API key from: https://wandb.ai/settings\n",
            "‚úÖ Successfully logged in to W&B\n",
            "üìä W&B not connected - training will continue without logging\n",
            "‚úÖ W&B setup complete (Enabled: False)\n"
          ]
        }
      ],
      "source": [
        "# Login to Weights & Biases for experiment tracking\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "print(\"üìà SETTING UP WEIGHTS & BIASES\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Check if API key is available\n",
        "if os.environ.get(\"WANDB_API_KEY\"):\n",
        "    print(\"‚úÖ W&B API key found in environment\")\n",
        "    try:\n",
        "        wandb.login(relogin=True)\n",
        "        print(\"‚úÖ Successfully logged in to W&B\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è W&B relogin failed: {e}\")\n",
        "        print(\"Trying manual login...\")\n",
        "        wandb.login()\n",
        "else:\n",
        "    print(\"üîë Please enter your W&B API key when prompted\")\n",
        "    print(\"üí° Get your API key from: https://wandb.ai/settings\")\n",
        "    try:\n",
        "        wandb.login()\n",
        "        print(\"‚úÖ Successfully logged in to W&B\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå W&B login failed: {e}\")\n",
        "        print(\"Continuing without W&B logging...\")\n",
        "\n",
        "# Check connection status\n",
        "if wandb.run:\n",
        "    print(f\"üöÄ W&B Run URL: {wandb.run.url}\")\n",
        "    USE_WANDB = True\n",
        "else:\n",
        "    print(\"üìä W&B not connected - training will continue without logging\")\n",
        "    USE_WANDB = False\n",
        "\n",
        "print(f\"‚úÖ W&B setup complete (Enabled: {USE_WANDB})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5190f01",
      "metadata": {
        "id": "b5190f01"
      },
      "source": [
        "## üîÑ Step 6: Locate Checkpoint from Epoch 19\n",
        "\n",
        "Finding your saved checkpoint to resume training from where you left off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "61b35ced",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61b35ced",
        "outputId": "d584f10c-807c-4a0f-a05e-ac739c6b0bdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Looking for checkpoint from epoch 100...\n",
            "üíæ New checkpoints will be saved to: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints\n",
            "‚ùå No checkpoint found for epoch 19!\n",
            "üöÄ Starting fresh training from epoch 1\n"
          ]
        }
      ],
      "source": [
        "# Locate checkpoint from epoch 19\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "\n",
        "print(\"üîç Looking for checkpoint from epoch 100...\")\n",
        "\n",
        "# Configuration\n",
        "# Always start from the beginning\n",
        "checkpoint_path = None\n",
        "checkpoint_info = None\n",
        "RESUME_CHECKPOINT = None # Ensure this is explicitly set to None\n",
        "\n",
        "# Set up checkpoint directory for new saves\n",
        "checkpoint_save_dir = '/content/drive/MyDrive/ViT-FishID/pretrained_checkpoints'\n",
        "os.makedirs(checkpoint_save_dir, exist_ok=True)\n",
        "print(f\"üíæ New checkpoints will be saved to: {checkpoint_save_dir}\")\n",
        "\n",
        "\n",
        "if checkpoint_path:\n",
        "    print(f\"\\nüéâ Checkpoint ready for resuming training!\")\n",
        "    print(f\"üìÑ File: {os.path.basename(checkpoint_path)}\")\n",
        "    print(f\"üìè Size: {os.path.getsize(checkpoint_path) / (1024*1024):.1f} MB\")\n",
        "    print(f\"üíæ New checkpoints will be saved to: {checkpoint_save_dir}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No checkpoint found for epoch 19!\")\n",
        "    print(\"üöÄ Starting fresh training from epoch 1\")\n",
        "\n",
        "# Store checkpoint path for later use\n",
        "RESUME_CHECKPOINT = checkpoint_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fe6af6d",
      "metadata": {
        "id": "0fe6af6d"
      },
      "source": [
        "## ‚öôÔ∏è Step 7: Configure Training Parameters\n",
        "\n",
        "Configure the training settings for your semi-supervised fish classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "tf_3Wca69-JJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf_3Wca69-JJ",
        "outputId": "1c309b67-092a-4448-e82c-85f29fe42508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚öôÔ∏è TRAINING CONFIGURATION\n",
            "==================================================\n",
            "üìä Auto-detected 37 fish species\n",
            "üìÅ Checkpoints: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints\n",
            "üíæ Backups: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints_backup\n",
            "\n",
            "üìã TRAINING CONFIGURATION\n",
            "==================================================\n",
            "üéØ Training mode: semi_supervised\n",
            "üìä Total epochs: 100\n",
            "üì¶ Batch size: 16\n",
            "üß† Model: vit_small_patch16_224\n",
            "üêü Number of species: 37\n",
            "‚öñÔ∏è Consistency weight: 2.0\n",
            "üéØ Pseudo-label threshold: 0.7\n",
            "üíæ Save frequency: Every 10 epochs\n",
            "üìà W&B logging: False\n",
            "\n",
            "‚è±Ô∏è Estimated training time: 5.0 hours\n",
            "üí° Recommendation: Use Colab Pro for longer training sessions\n",
            "\n",
            "‚úÖ Configuration complete - ready to start training!\n"
          ]
        }
      ],
      "source": [
        "# Training Configuration for Semi-Supervised Fish Classification\n",
        "import os\n",
        "\n",
        "print(\"‚öôÔ∏è TRAINING CONFIGURATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Auto-detect number of species from dataset\n",
        "NUM_CLASSES = 37  # Default\n",
        "if 'DATA_DIR' in globals() and os.path.exists(DATA_DIR):\n",
        "    labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
        "    if os.path.exists(labeled_dir):\n",
        "        species_count = len([d for d in os.listdir(labeled_dir)\n",
        "                           if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')])\n",
        "        NUM_CLASSES = species_count\n",
        "        print(f\"üìä Auto-detected {species_count} fish species\")\n",
        "\n",
        "# Create checkpoint directories\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/ViT-FishID/pretrained_checkpoints'\n",
        "BACKUP_DIR = '/content/drive/MyDrive/ViT-FishID/pretrained_checkpoints_backup'\n",
        "\n",
        "try:\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "    os.makedirs(BACKUP_DIR, exist_ok=True)\n",
        "    print(f\"üìÅ Checkpoints: {CHECKPOINT_DIR}\")\n",
        "    print(f\"üíæ Backups: {BACKUP_DIR}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not create Google Drive directories: {e}\")\n",
        "    CHECKPOINT_DIR = '/content/pretraine_checkpoints'\n",
        "    BACKUP_DIR = '/content/pretrained_checkpoints_backup'\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "    os.makedirs(BACKUP_DIR, exist_ok=True)\n",
        "    print(f\"üìÅ Using local checkpoints: {CHECKPOINT_DIR}\")\n",
        "\n",
        "# Training Configuration\n",
        "TRAINING_CONFIG = {\n",
        "    # BASIC SETTINGS\n",
        "    'mode': 'semi_supervised',\n",
        "    'data_dir': DATA_DIR if 'DATA_DIR' in globals() else '/content/fish_cutouts',\n",
        "    'epochs': 100,\n",
        "    'batch_size': 16,\n",
        "    'learning_rate': 1e-4,\n",
        "    'weight_decay': 0.05,\n",
        "\n",
        "    # MODEL SETTINGS\n",
        "    'model_name': 'vit_small_patch16_224',\n",
        "    'num_classes': NUM_CLASSES,\n",
        "    'pretrained': True,\n",
        "\n",
        "    # SEMI-SUPERVISED SETTINGS\n",
        "    'consistency_weight': 2.0,\n",
        "    'pseudo_label_threshold': 0.7,\n",
        "    'temperature': 4.0,\n",
        "    'warmup_epochs': 10,\n",
        "    'ramp_up_epochs': 30,\n",
        "\n",
        "    # CHECKPOINT SETTINGS\n",
        "    'save_frequency': 10,  # Save every 10 epochs\n",
        "    'checkpoint_dir': CHECKPOINT_DIR,\n",
        "    'backup_dir': BACKUP_DIR,\n",
        "\n",
        "    # LOGGING SETTINGS\n",
        "    'use_wandb': USE_WANDB if 'USE_WANDB' in globals() else False,\n",
        "    'wandb_project': 'ViT-FishID-Training',\n",
        "    'wandb_run_name': f'fish-classification-{NUM_CLASSES}-classes',\n",
        "}\n",
        "\n",
        "print(\"\\nüìã TRAINING CONFIGURATION\")\n",
        "print(\"=\"*50)\n",
        "print(f\"üéØ Training mode: {TRAINING_CONFIG['mode']}\")\n",
        "print(f\"üìä Total epochs: {TRAINING_CONFIG['epochs']}\")\n",
        "print(f\"üì¶ Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
        "print(f\"üß† Model: {TRAINING_CONFIG['model_name']}\")\n",
        "print(f\"üêü Number of species: {TRAINING_CONFIG['num_classes']}\")\n",
        "print(f\"‚öñÔ∏è Consistency weight: {TRAINING_CONFIG['consistency_weight']}\")\n",
        "print(f\"üéØ Pseudo-label threshold: {TRAINING_CONFIG['pseudo_label_threshold']}\")\n",
        "print(f\"üíæ Save frequency: Every {TRAINING_CONFIG['save_frequency']} epochs\")\n",
        "print(f\"üìà W&B logging: {TRAINING_CONFIG['use_wandb']}\")\n",
        "\n",
        "# Time estimation\n",
        "estimated_time_hours = TRAINING_CONFIG['epochs'] * 3 / 60  # ~3 minutes per epoch\n",
        "print(f\"\\n‚è±Ô∏è Estimated training time: {estimated_time_hours:.1f} hours\")\n",
        "print(f\"üí° Recommendation: Use Colab Pro for longer training sessions\")\n",
        "\n",
        "print(\"\\n‚úÖ Configuration complete - ready to start training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34762fd6",
      "metadata": {
        "id": "34762fd6"
      },
      "source": [
        "## ü§ñ Step 7a: Load MAE Pre-trained Model (Optional)\n",
        "\n",
        "**This step loads your pre-trained MAE model to initialize the ViT encoder with better features.**\n",
        "\n",
        "The MAE (Masked Autoencoder) model you trained provides much better initial weights for the Vision Transformer compared to ImageNet pretraining, especially for fish images since it was trained specifically on your fish dataset.\n",
        "\n",
        "Benefits of using MAE initialization:\n",
        "- **Better Feature Representations**: Learned specifically on fish images\n",
        "- **Faster Convergence**: Model starts with relevant features\n",
        "- **Improved Performance**: Often leads to 2-5% accuracy improvement\n",
        "\n",
        "### üìÅ MAE Model Locations\n",
        "\n",
        "Your MAE models should be in one of these locations:\n",
        "- **Local**: `/Users/catalinathomson/Desktop/Fish/ViT-FishID/mae_checkpoints/mae_final_model.pth`\n",
        "- **Google Drive**: `/content/drive/MyDrive/mae_checkpoints/mae_final_model.pth` (after upload)\n",
        "\n",
        "### üîß Setup Instructions\n",
        "\n",
        "1. **Upload MAE Model**: Upload your `mae_final_model.pth` or `mae_best_model.pth` to Google Drive\n",
        "2. **Update Path**: Modify `MAE_MODEL_PATH` in the next cell if needed\n",
        "3. **Enable/Disable**: Set `LOAD_MAE_PRETRAINED = True/False` to control MAE loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "k8pzp_Lc8NfN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8pzp_Lc8NfN",
        "outputId": "ec5dcbe7-a609-459b-d129-7a11fdbaf131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ SETTING UP MAE-INITIALIZED ViT MODEL\n",
            "============================================================\n",
            "Configured MAE_MODEL_PATH: /content/drive/MyDrive/mae_checkpoints/mae_final_model.pth\n",
            "Configured LOAD_MAE_PRETRAINED: True\n",
            "Attempting to load MAE pretrained model from: /content/drive/MyDrive/mae_checkpoints/mae_final_model.pth\n",
            "‚úÖ Found MAE model: mae_final_model.pth\n",
            "üìè Size: 149.6 MB\n",
            "üì• Loading MAE checkpoint from: /content/drive/MyDrive/mae_checkpoints/mae_final_model.pth\n",
            "‚úÖ MAE checkpoint loaded in 0.23 seconds.\n",
            "üìä MAE trained for 50 epochs\n",
            "‚úÖ Found model state dictionary in checkpoint.\n",
            "Filtering MAE state dictionary for encoder weights...\n",
            "üìä Extracted 78 encoder parameters from MAE in 0.00 seconds.\n",
            "üéâ MAE encoder weights loaded successfully!\n",
            "‚úÖ TRAINING_CONFIG updated for MAE pretraining.\n",
            "\n",
            "üß™ Testing model creation...\n",
            "Using model_name: vit_small_patch16_224\n",
            "Using MAE weights for test model: True\n",
            "Using ImageNet pretrained for test model: False\n",
            "üèóÔ∏è Creating ViT model: vit_small_patch16_224\n",
            "Using ImageNet pretrained weights: False\n",
            "‚úÖ ViT model created in 0.34 seconds.\n",
            "‚ö° Initializing ViT backbone with MAE encoder weights...\n",
            "‚úÖ Successfully transferred 0 MAE encoder weights in 0.00 seconds.\n",
            "üéØ ViT model initialized with MAE-learned features!\n",
            "Moving test model to device: cuda\n",
            "‚úÖ Model test successful!\n",
            "üìä Input shape: torch.Size([1, 3, 224, 224])\n",
            "üìä Output shape: torch.Size([1, 37])\n",
            "üéØ Model ready for training!\n",
            "\n",
            "============================================================\n",
            "‚úÖ MAE INITIALIZATION SETUP COMPLETE!\n",
            "ü§ñ MAE pretrained: True\n",
            "üåê ImageNet pretrained: False\n",
            "üìä Model: vit_small_patch16_224 with 37 classes\n",
            "üéâ Your model will start with MAE-learned features specific to fish images!\n",
            "üöÄ This should lead to faster training and better performance!\n",
            "üéØ Ready to proceed to training!\n"
          ]
        }
      ],
      "source": [
        "# Load MAE Pre-trained Model and Create Custom ViT Model\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from model import ViTForFishClassification\n",
        "import time # Import time for basic profiling\n",
        "\n",
        "print(\"ü§ñ SETTING UP MAE-INITIALIZED ViT MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Configuration for MAE loading\n",
        "MAE_MODEL_PATH = '/content/drive/MyDrive/mae_checkpoints/mae_final_model.pth'  # Update this path if needed\n",
        "LOAD_MAE_PRETRAINED = True  # Set to False to skip MAE loading\n",
        "\n",
        "# Global variable to store MAE state for later use\n",
        "MAE_ENCODER_WEIGHTS = None\n",
        "print(f\"Configured MAE_MODEL_PATH: {MAE_MODEL_PATH}\")\n",
        "print(f\"Configured LOAD_MAE_PRETRAINED: {LOAD_MAE_PRETRAINED}\")\n",
        "\n",
        "\n",
        "def load_mae_encoder_weights(mae_checkpoint_path):\n",
        "    \"\"\"\n",
        "    Load and extract encoder weights from MAE checkpoint.\n",
        "\n",
        "    Args:\n",
        "        mae_checkpoint_path: Path to MAE checkpoint file\n",
        "\n",
        "    Returns:\n",
        "        dict: Filtered encoder weights compatible with ViT backbone\n",
        "    \"\"\"\n",
        "    print(f\"üì• Loading MAE checkpoint from: {mae_checkpoint_path}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # Load MAE checkpoint\n",
        "        # Use map_location='cpu' first, then move to GPU if needed later\n",
        "        # Added weights_only=False based on error message\n",
        "        checkpoint = torch.load(mae_checkpoint_path, map_location='cpu', weights_only=False)\n",
        "        print(f\"‚úÖ MAE checkpoint loaded in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        # Print checkpoint info\n",
        "        if 'epoch' in checkpoint:\n",
        "            print(f\"üìä MAE trained for {checkpoint['epoch']} epochs\")\n",
        "        if 'train_loss' in checkpoint:\n",
        "            print(f\"üìâ Final MAE loss: {checkpoint['train_loss']:.4f}\")\n",
        "        if 'model_state_dict' in checkpoint or 'state_dict' in checkpoint:\n",
        "             print(\"‚úÖ Found model state dictionary in checkpoint.\")\n",
        "        else:\n",
        "             print(\"‚ö†Ô∏è Could not find 'model_state_dict' or 'state_dict' in checkpoint.\")\n",
        "\n",
        "\n",
        "        # Get model state dict\n",
        "        mae_state_dict = checkpoint.get('model_state_dict', checkpoint.get('state_dict', None))\n",
        "\n",
        "        if mae_state_dict is None:\n",
        "             print(\"‚ùå MAE state dictionary not found in checkpoint.\")\n",
        "             return None\n",
        "\n",
        "        # Filter encoder weights (remove decoder, mask token, and other non-encoder components)\n",
        "        encoder_weights = {}\n",
        "        filter_prefixes = ['patch_embed', 'pos_embed', 'cls_token', 'blocks', 'norm']\n",
        "        exclude_substrings = ['decoder', 'mask_token', 'head']\n",
        "\n",
        "        print(\"Filtering MAE state dictionary for encoder weights...\")\n",
        "        start_time = time.time()\n",
        "        for key, value in mae_state_dict.items():\n",
        "            # Keep only encoder-related weights\n",
        "            if any(prefix in key for prefix in filter_prefixes) and not any(exclude in key for exclude in exclude_substrings):\n",
        "                encoder_weights[key] = value\n",
        "\n",
        "        print(f\"üìä Extracted {len(encoder_weights)} encoder parameters from MAE in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        if not encoder_weights:\n",
        "             print(\"‚ö†Ô∏è No encoder weights were extracted. Check filter logic or checkpoint structure.\")\n",
        "\n",
        "\n",
        "        return encoder_weights\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading or processing MAE checkpoint: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_mae_initialized_model(num_classes, model_name='vit_small_patch16_224', mae_weights=None):\n",
        "    \"\"\"\n",
        "    Create ViT model and optionally initialize with MAE weights.\n",
        "\n",
        "    Args:\n",
        "        num_classes: Number of classification classes\n",
        "        model_name: ViT model architecture name\n",
        "        mae_weights: Optional MAE encoder weights dictionary\n",
        "\n",
        "    Returns:\n",
        "        ViTForFishClassification: Initialized model\n",
        "    \"\"\"\n",
        "    print(f\"üèóÔ∏è Creating ViT model: {model_name}\")\n",
        "\n",
        "    # Create ViT model (without ImageNet pretraining if we have MAE weights)\n",
        "    use_imagenet_pretrained = mae_weights is None\n",
        "    print(f\"Using ImageNet pretrained weights: {use_imagenet_pretrained}\")\n",
        "    start_time = time.time()\n",
        "    model = ViTForFishClassification(\n",
        "        num_classes=num_classes,\n",
        "        model_name=model_name,\n",
        "        pretrained=use_imagenet_pretrained,\n",
        "        dropout_rate=0.1\n",
        "    )\n",
        "    print(f\"‚úÖ ViT model created in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    if mae_weights is not None:\n",
        "        print(\"‚ö° Initializing ViT backbone with MAE encoder weights...\")\n",
        "        start_time = time.time()\n",
        "        # Get current backbone state dict\n",
        "        backbone_state = model.backbone.state_dict()\n",
        "\n",
        "        # Update with MAE weights (only for matching keys and shapes)\n",
        "        updated_keys = []\n",
        "        shape_mismatches = []\n",
        "\n",
        "        for mae_key, mae_weight in mae_weights.items():\n",
        "            if mae_key in backbone_state:\n",
        "                if mae_weight.shape == backbone_state[mae_key].shape:\n",
        "                    backbone_state[mae_key] = mae_weight.clone()\n",
        "                    updated_keys.append(mae_key)\n",
        "                else:\n",
        "                    shape_mismatches.append(f\"{mae_key}: MAE{mae_weight.shape} != ViT{backbone_state[mae_key].shape}\")\n",
        "\n",
        "        # Load updated weights\n",
        "        try:\n",
        "            model.backbone.load_state_dict(backbone_state)\n",
        "            print(f\"‚úÖ Successfully transferred {len(updated_keys)} MAE encoder weights in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "            if shape_mismatches:\n",
        "                print(f\"‚ö†Ô∏è Found {len(shape_mismatches)} shape mismatches (using original weights for these):\")\n",
        "                for mismatch in shape_mismatches[:5]:  # Show first 5 mismatches\n",
        "                    print(f\"   {mismatch}\")\n",
        "\n",
        "            print(\"üéØ ViT model initialized with MAE-learned features!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading MAE weights into ViT backbone: {e}\")\n",
        "            print(\"Continuing with potentially partially loaded weights or default ImageNet (if applicable).\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"üåê Using ImageNet pretrained weights\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Main execution\n",
        "if LOAD_MAE_PRETRAINED:\n",
        "    print(f\"Attempting to load MAE pretrained model from: {MAE_MODEL_PATH}\")\n",
        "    # Check if MAE model exists in Google Drive\n",
        "    if os.path.exists(MAE_MODEL_PATH):\n",
        "        print(f\"‚úÖ Found MAE model: {os.path.basename(MAE_MODEL_PATH)}\")\n",
        "        try:\n",
        "            file_size = os.path.getsize(MAE_MODEL_PATH) / (1024**2)\n",
        "            print(f\"üìè Size: {file_size:.1f} MB\")\n",
        "        except Exception as e:\n",
        "             print(f\"‚ö†Ô∏è Could not get file size: {e}\")\n",
        "\n",
        "\n",
        "        try:\n",
        "            # Load MAE encoder weights\n",
        "            MAE_ENCODER_WEIGHTS = load_mae_encoder_weights(MAE_MODEL_PATH)\n",
        "\n",
        "            if MAE_ENCODER_WEIGHTS is not None:\n",
        "                 print(\"üéâ MAE encoder weights loaded successfully!\")\n",
        "\n",
        "                 # Update training config\n",
        "                 if 'TRAINING_CONFIG' in globals():\n",
        "                    TRAINING_CONFIG['mae_pretrained'] = True\n",
        "                    TRAINING_CONFIG['mae_model_path'] = MAE_MODEL_PATH\n",
        "                    TRAINING_CONFIG['pretrained'] = False  # Don't use ImageNet since we have MAE\n",
        "                    print(\"‚úÖ TRAINING_CONFIG updated for MAE pretraining.\")\n",
        "                 else:\n",
        "                    print(\"‚ö†Ô∏è TRAINING_CONFIG not found. Cannot update config with MAE settings.\")\n",
        "\n",
        "            else:\n",
        "                 print(\"‚ùå Failed to load MAE encoder weights. MAE_ENCODER_WEIGHTS is None.\")\n",
        "                 print(\"üîÑ Falling back to ImageNet pretrained weights...\")\n",
        "                 MAE_ENCODER_WEIGHTS = None\n",
        "                 if 'TRAINING_CONFIG' in globals():\n",
        "                    TRAINING_CONFIG['mae_pretrained'] = False\n",
        "                    TRAINING_CONFIG['pretrained'] = True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during MAE loading process: {e}\")\n",
        "            print(\"üîÑ Falling back to ImageNet pretrained weights...\")\n",
        "            MAE_ENCODER_WEIGHTS = None\n",
        "            if 'TRAINING_CONFIG' in globals():\n",
        "                TRAINING_CONFIG['mae_pretrained'] = False\n",
        "                TRAINING_CONFIG['pretrained'] = True\n",
        "\n",
        "\n",
        "    else:\n",
        "        # MAE model not found, check alternative locations\n",
        "        print(f\"‚ùå MAE model not found at: {MAE_MODEL_PATH}\")\n",
        "\n",
        "        # Try to copy from local mae_checkpoints if exists\n",
        "        local_mae_path = f'/content/ViT-FishID/mae_checkpoints/{os.path.basename(MAE_MODEL_PATH)}'\n",
        "        print(f\"Checking local path: {local_mae_path}\")\n",
        "        if os.path.exists(local_mae_path):\n",
        "            print(f\"ÔøΩ Found MAE model in local repository: {local_mae_path}\")\n",
        "            try:\n",
        "                # Create directory and copy\n",
        "                print(f\"Attempting to create directory: {os.path.dirname(MAE_MODEL_PATH)}\")\n",
        "                os.makedirs(os.path.dirname(MAE_MODEL_PATH), exist_ok=True)\n",
        "                print(f\"Copying from {local_mae_path} to {MAE_MODEL_PATH}\")\n",
        "                shutil.copy2(local_mae_path, MAE_MODEL_PATH)\n",
        "                print(f\"‚úÖ Copied MAE model to Google Drive: {MAE_MODEL_PATH}\")\n",
        "\n",
        "                # Now load it\n",
        "                MAE_ENCODER_WEIGHTS = load_mae_encoder_weights(MAE_MODEL_PATH)\n",
        "                if MAE_ENCODER_WEIGHTS is not None:\n",
        "                    if 'TRAINING_CONFIG' in globals():\n",
        "                        TRAINING_CONFIG['mae_pretrained'] = True\n",
        "                        TRAINING_CONFIG['mae_model_path'] = MAE_MODEL_PATH\n",
        "                        TRAINING_CONFIG['pretrained'] = False\n",
        "                else:\n",
        "                     print(\"‚ùå Failed to load MAE encoder weights after copying.\")\n",
        "                     MAE_ENCODER_WEIGHTS = None\n",
        "                     if 'TRAINING_CONFIG' in globals():\n",
        "                        TRAINING_CONFIG['mae_pretrained'] = False\n",
        "                        TRAINING_CONFIG['pretrained'] = True\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error copying/loading MAE model from local path: {e}\")\n",
        "                MAE_ENCODER_WEIGHTS = None\n",
        "                if 'TRAINING_CONFIG' in globals():\n",
        "                    TRAINING_CONFIG['mae_pretrained'] = False\n",
        "                    TRAINING_CONFIG['pretrained'] = True\n",
        "        else:\n",
        "            print(\"ÔøΩüìù MAE model not found in local repository either.\")\n",
        "            print(\"ÔøΩüìù Available options:\")\n",
        "            print(\"1. Upload mae_final_model.pth or mae_best_model.pth to /content/drive/MyDrive/mae_checkpoints/\")\n",
        "            print(\"2. Update MAE_MODEL_PATH variable to correct location\")\n",
        "            print(\"3. Set LOAD_MAE_PRETRAINED = False to use ImageNet weights\")\n",
        "            print(\"üîÑ Continuing with ImageNet pretrained weights...\")\n",
        "            MAE_ENCODER_WEIGHTS = None\n",
        "            if 'TRAINING_CONFIG' in globals():\n",
        "                TRAINING_CONFIG['mae_pretrained'] = False\n",
        "                TRAINING_CONFIG['pretrained'] = True\n",
        "\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è LOAD_MAE_PRETRAINED is False. Skipping MAE loading - will use ImageNet pretrained weights\")\n",
        "    MAE_ENCODER_WEIGHTS = None\n",
        "    if 'TRAINING_CONFIG' in globals():\n",
        "        TRAINING_CONFIG['mae_pretrained'] = False\n",
        "        TRAINING_CONFIG['pretrained'] = True\n",
        "\n",
        "\n",
        "# Test model creation (optional - this creates a model to verify everything works)\n",
        "print(f\"\\nüß™ Testing model creation...\")\n",
        "if 'NUM_CLASSES' not in globals():\n",
        "    print(\"‚ö†Ô∏è NUM_CLASSES not defined. Skipping model creation test.\")\n",
        "else:\n",
        "    try:\n",
        "        # Ensure model name and pretrained flag are correctly picked up from TRAINING_CONFIG\n",
        "        model_name_for_test = TRAINING_CONFIG.get('model_name', 'vit_small_patch16_224')\n",
        "        use_imagenet_for_test = TRAINING_CONFIG.get('pretrained', True) # Use the updated flag\n",
        "\n",
        "        # If MAE weights were loaded, pass them, otherwise rely on TRAINING_CONFIG['pretrained']\n",
        "        weights_for_test = MAE_ENCODER_WEIGHTS if MAE_ENCODER_WEIGHTS is not None else None\n",
        "\n",
        "        print(f\"Using model_name: {model_name_for_test}\")\n",
        "        print(f\"Using MAE weights for test model: {weights_for_test is not None}\")\n",
        "        print(f\"Using ImageNet pretrained for test model: {use_imagenet_for_test}\")\n",
        "\n",
        "        test_model = create_mae_initialized_model(\n",
        "            num_classes=NUM_CLASSES,\n",
        "            model_name=model_name_for_test,\n",
        "            mae_weights=weights_for_test # Pass MAE weights if available\n",
        "        )\n",
        "\n",
        "        # Move model to device for testing (optional but good practice)\n",
        "        # Assuming DEVICE is defined globally from Step 1\n",
        "        if 'DEVICE' in globals():\n",
        "            print(f\"Moving test model to device: {DEVICE}\")\n",
        "            test_model.to(DEVICE)\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è DEVICE variable not found. Skipping moving test model to device.\")\n",
        "\n",
        "\n",
        "        # Test forward pass\n",
        "        test_input = torch.randn(1, 3, 224, 224)\n",
        "        # Move test input to the same device as the model\n",
        "        if 'DEVICE' in globals():\n",
        "            test_input = test_input.to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_output = test_model(test_input)\n",
        "\n",
        "        print(f\"‚úÖ Model test successful!\")\n",
        "        print(f\"üìä Input shape: {test_input.shape}\")\n",
        "        print(f\"üìä Output shape: {test_output.shape}\")\n",
        "        print(f\"üéØ Model ready for training!\")\n",
        "\n",
        "        # Clean up test model\n",
        "        del test_model, test_input, test_output\n",
        "        if 'DEVICE' in globals():\n",
        "             torch.cuda.empty_cache() # Clear GPU cache after test if on GPU\n",
        "        import gc\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Model test failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Print full traceback for debugging\n",
        "\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(f\"‚úÖ MAE INITIALIZATION SETUP COMPLETE!\")\n",
        "if 'TRAINING_CONFIG' in globals():\n",
        "    print(f\"ü§ñ MAE pretrained: {TRAINING_CONFIG.get('mae_pretrained', False)}\")\n",
        "    print(f\"üåê ImageNet pretrained: {TRAINING_CONFIG.get('pretrained', True)}\")\n",
        "    print(f\"üìä Model: {TRAINING_CONFIG.get('model_name', 'N/A')} with {TRAINING_CONFIG.get('num_classes', 'N/A')} classes\")\n",
        "\n",
        "    if TRAINING_CONFIG.get('mae_pretrained', False):\n",
        "        print(\"üéâ Your model will start with MAE-learned features specific to fish images!\")\n",
        "        print(\"üöÄ This should lead to faster training and better performance!\")\n",
        "    else:\n",
        "        print(\"üåê Your model will use standard ImageNet pretrained features.\")\n",
        "else:\n",
        "     print(\"‚ö†Ô∏è TRAINING_CONFIG was not found, cannot provide detailed summary.\")\n",
        "\n",
        "\n",
        "print(\"üéØ Ready to proceed to training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa9cbd50",
      "metadata": {
        "id": "aa9cbd50"
      },
      "source": [
        "## üöÄ Step 8: Start Semi-Supervised Training\n",
        "\n",
        "This cell will start the complete training process. Expected time: 4-6 hours for 100 epochs.\n",
        "\n",
        "**Training Process:**\n",
        "1. **Supervised Learning**: Uses labeled fish images with ground truth\n",
        "2. **Semi-Supervised Learning**: Leverages unlabeled images with pseudo-labels\n",
        "3. **EMA Teacher-Student**: Uses exponential moving average for consistency\n",
        "4. **Automatic Checkpointing**: Saves progress every 10 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "eC34ujegJAf8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC34ujegJAf8",
        "outputId": "9dfe2769-68e5-4482-d857-888deed951cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ STARTING SEMI-SUPERVISED FISH CLASSIFICATION TRAINING\n",
            "============================================================\n",
            "/content/ViT-FishID\n",
            "üîç Found existing checkpoints. Latest: Epoch 10\n",
            "üÜï Starting fresh training from epoch 1\n",
            "ü§ñ Preparing MAE-enhanced training script...\n",
            "üìã TRAINING CONFIGURATION:\n",
            "============================================================\n",
            "üéØ Training 37 fish species\n",
            "üìä Mode: semi_supervised\n",
            "ü§ñ MAE pretrained: True\n",
            "üåê ImageNet pretrained: False\n",
            "üÜï Starting fresh training\n",
            "‚è±Ô∏è Estimated time: 5.0 hours\n",
            "üíæ Checkpoints: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints\n",
            "üìà W&B logging: False\n",
            "üéâ Using MAE-learned features from: mae_final_model.pth\n",
            "üöÄ This should significantly improve training performance!\n",
            "\n",
            "üé¨ TRAINING STARTED\n",
            "‚è∞ Started at: 2025-08-18 12:25:15\n",
            "2025-08-18 12:25:23.021199: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755519923.055638   50826 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755519923.065726   50826 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755519923.090092   50826 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755519923.090123   50826 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755519923.090129   50826 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755519923.090132   50826 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "‚úÖ Loaded configuration from arguments.\n",
            "üìä Configured num_classes: 37\n",
            "üÜï Training from scratch based on arguments.\n",
            "Using GPU: Tesla T4\n",
            "Random seed set to 42\n",
            "ü§ñ Creating model for training...\n",
            "Attempting to load MAE pretrained model from: /content/drive/MyDrive/mae_checkpoints/mae_final_model.pth\n",
            "üì• Loading MAE checkpoint from: /content/drive/MyDrive/mae_checkpoints/mae_final_model.pth\n",
            "‚úÖ MAE checkpoint loaded.\n",
            "Filtering MAE state dictionary for encoder weights...\n",
            "üìä Extracted 78 encoder parameters from MAE.\n",
            "üéâ MAE encoder weights loaded successfully within script!\n",
            "Debug: Checking parameter name compatibility...\n",
            "First 5 MAE parameter names: ['encoder.cls_token', 'encoder.pos_embed', 'encoder.patch_embed.proj.weight', 'encoder.patch_embed.proj.bias', 'encoder.blocks.0.norm1.weight']\n",
            "First 5 ViT backbone parameter names: ['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.norm1.weight']\n",
            "‚úÖ Mapped encoder.cls_token -> cls_token\n",
            "‚úÖ Mapped encoder.pos_embed -> pos_embed\n",
            "‚úÖ Mapped encoder.patch_embed.proj.weight -> patch_embed.proj.weight\n",
            "‚úÖ Mapped encoder.patch_embed.proj.bias -> patch_embed.proj.bias\n",
            "‚úÖ Mapped encoder.blocks.0.norm1.weight -> blocks.0.norm1.weight\n",
            "‚úÖ Mapped encoder.blocks.0.norm1.bias -> blocks.0.norm1.bias\n",
            "‚úÖ Mapped encoder.blocks.0.attn.qkv.weight -> blocks.0.attn.qkv.weight\n",
            "‚úÖ Mapped encoder.blocks.0.attn.qkv.bias -> blocks.0.attn.qkv.bias\n",
            "‚úÖ Mapped encoder.blocks.0.attn.proj.weight -> blocks.0.attn.proj.weight\n",
            "‚úÖ Mapped encoder.blocks.0.attn.proj.bias -> blocks.0.attn.proj.bias\n",
            "‚úÖ Mapped encoder.blocks.0.norm2.weight -> blocks.0.norm2.weight\n",
            "‚úÖ Mapped encoder.blocks.0.norm2.bias -> blocks.0.norm2.bias\n",
            "‚úÖ Mapped encoder.blocks.0.mlp.fc1.weight -> blocks.0.mlp.fc1.weight\n",
            "‚úÖ Mapped encoder.blocks.0.mlp.fc1.bias -> blocks.0.mlp.fc1.bias\n",
            "‚úÖ Mapped encoder.blocks.0.mlp.fc2.weight -> blocks.0.mlp.fc2.weight\n",
            "‚úÖ Mapped encoder.blocks.0.mlp.fc2.bias -> blocks.0.mlp.fc2.bias\n",
            "‚úÖ Mapped encoder.blocks.1.norm1.weight -> blocks.1.norm1.weight\n",
            "‚úÖ Mapped encoder.blocks.1.norm1.bias -> blocks.1.norm1.bias\n",
            "‚úÖ Mapped encoder.blocks.1.attn.qkv.weight -> blocks.1.attn.qkv.weight\n",
            "‚úÖ Mapped encoder.blocks.1.attn.qkv.bias -> blocks.1.attn.qkv.bias\n",
            "‚úÖ Mapped encoder.blocks.1.attn.proj.weight -> blocks.1.attn.proj.weight\n",
            "‚úÖ Mapped encoder.blocks.1.attn.proj.bias -> blocks.1.attn.proj.bias\n",
            "‚úÖ Mapped encoder.blocks.1.norm2.weight -> blocks.1.norm2.weight\n",
            "‚úÖ Mapped encoder.blocks.1.norm2.bias -> blocks.1.norm2.bias\n",
            "‚úÖ Mapped encoder.blocks.1.mlp.fc1.weight -> blocks.1.mlp.fc1.weight\n",
            "‚úÖ Mapped encoder.blocks.1.mlp.fc1.bias -> blocks.1.mlp.fc1.bias\n",
            "‚úÖ Mapped encoder.blocks.1.mlp.fc2.weight -> blocks.1.mlp.fc2.weight\n",
            "‚úÖ Mapped encoder.blocks.1.mlp.fc2.bias -> blocks.1.mlp.fc2.bias\n",
            "‚úÖ Mapped encoder.blocks.2.norm1.weight -> blocks.2.norm1.weight\n",
            "‚úÖ Mapped encoder.blocks.2.norm1.bias -> blocks.2.norm1.bias\n",
            "‚úÖ Mapped encoder.blocks.2.attn.qkv.weight -> blocks.2.attn.qkv.weight\n",
            "‚úÖ Mapped encoder.blocks.2.attn.qkv.bias -> blocks.2.attn.qkv.bias\n",
            "‚úÖ Mapped encoder.blocks.2.attn.proj.weight -> blocks.2.attn.proj.weight\n",
            "‚úÖ Mapped encoder.blocks.2.attn.proj.bias -> blocks.2.attn.proj.bias\n",
            "‚úÖ Mapped encoder.blocks.2.norm2.weight -> blocks.2.norm2.weight\n",
            "‚úÖ Mapped encoder.blocks.2.norm2.bias -> blocks.2.norm2.bias\n",
            "‚úÖ Mapped encoder.blocks.2.mlp.fc1.weight -> blocks.2.mlp.fc1.weight\n",
            "‚úÖ Mapped encoder.blocks.2.mlp.fc1.bias -> blocks.2.mlp.fc1.bias\n",
            "‚úÖ Mapped encoder.blocks.2.mlp.fc2.weight -> blocks.2.mlp.fc2.weight\n",
            "‚úÖ Mapped encoder.blocks.2.mlp.fc2.bias -> blocks.2.mlp.fc2.bias\n",
            "‚úÖ Mapped encoder.blocks.3.norm1.weight -> blocks.3.norm1.weight\n",
            "‚úÖ Mapped encoder.blocks.3.norm1.bias -> blocks.3.norm1.bias\n",
            "‚úÖ Mapped encoder.blocks.3.attn.qkv.weight -> blocks.3.attn.qkv.weight\n",
            "‚úÖ Mapped encoder.blocks.3.attn.qkv.bias -> blocks.3.attn.qkv.bias\n",
            "‚úÖ Mapped encoder.blocks.3.attn.proj.weight -> blocks.3.attn.proj.weight\n",
            "‚úÖ Mapped encoder.blocks.3.attn.proj.bias -> blocks.3.attn.proj.bias\n",
            "‚úÖ Mapped encoder.blocks.3.norm2.weight -> blocks.3.norm2.weight\n",
            "‚úÖ Mapped encoder.blocks.3.norm2.bias -> blocks.3.norm2.bias\n",
            "‚úÖ Mapped encoder.blocks.3.mlp.fc1.weight -> blocks.3.mlp.fc1.weight\n",
            "‚úÖ Mapped encoder.blocks.3.mlp.fc1.bias -> blocks.3.mlp.fc1.bias\n",
            "‚úÖ Mapped encoder.blocks.3.mlp.fc2.weight -> blocks.3.mlp.fc2.weight\n",
            "‚úÖ Mapped encoder.blocks.3.mlp.fc2.bias -> blocks.3.mlp.fc2.bias\n",
            "‚úÖ Mapped encoder.blocks.4.norm1.weight -> blocks.4.norm1.weight\n",
            "‚úÖ Mapped encoder.blocks.4.norm1.bias -> blocks.4.norm1.bias\n",
            "‚úÖ Mapped encoder.blocks.4.attn.qkv.weight -> blocks.4.attn.qkv.weight\n",
            "‚úÖ Mapped encoder.blocks.4.attn.qkv.bias -> blocks.4.attn.qkv.bias\n",
            "‚úÖ Mapped encoder.blocks.4.attn.proj.weight -> blocks.4.attn.proj.weight\n",
            "‚úÖ Mapped encoder.blocks.4.attn.proj.bias -> blocks.4.attn.proj.bias\n",
            "‚úÖ Mapped encoder.blocks.4.norm2.weight -> blocks.4.norm2.weight\n",
            "‚úÖ Mapped encoder.blocks.4.norm2.bias -> blocks.4.norm2.bias\n",
            "‚úÖ Mapped encoder.blocks.4.mlp.fc1.weight -> blocks.4.mlp.fc1.weight\n",
            "‚úÖ Mapped encoder.blocks.4.mlp.fc1.bias -> blocks.4.mlp.fc1.bias\n",
            "‚úÖ Mapped encoder.blocks.4.mlp.fc2.weight -> blocks.4.mlp.fc2.weight\n",
            "‚úÖ Mapped encoder.blocks.4.mlp.fc2.bias -> blocks.4.mlp.fc2.bias\n",
            "‚úÖ Mapped encoder.blocks.5.norm1.weight -> blocks.5.norm1.weight\n",
            "‚úÖ Mapped encoder.blocks.5.norm1.bias -> blocks.5.norm1.bias\n",
            "‚úÖ Mapped encoder.blocks.5.attn.qkv.weight -> blocks.5.attn.qkv.weight\n",
            "‚úÖ Mapped encoder.blocks.5.attn.qkv.bias -> blocks.5.attn.qkv.bias\n",
            "‚úÖ Mapped encoder.blocks.5.attn.proj.weight -> blocks.5.attn.proj.weight\n",
            "‚úÖ Mapped encoder.blocks.5.attn.proj.bias -> blocks.5.attn.proj.bias\n",
            "‚úÖ Mapped encoder.blocks.5.norm2.weight -> blocks.5.norm2.weight\n",
            "‚úÖ Mapped encoder.blocks.5.norm2.bias -> blocks.5.norm2.bias\n",
            "‚úÖ Mapped encoder.blocks.5.mlp.fc1.weight -> blocks.5.mlp.fc1.weight\n",
            "‚úÖ Mapped encoder.blocks.5.mlp.fc1.bias -> blocks.5.mlp.fc1.bias\n",
            "‚úÖ Mapped encoder.blocks.5.mlp.fc2.weight -> blocks.5.mlp.fc2.weight\n",
            "‚úÖ Mapped encoder.blocks.5.mlp.fc2.bias -> blocks.5.mlp.fc2.bias\n",
            "‚úÖ Mapped encoder.norm.weight -> norm.weight\n",
            "‚úÖ Mapped encoder.norm.bias -> norm.bias\n",
            "‚úÖ Loaded 78 MAE encoder weights into model backbone state dict.\n",
            "üéâ MAE pretraining weights successfully loaded!\n",
            "üìä Weight mapping summary:\n",
            "  1. encoder.cls_token -> cls_token\n",
            "  2. encoder.pos_embed -> pos_embed\n",
            "  3. encoder.patch_embed.proj.weight -> patch_embed.proj.weight\n",
            "  4. encoder.patch_embed.proj.bias -> patch_embed.proj.bias\n",
            "  5. encoder.blocks.0.norm1.weight -> blocks.0.norm1.weight\n",
            "  ... and 73 more\n",
            "‚úÖ Student model created and initialized with MAE weights.\n",
            "‚úÖ EMA Teacher initialized with momentum: 0.999\n",
            "üéì EMA Teacher model created and its internal model moved to device.\n",
            "Loading data...\n",
            "‚ö†Ô∏è  Warning: Some classes have only 1 sample(s). Using random splitting instead of stratified.\n",
            "   Classes with 1 sample: ['Carangidae_Caranx_heberi', 'Serranidae_Lipropoma_spp1', 'Sparidae_Sparodon_durbanesis']\n",
            "/content/ViT-FishID/data.py:229: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
            "üìä Dataset initialized:\n",
            "  - Labeled samples: 3,084\n",
            "  - Unlabeled samples: 6,168\n",
            "  - Total samples per epoch: 9,252\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "üìä Semi-supervised data loaders created:\n",
            "  - Train labeled: 3,084\n",
            "  - Train unlabeled: 6,168\n",
            "  - Val samples: 1,029\n",
            "  - Test samples: 1,029\n",
            "  - Classes: 37\n",
            "  - Split ratios: Train=60.0%, Val=20.0%, Test=20.0%\n",
            "‚úÖ Data loaders created.\n",
            "üìä Number of classes: 37\n",
            "üéØ Training mode: semi_supervised\n",
            "Setting up trainer...\n",
            "‚úÖ Semi-Supervised Trainer initialized\n",
            "  - Consistency weight: 2.0\n",
            "  - Pseudo-label threshold: 0.7\n",
            "  - Learning rate: 0.0001\n",
            "  - Warmup epochs: 10\n",
            "  - Ramp-up epochs: 30\n",
            "‚úÖ SemiSupervisedTrainer created.\n",
            "üìä W&B logging disabled.\n",
            "üöÄ Starting training from epoch 1\n",
            "\n",
            "üìÖ Epoch 1/100\n",
            "Epoch 1: 100% 578/578 [01:59<00:00,  4.83it/s, Total=2.7041, Sup=2.7088, Cons=0.0008, L-Acc=20.8%, P-Acc=0.0%]\n",
            "üìä Epoch 1 - Train Loss: {'supervised_loss': 2.708761729442061, 'consistency_loss': 0.0007817315838337104, 'total_loss': 2.7040752904637873, 'labeled_accuracy': 20.81712033683688, 'pseudo_accuracy': 0, 'high_conf_pseudo_labels': 0, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 26.43343051506317% (Best: 26.43343051506317%)\n",
            "üíæ Saving checkpoint for epoch 1...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 2/100\n",
            "Epoch 2: 100% 578/578 [01:59<00:00,  4.83it/s, Total=2.4986, Sup=2.4986, Cons=0.0006, L-Acc=24.9%, P-Acc=0.0%]\n",
            "üìä Epoch 2 - Train Loss: {'supervised_loss': 2.4985951292473554, 'consistency_loss': 0.0005951608508719762, 'total_loss': 2.4985951292473554, 'labeled_accuracy': 24.90272347225135, 'pseudo_accuracy': 0, 'high_conf_pseudo_labels': 0, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 26.62779397473275% (Best: 26.62779397473275%)\n",
            "üíæ Saving checkpoint for epoch 2...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 3/100\n",
            "Epoch 3: 100% 578/578 [02:00<00:00,  4.79it/s, Total=2.4123, Sup=2.4165, Cons=0.0003, L-Acc=27.1%, P-Acc=0.0%]\n",
            "üìä Epoch 3 - Train Loss: {'supervised_loss': 2.4164792443891954, 'consistency_loss': 0.0003235533612556304, 'total_loss': 2.4122984844508055, 'labeled_accuracy': 27.060350130317584, 'pseudo_accuracy': 0, 'high_conf_pseudo_labels': 0, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 30.515063168124392% (Best: 30.515063168124392%)\n",
            "üíæ Saving checkpoint for epoch 3...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 4/100\n",
            "Epoch 4: 100% 578/578 [01:59<00:00,  4.84it/s, Total=2.3592, Sup=2.3592, Cons=0.0003, L-Acc=29.2%, P-Acc=0.0%]\n",
            "üìä Epoch 4 - Train Loss: {'supervised_loss': 2.3592205960239094, 'consistency_loss': 0.00026518656253580254, 'total_loss': 2.3592205960239094, 'labeled_accuracy': 29.178837765969458, 'pseudo_accuracy': 0, 'high_conf_pseudo_labels': 0, 'labeled_samples': 3081, 'unlabeled_samples': 6167}, Val Acc: 32.16715257531584% (Best: 32.16715257531584%)\n",
            "üíæ Saving checkpoint for epoch 4...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 5/100\n",
            "Epoch 5: 100% 578/578 [01:59<00:00,  4.84it/s, Total=2.2789, Sup=2.2868, Cons=0.0002, L-Acc=30.1%, P-Acc=0.0%]\n",
            "üìä Epoch 5 - Train Loss: {'supervised_loss': 2.286816844923629, 'consistency_loss': 0.00024238573468081205, 'total_loss': 2.2789039838685294, 'labeled_accuracy': 30.11031767333351, 'pseudo_accuracy': 0, 'high_conf_pseudo_labels': 0, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 29.54324586977648% (Best: 32.16715257531584%)\n",
            "\n",
            "üìÖ Epoch 6/100\n",
            "Epoch 6: 100% 578/578 [01:58<00:00,  4.86it/s, Total=2.2760, Sup=2.2799, Cons=0.0002, L-Acc=30.6%, P-Acc=100.0%]\n",
            "üìä Epoch 6 - Train Loss: {'supervised_loss': 2.2799456179451902, 'consistency_loss': 0.0002291943403311351, 'total_loss': 2.2760010753535895, 'labeled_accuracy': 30.619526120389082, 'pseudo_accuracy': 100.0, 'high_conf_pseudo_labels': 4, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 32.26433430515063% (Best: 32.26433430515063%)\n",
            "üíæ Saving checkpoint for epoch 6...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 7/100\n",
            "Epoch 7: 100% 578/578 [02:00<00:00,  4.81it/s, Total=2.2433, Sup=2.2471, Cons=0.0002, L-Acc=32.1%, P-Acc=90.9%]\n",
            "üìä Epoch 7 - Train Loss: {'supervised_loss': 2.2471439761966727, 'consistency_loss': 0.0002436993062597092, 'total_loss': 2.2432561838503116, 'labeled_accuracy': 32.101167005632945, 'pseudo_accuracy': 90.9090909090909, 'high_conf_pseudo_labels': 11, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 36.054421768707485% (Best: 36.054421768707485%)\n",
            "üíæ Saving checkpoint for epoch 7...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 8/100\n",
            "Epoch 8: 100% 578/578 [01:59<00:00,  4.83it/s, Total=2.2288, Sup=2.2288, Cons=0.0003, L-Acc=32.2%, P-Acc=97.6%]\n",
            "üìä Epoch 8 - Train Loss: {'supervised_loss': 2.2288140090485347, 'consistency_loss': 0.0002590109336914601, 'total_loss': 2.2288140090485347, 'labeled_accuracy': 32.22979516342297, 'pseudo_accuracy': 97.5609756097561, 'high_conf_pseudo_labels': 41, 'labeled_samples': 3081, 'unlabeled_samples': 6167}, Val Acc: 34.11078717201166% (Best: 36.054421768707485%)\n",
            "\n",
            "üìÖ Epoch 9/100\n",
            "Epoch 9: 100% 578/578 [01:58<00:00,  4.86it/s, Total=2.2145, Sup=2.2145, Cons=0.0003, L-Acc=33.0%, P-Acc=94.3%]\n",
            "üìä Epoch 9 - Train Loss: {'supervised_loss': 2.214463017287964, 'consistency_loss': 0.0002618836814813174, 'total_loss': 2.214463017287964, 'labeled_accuracy': 33.01978559831192, 'pseudo_accuracy': 94.28571428571429, 'high_conf_pseudo_labels': 70, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 37.12342079689019% (Best: 37.12342079689019%)\n",
            "üíæ Saving checkpoint for epoch 9...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 10/100\n",
            "Epoch 10: 100% 578/578 [01:59<00:00,  4.83it/s, Total=2.2173, Sup=2.2173, Cons=0.0003, L-Acc=33.2%, P-Acc=93.9%]\n",
            "üìä Epoch 10 - Train Loss: {'supervised_loss': 2.2173362809481505, 'consistency_loss': 0.00029768669276069654, 'total_loss': 2.2173362809481505, 'labeled_accuracy': 33.236056817954, 'pseudo_accuracy': 93.8596490558825, 'high_conf_pseudo_labels': 114, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 37.12342079689019% (Best: 37.12342079689019%)\n",
            "üíæ Saving checkpoint for epoch 10...\n",
            "‚úÖ Saved checkpoint: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/checkpoint_epoch_10.pth\n",
            "\n",
            "üìÖ Epoch 11/100\n",
            "Epoch 11: 100% 578/578 [02:03<00:00,  4.68it/s, Total=2.1594, Sup=2.1594, Cons=0.0003, L-Acc=34.7%, P-Acc=98.2%]\n",
            "üìä Epoch 11 - Train Loss: {'supervised_loss': 2.159387759168255, 'consistency_loss': 0.0002947098847285677, 'total_loss': 2.1594172294370857, 'labeled_accuracy': 34.66277528365762, 'pseudo_accuracy': 98.15950920245399, 'high_conf_pseudo_labels': 163, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 38.28960155490768% (Best: 38.28960155490768%)\n",
            "üíæ Saving checkpoint for epoch 11...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 12/100\n",
            "Epoch 12: 100% 578/578 [02:04<00:00,  4.65it/s, Total=2.1447, Sup=2.1446, Cons=0.0003, L-Acc=35.9%, P-Acc=97.6%]\n",
            "üìä Epoch 12 - Train Loss: {'supervised_loss': 2.1446115589059347, 'consistency_loss': 0.0003120685790056785, 'total_loss': 2.144673973940648, 'labeled_accuracy': 35.85334163080323, 'pseudo_accuracy': 97.57281553398059, 'high_conf_pseudo_labels': 206, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 41.10787172011662% (Best: 41.10787172011662%)\n",
            "üíæ Saving checkpoint for epoch 12...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 13/100\n",
            "Epoch 13: 100% 578/578 [02:03<00:00,  4.68it/s, Total=2.1390, Sup=2.1389, Cons=0.0003, L-Acc=36.0%, P-Acc=97.8%]\n",
            "üìä Epoch 13 - Train Loss: {'supervised_loss': 2.1389382692975567, 'consistency_loss': 0.00030325581142388825, 'total_loss': 2.1390292450115345, 'labeled_accuracy': 35.95068102854556, 'pseudo_accuracy': 97.83393501805054, 'high_conf_pseudo_labels': 277, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 40.2332361516035% (Best: 41.10787172011662%)\n",
            "\n",
            "üìÖ Epoch 14/100\n",
            "Epoch 14: 100% 578/578 [02:03<00:00,  4.69it/s, Total=2.0791, Sup=2.0898, Cons=0.0003, L-Acc=36.3%, P-Acc=95.7%]\n",
            "üìä Epoch 14 - Train Loss: {'supervised_loss': 2.089778368473053, 'consistency_loss': 0.0003308154380672642, 'total_loss': 2.0790640972619263, 'labeled_accuracy': 36.29581537070384, 'pseudo_accuracy': 95.6989247311828, 'high_conf_pseudo_labels': 279, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 36.44314868804665% (Best: 41.10787172011662%)\n",
            "\n",
            "üìÖ Epoch 15/100\n",
            "Epoch 15: 100% 578/578 [02:03<00:00,  4.69it/s, Total=2.0603, Sup=2.0601, Cons=0.0003, L-Acc=36.9%, P-Acc=95.8%]\n",
            "üìä Epoch 15 - Train Loss: {'supervised_loss': 2.0601228609629567, 'consistency_loss': 0.0003258666461629706, 'total_loss': 2.0602857953124394, 'labeled_accuracy': 36.9120983089134, 'pseudo_accuracy': 95.83333333333333, 'high_conf_pseudo_labels': 336, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 42.565597667638485% (Best: 42.565597667638485%)\n",
            "üíæ Saving checkpoint for epoch 15...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 16/100\n",
            "Epoch 16: 100% 578/578 [02:03<00:00,  4.66it/s, Total=2.0357, Sup=2.0354, Cons=0.0004, L-Acc=38.3%, P-Acc=96.9%]\n",
            "üìä Epoch 16 - Train Loss: {'supervised_loss': 2.035449730174352, 'consistency_loss': 0.000354105603175639, 'total_loss': 2.035662193354026, 'labeled_accuracy': 38.326847844550585, 'pseudo_accuracy': 96.85230024213075, 'high_conf_pseudo_labels': 413, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 36.540330417881435% (Best: 42.565597667638485%)\n",
            "\n",
            "üìÖ Epoch 17/100\n",
            "Epoch 17: 100% 578/578 [02:03<00:00,  4.69it/s, Total=2.0069, Sup=2.0067, Cons=0.0004, L-Acc=38.9%, P-Acc=96.6%]\n",
            "üìä Epoch 17 - Train Loss: {'supervised_loss': 2.0066623602668305, 'consistency_loss': 0.00036107636158351356, 'total_loss': 2.0069151145879784, 'labeled_accuracy': 38.92312642610038, 'pseudo_accuracy': 96.58848612445757, 'high_conf_pseudo_labels': 469, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 43.14868804664723% (Best: 43.14868804664723%)\n",
            "üíæ Saving checkpoint for epoch 17...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 18/100\n",
            "Epoch 18: 100% 578/578 [02:03<00:00,  4.67it/s, Total=1.9810, Sup=1.9807, Cons=0.0004, L-Acc=39.3%, P-Acc=95.6%]\n",
            "üìä Epoch 18 - Train Loss: {'supervised_loss': 1.9807431497788346, 'consistency_loss': 0.0003794570641330714, 'total_loss': 1.981046715409698, 'labeled_accuracy': 39.337876902118744, 'pseudo_accuracy': 95.64356432622021, 'high_conf_pseudo_labels': 505, 'labeled_samples': 3081, 'unlabeled_samples': 6167}, Val Acc: 44.606413994169095% (Best: 44.606413994169095%)\n",
            "üíæ Saving checkpoint for epoch 18...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 19/100\n",
            "Epoch 19: 100% 578/578 [02:04<00:00,  4.65it/s, Total=1.9557, Sup=1.9554, Cons=0.0004, L-Acc=40.3%, P-Acc=96.3%]\n",
            "üìä Epoch 19 - Train Loss: {'supervised_loss': 1.9553763515809004, 'consistency_loss': 0.0003511562731876731, 'total_loss': 1.955692392835155, 'labeled_accuracy': 40.259739929050596, 'pseudo_accuracy': 96.34369284230569, 'high_conf_pseudo_labels': 547, 'labeled_samples': 3080, 'unlabeled_samples': 6168}, Val Acc: 45.09232264334305% (Best: 45.09232264334305%)\n",
            "üíæ Saving checkpoint for epoch 19...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 20/100\n",
            "Epoch 20: 100% 578/578 [02:04<00:00,  4.65it/s, Total=1.9597, Sup=1.9696, Cons=0.0004, L-Acc=39.7%, P-Acc=96.3%]\n",
            "üìä Epoch 20 - Train Loss: {'supervised_loss': 1.969554793264555, 'consistency_loss': 0.0003585662898425477, 'total_loss': 1.9596907582129879, 'labeled_accuracy': 39.662446850656266, 'pseudo_accuracy': 96.31268426450656, 'high_conf_pseudo_labels': 678, 'labeled_samples': 3081, 'unlabeled_samples': 6167}, Val Acc: 44.70359572400389% (Best: 45.09232264334305%)\n",
            "üíæ Saving checkpoint for epoch 20...\n",
            "‚úÖ Saved checkpoint: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/checkpoint_epoch_20.pth\n",
            "\n",
            "üìÖ Epoch 21/100\n",
            "Epoch 21: 100% 578/578 [02:03<00:00,  4.67it/s, Total=1.9171, Sup=1.9200, Cons=0.0004, L-Acc=42.1%, P-Acc=97.2%]\n",
            "üìä Epoch 21 - Train Loss: {'supervised_loss': 1.9199876319391063, 'consistency_loss': 0.00036241376202430245, 'total_loss': 1.9170645103574557, 'labeled_accuracy': 42.1206221388781, 'pseudo_accuracy': 97.16840533101612, 'high_conf_pseudo_labels': 671, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 47.521865889212826% (Best: 47.521865889212826%)\n",
            "üíæ Saving checkpoint for epoch 21...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 22/100\n",
            "Epoch 22: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.9433, Sup=1.9461, Cons=0.0004, L-Acc=41.6%, P-Acc=96.8%]\n",
            "üìä Epoch 22 - Train Loss: {'supervised_loss': 1.9461207510177143, 'consistency_loss': 0.00042743420117183437, 'total_loss': 1.9432666843096809, 'labeled_accuracy': 41.628812085646146, 'pseudo_accuracy': 96.82080920445436, 'high_conf_pseudo_labels': 692, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 45.57823129251701% (Best: 47.521865889212826%)\n",
            "\n",
            "üìÖ Epoch 23/100\n",
            "Epoch 23: 100% 578/578 [02:03<00:00,  4.70it/s, Total=1.8850, Sup=1.8845, Cons=0.0004, L-Acc=43.4%, P-Acc=98.1%]\n",
            "üìä Epoch 23 - Train Loss: {'supervised_loss': 1.8844577041567403, 'consistency_loss': 0.0003909738876862503, 'total_loss': 1.884965970545079, 'labeled_accuracy': 43.35278814404855, 'pseudo_accuracy': 98.14077022192804, 'high_conf_pseudo_labels': 753, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 47.521865889212826% (Best: 47.521865889212826%)\n",
            "\n",
            "üìÖ Epoch 24/100\n",
            "Epoch 24: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.8848, Sup=1.8842, Cons=0.0004, L-Acc=43.7%, P-Acc=96.6%]\n",
            "üìä Epoch 24 - Train Loss: {'supervised_loss': 1.884228822372364, 'consistency_loss': 0.00040160148201818873, 'total_loss': 1.8847910635508467, 'labeled_accuracy': 43.67293936149552, 'pseudo_accuracy': 96.57282735668169, 'high_conf_pseudo_labels': 817, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 49.46550048590865% (Best: 49.46550048590865%)\n",
            "üíæ Saving checkpoint for epoch 24...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 25/100\n",
            "Epoch 25: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.8104, Sup=1.8129, Cons=0.0004, L-Acc=44.3%, P-Acc=97.5%]\n",
            "üìä Epoch 25 - Train Loss: {'supervised_loss': 1.8128554206373786, 'consistency_loss': 0.00043962468308252425, 'total_loss': 1.810378428412421, 'labeled_accuracy': 44.289422068636114, 'pseudo_accuracy': 97.47416752268882, 'high_conf_pseudo_labels': 871, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 49.56268221574344% (Best: 49.56268221574344%)\n",
            "üíæ Saving checkpoint for epoch 25...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 26/100\n",
            "Epoch 26: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.8268, Sup=1.8293, Cons=0.0004, L-Acc=44.6%, P-Acc=96.4%]\n",
            "üìä Epoch 26 - Train Loss: {'supervised_loss': 1.8293033725674066, 'consistency_loss': 0.0004397331264976148, 'total_loss': 1.82684205905059, 'labeled_accuracy': 44.6498050903068, 'pseudo_accuracy': 96.42058151040301, 'high_conf_pseudo_labels': 894, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 51.60349854227405% (Best: 51.60349854227405%)\n",
            "üíæ Saving checkpoint for epoch 26...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 27/100\n",
            "Epoch 27: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.8123, Sup=1.8148, Cons=0.0004, L-Acc=44.4%, P-Acc=97.4%]\n",
            "üìä Epoch 27 - Train Loss: {'supervised_loss': 1.814752793446364, 'consistency_loss': 0.000409600630493804, 'total_loss': 1.8123094022511077, 'labeled_accuracy': 44.39040163611624, 'pseudo_accuracy': 97.3523420889363, 'high_conf_pseudo_labels': 982, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 48.68804664723032% (Best: 51.60349854227405%)\n",
            "\n",
            "üìÖ Epoch 28/100\n",
            "Epoch 28: 100% 578/578 [02:02<00:00,  4.70it/s, Total=1.7740, Sup=1.7794, Cons=0.0004, L-Acc=44.4%, P-Acc=96.5%]\n",
            "üìä Epoch 28 - Train Loss: {'supervised_loss': 1.7793609343158703, 'consistency_loss': 0.00044906775194287113, 'total_loss': 1.774012298517978, 'labeled_accuracy': 44.404800151177824, 'pseudo_accuracy': 96.4894682866042, 'high_conf_pseudo_labels': 997, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 49.56268221574344% (Best: 51.60349854227405%)\n",
            "\n",
            "üìÖ Epoch 29/100\n",
            "Epoch 29: 100% 578/578 [02:03<00:00,  4.70it/s, Total=1.8006, Sup=1.8060, Cons=0.0005, L-Acc=44.3%, P-Acc=96.5%]\n",
            "üìä Epoch 29 - Train Loss: {'supervised_loss': 1.8060263109186456, 'consistency_loss': 0.0004557754377053831, 'total_loss': 1.8006430594596985, 'labeled_accuracy': 44.26069996329133, 'pseudo_accuracy': 96.52087459981323, 'high_conf_pseudo_labels': 1006, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 49.85422740524781% (Best: 51.60349854227405%)\n",
            "\n",
            "üìÖ Epoch 30/100\n",
            "Epoch 30: 100% 578/578 [02:03<00:00,  4.70it/s, Total=1.7573, Sup=1.7564, Cons=0.0005, L-Acc=45.5%, P-Acc=96.8%]\n",
            "üìä Epoch 30 - Train Loss: {'supervised_loss': 1.7564136655324472, 'consistency_loss': 0.00045781189374985, 'total_loss': 1.757329291925092, 'labeled_accuracy': 45.507622074469936, 'pseudo_accuracy': 96.82835815913641, 'high_conf_pseudo_labels': 1072, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 51.50631681243926% (Best: 51.60349854227405%)\n",
            "üíæ Saving checkpoint for epoch 30...\n",
            "‚úÖ Saved checkpoint: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/checkpoint_epoch_30.pth\n",
            "\n",
            "üìÖ Epoch 31/100\n",
            "Epoch 31: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.7373, Sup=1.7423, Cons=0.0005, L-Acc=46.1%, P-Acc=96.8%]\n",
            "üìä Epoch 31 - Train Loss: {'supervised_loss': 1.7423253370862868, 'consistency_loss': 0.0004947996564583279, 'total_loss': 1.7372861280730179, 'labeled_accuracy': 46.12138872756388, 'pseudo_accuracy': 96.81349568961375, 'high_conf_pseudo_labels': 1067, 'labeled_samples': 3081, 'unlabeled_samples': 6167}, Val Acc: 49.17395529640427% (Best: 51.60349854227405%)\n",
            "\n",
            "üìÖ Epoch 32/100\n",
            "Epoch 32: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.7515, Sup=1.7506, Cons=0.0005, L-Acc=47.1%, P-Acc=97.5%]\n",
            "üìä Epoch 32 - Train Loss: {'supervised_loss': 1.750561393629824, 'consistency_loss': 0.00047763259041124704, 'total_loss': 1.7515166589287738, 'labeled_accuracy': 47.07981784493795, 'pseudo_accuracy': 97.51332143441398, 'high_conf_pseudo_labels': 1126, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 49.85422740524781% (Best: 51.60349854227405%)\n",
            "\n",
            "üìÖ Epoch 33/100\n",
            "Epoch 33: 100% 578/578 [02:03<00:00,  4.70it/s, Total=1.7146, Sup=1.7136, Cons=0.0005, L-Acc=47.5%, P-Acc=95.6%]\n",
            "üìä Epoch 33 - Train Loss: {'supervised_loss': 1.713618271826254, 'consistency_loss': 0.0004962467364213593, 'total_loss': 1.7146107688236814, 'labeled_accuracy': 47.53406824839726, 'pseudo_accuracy': 95.56313980480103, 'high_conf_pseudo_labels': 1172, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 51.60349854227405% (Best: 51.60349854227405%)\n",
            "\n",
            "üìÖ Epoch 34/100\n",
            "Epoch 34: 100% 578/578 [02:03<00:00,  4.67it/s, Total=1.6987, Sup=1.6978, Cons=0.0005, L-Acc=47.8%, P-Acc=96.9%]\n",
            "üìä Epoch 34 - Train Loss: {'supervised_loss': 1.6977677879706798, 'consistency_loss': 0.0004644541884923538, 'total_loss': 1.6986966967634263, 'labeled_accuracy': 47.84300961619726, 'pseudo_accuracy': 96.91252137539112, 'high_conf_pseudo_labels': 1166, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 50.923226433430514% (Best: 51.60349854227405%)\n",
            "\n",
            "üìÖ Epoch 35/100\n",
            "Epoch 35: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.7097, Sup=1.7146, Cons=0.0005, L-Acc=46.6%, P-Acc=96.5%]\n",
            "üìä Epoch 35 - Train Loss: {'supervised_loss': 1.7145898329197533, 'consistency_loss': 0.0004969055241565574, 'total_loss': 1.7096508042284635, 'labeled_accuracy': 46.56067439815118, 'pseudo_accuracy': 96.47946339660798, 'high_conf_pseudo_labels': 1193, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 53.74149659863946% (Best: 53.74149659863946%)\n",
            "üíæ Saving checkpoint for epoch 35...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 36/100\n",
            "Epoch 36: 100% 578/578 [02:03<00:00,  4.67it/s, Total=1.7035, Sup=1.7025, Cons=0.0005, L-Acc=47.2%, P-Acc=95.0%]\n",
            "üìä Epoch 36 - Train Loss: {'supervised_loss': 1.7025162491522032, 'consistency_loss': 0.0005058869554852151, 'total_loss': 1.703528026976078, 'labeled_accuracy': 47.243838709152165, 'pseudo_accuracy': 94.96753232510059, 'high_conf_pseudo_labels': 1232, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 49.951409135082606% (Best: 53.74149659863946%)\n",
            "\n",
            "üìÖ Epoch 37/100\n",
            "Epoch 37: 100% 578/578 [02:03<00:00,  4.70it/s, Total=1.6867, Sup=1.6857, Cons=0.0005, L-Acc=47.9%, P-Acc=96.4%]\n",
            "üìä Epoch 37 - Train Loss: {'supervised_loss': 1.6856564747807683, 'consistency_loss': 0.0005218268244826979, 'total_loss': 1.68670012912742, 'labeled_accuracy': 47.89097940883104, 'pseudo_accuracy': 96.38364770877287, 'high_conf_pseudo_labels': 1272, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 52.76967930029154% (Best: 53.74149659863946%)\n",
            "\n",
            "üìÖ Epoch 38/100\n",
            "Epoch 38: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.6711, Sup=1.6700, Cons=0.0005, L-Acc=48.1%, P-Acc=96.5%]\n",
            "üìä Epoch 38 - Train Loss: {'supervised_loss': 1.6700191391586845, 'consistency_loss': 0.0005174574721564303, 'total_loss': 1.6710540549926278, 'labeled_accuracy': 48.05321171189345, 'pseudo_accuracy': 96.46616529306971, 'high_conf_pseudo_labels': 1330, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 52.28377065111759% (Best: 53.74149659863946%)\n",
            "\n",
            "üìÖ Epoch 39/100\n",
            "Epoch 39: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.6808, Sup=1.6855, Cons=0.0005, L-Acc=50.1%, P-Acc=95.8%]\n",
            "üìä Epoch 39 - Train Loss: {'supervised_loss': 1.6854946927374437, 'consistency_loss': 0.0005450752032252035, 'total_loss': 1.6807526796326084, 'labeled_accuracy': 50.1459611561095, 'pseudo_accuracy': 95.81263290795135, 'high_conf_pseudo_labels': 1409, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 51.60349854227405% (Best: 53.74149659863946%)\n",
            "\n",
            "üìÖ Epoch 40/100\n",
            "Epoch 40: 100% 578/578 [02:03<00:00,  4.70it/s, Total=1.5900, Sup=1.5917, Cons=0.0005, L-Acc=50.6%, P-Acc=96.7%]\n",
            "üìä Epoch 40 - Train Loss: {'supervised_loss': 1.5916855830047119, 'consistency_loss': 0.0005220029516500875, 'total_loss': 1.5899758090799438, 'labeled_accuracy': 50.64892879358597, 'pseudo_accuracy': 96.68079084730418, 'high_conf_pseudo_labels': 1416, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 52.86686103012634% (Best: 53.74149659863946%)\n",
            "üíæ Saving checkpoint for epoch 40...\n",
            "‚úÖ Saved checkpoint: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/checkpoint_epoch_40.pth\n",
            "\n",
            "üìÖ Epoch 41/100\n",
            "Epoch 41: 100% 578/578 [02:03<00:00,  4.70it/s, Total=1.6420, Sup=1.6409, Cons=0.0005, L-Acc=50.0%, P-Acc=96.6%]\n",
            "üìä Epoch 41 - Train Loss: {'supervised_loss': 1.640921805041059, 'consistency_loss': 0.0005393055929567916, 'total_loss': 1.6420004112085256, 'labeled_accuracy': 49.95134562746952, 'pseudo_accuracy': 96.56897774587962, 'high_conf_pseudo_labels': 1399, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 51.89504373177842% (Best: 53.74149659863946%)\n",
            "\n",
            "üìÖ Epoch 42/100\n",
            "Epoch 42: 100% 578/578 [02:02<00:00,  4.71it/s, Total=1.5783, Sup=1.5854, Cons=0.0006, L-Acc=50.9%, P-Acc=96.4%]\n",
            "üìä Epoch 42 - Train Loss: {'supervised_loss': 1.5854166688608087, 'consistency_loss': 0.0005550399272040799, 'total_loss': 1.5782979422202172, 'labeled_accuracy': 50.90850053740198, 'pseudo_accuracy': 96.43854741943615, 'high_conf_pseudo_labels': 1432, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 52.76967930029154% (Best: 53.74149659863946%)\n",
            "\n",
            "üìÖ Epoch 43/100\n",
            "Epoch 43: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.5784, Sup=1.5774, Cons=0.0005, L-Acc=50.9%, P-Acc=95.7%]\n",
            "üìä Epoch 43 - Train Loss: {'supervised_loss': 1.5773544256610854, 'consistency_loss': 0.0005426743036280241, 'total_loss': 1.578439772464206, 'labeled_accuracy': 50.85955184703905, 'pseudo_accuracy': 95.73426556620565, 'high_conf_pseudo_labels': 1430, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 53.93586005830904% (Best: 53.93586005830904%)\n",
            "üíæ Saving checkpoint for epoch 43...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 44/100\n",
            "Epoch 44: 100% 578/578 [02:03<00:00,  4.66it/s, Total=1.5858, Sup=1.5901, Cons=0.0006, L-Acc=49.7%, P-Acc=93.9%]\n",
            "üìä Epoch 44 - Train Loss: {'supervised_loss': 1.5900928798752527, 'consistency_loss': 0.0005950003165349278, 'total_loss': 1.5857808265795168, 'labeled_accuracy': 49.70798139336974, 'pseudo_accuracy': 93.91891872045156, 'high_conf_pseudo_labels': 1480, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 52.76967930029154% (Best: 53.93586005830904%)\n",
            "\n",
            "üìÖ Epoch 45/100\n",
            "Epoch 45: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.5530, Sup=1.5545, Cons=0.0006, L-Acc=52.2%, P-Acc=96.4%]\n",
            "üìä Epoch 45 - Train Loss: {'supervised_loss': 1.5545352963243242, 'consistency_loss': 0.0005657073224340421, 'total_loss': 1.552977203195117, 'labeled_accuracy': 52.17250274405869, 'pseudo_accuracy': 96.36363622013678, 'high_conf_pseudo_labels': 1595, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 52.1865889212828% (Best: 53.93586005830904%)\n",
            "\n",
            "üìÖ Epoch 46/100\n",
            "Epoch 46: 100% 578/578 [02:02<00:00,  4.70it/s, Total=1.5618, Sup=1.5633, Cons=0.0006, L-Acc=51.6%, P-Acc=95.6%]\n",
            "üìä Epoch 46 - Train Loss: {'supervised_loss': 1.5633185728480539, 'consistency_loss': 0.0006076403484968984, 'total_loss': 1.561829149058398, 'labeled_accuracy': 51.55742981679868, 'pseudo_accuracy': 95.60509539561666, 'high_conf_pseudo_labels': 1570, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 51.40913508260447% (Best: 53.93586005830904%)\n",
            "\n",
            "üìÖ Epoch 47/100\n",
            "Epoch 47: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.5313, Sup=1.5300, Cons=0.0006, L-Acc=52.0%, P-Acc=95.2%]\n",
            "üìä Epoch 47 - Train Loss: {'supervised_loss': 1.529994680987716, 'consistency_loss': 0.0006352987558948307, 'total_loss': 1.531265278485407, 'labeled_accuracy': 51.994809812359556, 'pseudo_accuracy': 95.23809500624961, 'high_conf_pseudo_labels': 1596, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 54.61613216715258% (Best: 54.61613216715258%)\n",
            "üíæ Saving checkpoint for epoch 47...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 48/100\n",
            "Epoch 48: 100% 578/578 [02:03<00:00,  4.67it/s, Total=1.5345, Sup=1.5359, Cons=0.0006, L-Acc=51.8%, P-Acc=96.5%]\n",
            "üìä Epoch 48 - Train Loss: {'supervised_loss': 1.535893294721377, 'consistency_loss': 0.0006215318662410918, 'total_loss': 1.5344791029786182, 'labeled_accuracy': 51.84824866450714, 'pseudo_accuracy': 96.52439010434034, 'high_conf_pseudo_labels': 1640, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 52.478134110787174% (Best: 54.61613216715258%)\n",
            "\n",
            "üìÖ Epoch 49/100\n",
            "Epoch 49: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.5253, Sup=1.5240, Cons=0.0006, L-Acc=53.0%, P-Acc=95.8%]\n",
            "üìä Epoch 49 - Train Loss: {'supervised_loss': 1.5240244722399847, 'consistency_loss': 0.0006279937699288098, 'total_loss': 1.5252804572993703, 'labeled_accuracy': 53.01556371374662, 'pseudo_accuracy': 95.84076037891408, 'high_conf_pseudo_labels': 1683, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 51.70068027210884% (Best: 54.61613216715258%)\n",
            "\n",
            "üìÖ Epoch 50/100\n",
            "Epoch 50: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.5096, Sup=1.5083, Cons=0.0006, L-Acc=53.0%, P-Acc=94.9%]\n",
            "üìä Epoch 50 - Train Loss: {'supervised_loss': 1.5083039576207065, 'consistency_loss': 0.0006473212106420037, 'total_loss': 1.5095985992376573, 'labeled_accuracy': 53.00032392508636, 'pseudo_accuracy': 94.86581075483609, 'high_conf_pseudo_labels': 1714, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 54.713313896987366% (Best: 54.713313896987366%)\n",
            "üíæ Saving checkpoint for epoch 50...\n",
            "‚úÖ Saved checkpoint: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/checkpoint_epoch_50.pth\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 51/100\n",
            "Epoch 51: 100% 578/578 [02:03<00:00,  4.67it/s, Total=1.5157, Sup=1.5196, Cons=0.0007, L-Acc=52.7%, P-Acc=94.5%]\n",
            "üìä Epoch 51 - Train Loss: {'supervised_loss': 1.5196100803570718, 'consistency_loss': 0.0006741688839642485, 'total_loss': 1.515700252247921, 'labeled_accuracy': 52.74083643457621, 'pseudo_accuracy': 94.460146785928, 'high_conf_pseudo_labels': 1769, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 53.64431486880466% (Best: 54.713313896987366%)\n",
            "\n",
            "üìÖ Epoch 52/100\n",
            "Epoch 52: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.4776, Sup=1.4788, Cons=0.0007, L-Acc=52.4%, P-Acc=94.3%]\n",
            "üìä Epoch 52 - Train Loss: {'supervised_loss': 1.4788021693454871, 'consistency_loss': 0.0006715228214032567, 'total_loss': 1.4775867361227328, 'labeled_accuracy': 52.401037742533056, 'pseudo_accuracy': 94.31495152311341, 'high_conf_pseudo_labels': 1759, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 54.713313896987366% (Best: 54.713313896987366%)\n",
            "\n",
            "üìÖ Epoch 53/100\n",
            "Epoch 53: 100% 578/578 [02:03<00:00,  4.70it/s, Total=1.4935, Sup=1.4947, Cons=0.0007, L-Acc=53.3%, P-Acc=94.3%]\n",
            "üìä Epoch 53 - Train Loss: {'supervised_loss': 1.4947181980236977, 'consistency_loss': 0.0006824308768366737, 'total_loss': 1.493497040814593, 'labeled_accuracy': 53.339817963685505, 'pseudo_accuracy': 94.25483480546798, 'high_conf_pseudo_labels': 1758, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 53.64431486880466% (Best: 54.713313896987366%)\n",
            "\n",
            "üìÖ Epoch 54/100\n",
            "Epoch 54: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.4307, Sup=1.4318, Cons=0.0007, L-Acc=54.8%, P-Acc=95.2%]\n",
            "üìä Epoch 54 - Train Loss: {'supervised_loss': 1.4317766367562723, 'consistency_loss': 0.000699343264226199, 'total_loss': 1.4306982022062207, 'labeled_accuracy': 54.83452254096693, 'pseudo_accuracy': 95.16129013594761, 'high_conf_pseudo_labels': 1860, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 56.268221574344025% (Best: 56.268221574344025%)\n",
            "üíæ Saving checkpoint for epoch 54...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 55/100\n",
            "Epoch 55: 100% 578/578 [02:04<00:00,  4.66it/s, Total=1.4583, Sup=1.4594, Cons=0.0007, L-Acc=54.2%, P-Acc=94.3%]\n",
            "üìä Epoch 55 - Train Loss: {'supervised_loss': 1.4593898152651674, 'consistency_loss': 0.0006981702592642011, 'total_loss': 1.4582612584578678, 'labeled_accuracy': 54.200453700394434, 'pseudo_accuracy': 94.31455879130348, 'high_conf_pseudo_labels': 1882, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 52.76967930029154% (Best: 56.268221574344025%)\n",
            "\n",
            "üìÖ Epoch 56/100\n",
            "Epoch 56: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.4364, Sup=1.4399, Cons=0.0007, L-Acc=53.3%, P-Acc=94.2%]\n",
            "üìä Epoch 56 - Train Loss: {'supervised_loss': 1.4399348564394232, 'consistency_loss': 0.0006999819790545664, 'total_loss': 1.436352348686631, 'labeled_accuracy': 53.33981792441293, 'pseudo_accuracy': 94.18794110208539, 'high_conf_pseudo_labels': 1841, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 55.10204081632653% (Best: 56.268221574344025%)\n",
            "\n",
            "üìÖ Epoch 57/100\n",
            "Epoch 57: 100% 578/578 [02:03<00:00,  4.70it/s, Total=1.4046, Sup=1.4056, Cons=0.0007, L-Acc=55.1%, P-Acc=93.5%]\n",
            "üìä Epoch 57 - Train Loss: {'supervised_loss': 1.4055672917560234, 'consistency_loss': 0.000733475594488891, 'total_loss': 1.404602463747283, 'labeled_accuracy': 55.094094239605624, 'pseudo_accuracy': 93.54144217097978, 'high_conf_pseudo_labels': 1858, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 57.33722060252673% (Best: 57.33722060252673%)\n",
            "üíæ Saving checkpoint for epoch 57...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 58/100\n",
            "Epoch 58: 100% 578/578 [02:03<00:00,  4.67it/s, Total=1.3811, Sup=1.3844, Cons=0.0007, L-Acc=56.4%, P-Acc=94.7%]\n",
            "üìä Epoch 58 - Train Loss: {'supervised_loss': 1.3844120521098375, 'consistency_loss': 0.0007161160519207219, 'total_loss': 1.381053933575282, 'labeled_accuracy': 56.387807522302474, 'pseudo_accuracy': 94.66173331077205, 'high_conf_pseudo_labels': 1892, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 54.03304178814383% (Best: 57.33722060252673%)\n",
            "\n",
            "üìÖ Epoch 59/100\n",
            "Epoch 59: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.3848, Sup=1.3833, Cons=0.0007, L-Acc=56.5%, P-Acc=94.5%]\n",
            "üìä Epoch 59 - Train Loss: {'supervised_loss': 1.383282089580147, 'consistency_loss': 0.0007478133677106164, 'total_loss': 1.3847777170409483, 'labeled_accuracy': 56.48508384008197, 'pseudo_accuracy': 94.4616974935275, 'high_conf_pseudo_labels': 1932, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 54.61613216715258% (Best: 57.33722060252673%)\n",
            "\n",
            "üìÖ Epoch 60/100\n",
            "Epoch 60: 100% 578/578 [02:03<00:00,  4.70it/s, Total=1.3833, Sup=1.3841, Cons=0.0008, L-Acc=55.9%, P-Acc=94.3%]\n",
            "üìä Epoch 60 - Train Loss: {'supervised_loss': 1.384107719164219, 'consistency_loss': 0.0007720084198380765, 'total_loss': 1.3832570851549255, 'labeled_accuracy': 55.854686424634, 'pseudo_accuracy': 94.28137631165355, 'high_conf_pseudo_labels': 1976, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 56.073858114674444% (Best: 57.33722060252673%)\n",
            "üíæ Saving checkpoint for epoch 60...\n",
            "‚úÖ Saved checkpoint: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/checkpoint_epoch_60.pth\n",
            "\n",
            "üìÖ Epoch 61/100\n",
            "Epoch 61: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.3380, Sup=1.3389, Cons=0.0007, L-Acc=57.9%, P-Acc=94.7%]\n",
            "üìä Epoch 61 - Train Loss: {'supervised_loss': 1.3388592066562879, 'consistency_loss': 0.0007332366411143166, 'total_loss': 1.3380093132224253, 'labeled_accuracy': 57.852043616934154, 'pseudo_accuracy': 94.6850391710837, 'high_conf_pseudo_labels': 2032, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 55.587949465500486% (Best: 57.33722060252673%)\n",
            "\n",
            "üìÖ Epoch 62/100\n",
            "Epoch 62: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.3550, Sup=1.3535, Cons=0.0008, L-Acc=57.9%, P-Acc=93.8%]\n",
            "üìä Epoch 62 - Train Loss: {'supervised_loss': 1.3534784947995702, 'consistency_loss': 0.00075935892870321, 'total_loss': 1.3549972095375293, 'labeled_accuracy': 57.93058651884848, 'pseudo_accuracy': 93.77692644878002, 'high_conf_pseudo_labels': 2089, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 56.85131195335277% (Best: 57.33722060252673%)\n",
            "\n",
            "üìÖ Epoch 63/100\n",
            "Epoch 63: 100% 578/578 [02:02<00:00,  4.70it/s, Total=1.3748, Sup=1.3781, Cons=0.0008, L-Acc=56.9%, P-Acc=93.4%]\n",
            "üìä Epoch 63 - Train Loss: {'supervised_loss': 1.3780704051265882, 'consistency_loss': 0.0007730101337023949, 'total_loss': 1.374848016118792, 'labeled_accuracy': 56.87418877537303, 'pseudo_accuracy': 93.43610125806781, 'high_conf_pseudo_labels': 2011, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 55.19922254616132% (Best: 57.33722060252673%)\n",
            "\n",
            "üìÖ Epoch 64/100\n",
            "Epoch 64: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.3191, Sup=1.3175, Cons=0.0008, L-Acc=57.5%, P-Acc=93.7%]\n",
            "üìä Epoch 64 - Train Loss: {'supervised_loss': 1.317493219785779, 'consistency_loss': 0.0007855102400114972, 'total_loss': 1.3190642415149505, 'labeled_accuracy': 57.52757894861319, 'pseudo_accuracy': 93.69193141163124, 'high_conf_pseudo_labels': 2045, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 56.85131195335277% (Best: 57.33722060252673%)\n",
            "\n",
            "üìÖ Epoch 65/100\n",
            "Epoch 65: 100% 578/578 [02:03<00:00,  4.66it/s, Total=1.3480, Sup=1.3511, Cons=0.0008, L-Acc=57.9%, P-Acc=93.8%]\n",
            "üìä Epoch 65 - Train Loss: {'supervised_loss': 1.351077339767168, 'consistency_loss': 0.0008156562352329041, 'total_loss': 1.3480336434815312, 'labeled_accuracy': 57.87082059365594, 'pseudo_accuracy': 93.81139464087946, 'high_conf_pseudo_labels': 2036, 'labeled_samples': 3081, 'unlabeled_samples': 6167}, Val Acc: 52.76967930029154% (Best: 57.33722060252673%)\n",
            "\n",
            "üìÖ Epoch 66/100\n",
            "Epoch 66: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.3130, Sup=1.3113, Cons=0.0008, L-Acc=57.4%, P-Acc=93.4%]\n",
            "üìä Epoch 66 - Train Loss: {'supervised_loss': 1.3113430127321024, 'consistency_loss': 0.0008075162438554109, 'total_loss': 1.3129580436487098, 'labeled_accuracy': 57.444047510256596, 'pseudo_accuracy': 93.37321637711435, 'high_conf_pseudo_labels': 2173, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 55.49076773566569% (Best: 57.33722060252673%)\n",
            "\n",
            "üìÖ Epoch 67/100\n",
            "Epoch 67: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.3111, Sup=1.3094, Cons=0.0009, L-Acc=58.1%, P-Acc=93.1%]\n",
            "üìä Epoch 67 - Train Loss: {'supervised_loss': 1.3093772585919468, 'consistency_loss': 0.0008701221153053864, 'total_loss': 1.3111175003245628, 'labeled_accuracy': 58.06033030431687, 'pseudo_accuracy': 93.07025946873215, 'high_conf_pseudo_labels': 2078, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 55.29640427599611% (Best: 57.33722060252673%)\n",
            "\n",
            "üìÖ Epoch 68/100\n",
            "Epoch 68: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.2737, Sup=1.2763, Cons=0.0009, L-Acc=58.6%, P-Acc=93.3%]\n",
            "üìä Epoch 68 - Train Loss: {'supervised_loss': 1.276329294545576, 'consistency_loss': 0.0008911479787762473, 'total_loss': 1.27369522549245, 'labeled_accuracy': 58.62516158038385, 'pseudo_accuracy': 93.29014315907462, 'high_conf_pseudo_labels': 2161, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 57.04567541302235% (Best: 57.33722060252673%)\n",
            "\n",
            "üìÖ Epoch 69/100\n",
            "Epoch 69: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.2840, Sup=1.2846, Cons=0.0008, L-Acc=58.8%, P-Acc=93.7%]\n",
            "üìä Epoch 69 - Train Loss: {'supervised_loss': 1.2845618326743125, 'consistency_loss': 0.0008416530789177426, 'total_loss': 1.2840227122796428, 'labeled_accuracy': 58.844530499937164, 'pseudo_accuracy': 93.73848969194331, 'high_conf_pseudo_labels': 2172, 'labeled_samples': 3081, 'unlabeled_samples': 6167}, Val Acc: 57.434402332361515% (Best: 57.434402332361515%)\n",
            "üíæ Saving checkpoint for epoch 69...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 70/100\n",
            "Epoch 70: 100% 578/578 [02:03<00:00,  4.67it/s, Total=1.2459, Sup=1.2442, Cons=0.0009, L-Acc=60.0%, P-Acc=92.4%]\n",
            "üìä Epoch 70 - Train Loss: {'supervised_loss': 1.2441961696938608, 'consistency_loss': 0.0008733232500987082, 'total_loss': 1.2459428150619294, 'labeled_accuracy': 59.9610637510697, 'pseudo_accuracy': 92.39610921419583, 'high_conf_pseudo_labels': 2262, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 54.518950437317784% (Best: 57.434402332361515%)\n",
            "üíæ Saving checkpoint for epoch 70...\n",
            "‚úÖ Saved checkpoint: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/checkpoint_epoch_70.pth\n",
            "\n",
            "üìÖ Epoch 71/100\n",
            "Epoch 71: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.2421, Sup=1.2425, Cons=0.0009, L-Acc=59.9%, P-Acc=93.6%]\n",
            "üìä Epoch 71 - Train Loss: {'supervised_loss': 1.242524419141291, 'consistency_loss': 0.0008866393671388766, 'total_loss': 1.2421480027606684, 'labeled_accuracy': 59.90917883064698, 'pseudo_accuracy': 93.61793847883915, 'high_conf_pseudo_labels': 2319, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 57.24003887269193% (Best: 57.434402332361515%)\n",
            "\n",
            "üìÖ Epoch 72/100\n",
            "Epoch 72: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.2410, Sup=1.2413, Cons=0.0009, L-Acc=59.5%, P-Acc=92.8%]\n",
            "üìä Epoch 72 - Train Loss: {'supervised_loss': 1.2412824792015904, 'consistency_loss': 0.0009133877950397427, 'total_loss': 1.240961703656533, 'labeled_accuracy': 59.4550756750809, 'pseudo_accuracy': 92.7804036388389, 'high_conf_pseudo_labels': 2327, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 56.268221574344025% (Best: 57.434402332361515%)\n",
            "\n",
            "üìÖ Epoch 73/100\n",
            "Epoch 73: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.2318, Sup=1.2341, Cons=0.0010, L-Acc=60.1%, P-Acc=92.6%]\n",
            "üìä Epoch 73 - Train Loss: {'supervised_loss': 1.2340811083946999, 'consistency_loss': 0.0009787826510134175, 'total_loss': 1.2317684937949667, 'labeled_accuracy': 60.08430558760033, 'pseudo_accuracy': 92.56161944127419, 'high_conf_pseudo_labels': 2272, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 56.65694849368319% (Best: 57.434402332361515%)\n",
            "\n",
            "üìÖ Epoch 74/100\n",
            "Epoch 74: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.2111, Sup=1.2091, Cons=0.0010, L-Acc=61.6%, P-Acc=90.9%]\n",
            "üìä Epoch 74 - Train Loss: {'supervised_loss': 1.2090742530053076, 'consistency_loss': 0.00101723834041125, 'total_loss': 1.2111087294285763, 'labeled_accuracy': 61.628283513044714, 'pseudo_accuracy': 90.94437221907089, 'high_conf_pseudo_labels': 2319, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 57.628765792031096% (Best: 57.628765792031096%)\n",
            "üíæ Saving checkpoint for epoch 74...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 75/100\n",
            "Epoch 75: 100% 578/578 [02:03<00:00,  4.66it/s, Total=1.2344, Sup=1.2324, Cons=0.0010, L-Acc=60.4%, P-Acc=92.4%]\n",
            "üìä Epoch 75 - Train Loss: {'supervised_loss': 1.2324400788501781, 'consistency_loss': 0.0009775996994900687, 'total_loss': 1.2343952770129, 'labeled_accuracy': 60.37000916693668, 'pseudo_accuracy': 92.3768733529003, 'high_conf_pseudo_labels': 2335, 'labeled_samples': 3081, 'unlabeled_samples': 6167}, Val Acc: 56.754130223517976% (Best: 57.628765792031096%)\n",
            "\n",
            "üìÖ Epoch 76/100\n",
            "Epoch 76: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.2098, Sup=1.2120, Cons=0.0010, L-Acc=61.4%, P-Acc=93.2%]\n",
            "üìä Epoch 76 - Train Loss: {'supervised_loss': 1.2120055060691408, 'consistency_loss': 0.0009791381169911345, 'total_loss': 1.2097699917495577, 'labeled_accuracy': 61.36879605903589, 'pseudo_accuracy': 93.16167129349135, 'high_conf_pseudo_labels': 2369, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 56.754130223517976% (Best: 57.628765792031096%)\n",
            "\n",
            "üìÖ Epoch 77/100\n",
            "Epoch 77: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.1873, Sup=1.1854, Cons=0.0010, L-Acc=61.8%, P-Acc=93.3%]\n",
            "üìä Epoch 77 - Train Loss: {'supervised_loss': 1.1854182288802315, 'consistency_loss': 0.0009598432473917282, 'total_loss': 1.1873379146026055, 'labeled_accuracy': 61.835278279122676, 'pseudo_accuracy': 93.33898995051102, 'high_conf_pseudo_labels': 2357, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 56.46258503401361% (Best: 57.628765792031096%)\n",
            "\n",
            "üìÖ Epoch 78/100\n",
            "Epoch 78: 100% 578/578 [02:03<00:00,  4.67it/s, Total=1.1796, Sup=1.1775, Cons=0.0011, L-Acc=62.1%, P-Acc=92.2%]\n",
            "üìä Epoch 78 - Train Loss: {'supervised_loss': 1.1775348811671396, 'consistency_loss': 0.0010529228842214627, 'total_loss': 1.179640726434473, 'labeled_accuracy': 62.09468179083056, 'pseudo_accuracy': 92.18089568201809, 'high_conf_pseudo_labels': 2366, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 55.393586005830905% (Best: 57.628765792031096%)\n",
            "\n",
            "üìÖ Epoch 79/100\n",
            "Epoch 79: 100% 578/578 [02:02<00:00,  4.70it/s, Total=1.1361, Sup=1.1340, Cons=0.0010, L-Acc=62.9%, P-Acc=92.7%]\n",
            "üìä Epoch 79 - Train Loss: {'supervised_loss': 1.1339874097684264, 'consistency_loss': 0.0010397019563444204, 'total_loss': 1.1360668144188821, 'labeled_accuracy': 62.946138286033595, 'pseudo_accuracy': 92.68491999879875, 'high_conf_pseudo_labels': 2447, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 54.713313896987366% (Best: 57.628765792031096%)\n",
            "\n",
            "üìÖ Epoch 80/100\n",
            "Epoch 80: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.1531, Sup=1.1509, Cons=0.0011, L-Acc=63.3%, P-Acc=91.2%]\n",
            "üìä Epoch 80 - Train Loss: {'supervised_loss': 1.150912500360407, 'consistency_loss': 0.0011022087347401293, 'total_loss': 1.1531169165995707, 'labeled_accuracy': 63.335495894331196, 'pseudo_accuracy': 91.15755587145446, 'high_conf_pseudo_labels': 2488, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 57.04567541302235% (Best: 57.628765792031096%)\n",
            "üíæ Saving checkpoint for epoch 80...\n",
            "‚úÖ Saved checkpoint: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/checkpoint_epoch_80.pth\n",
            "\n",
            "üìÖ Epoch 81/100\n",
            "Epoch 81: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.1689, Sup=1.1667, Cons=0.0011, L-Acc=61.9%, P-Acc=92.2%]\n",
            "üìä Epoch 81 - Train Loss: {'supervised_loss': 1.166749764620536, 'consistency_loss': 0.0010892903665584194, 'total_loss': 1.1689283442151794, 'labeled_accuracy': 61.94029806775756, 'pseudo_accuracy': 92.15219950968908, 'high_conf_pseudo_labels': 2523, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 56.559766763848394% (Best: 57.628765792031096%)\n",
            "\n",
            "üìÖ Epoch 82/100\n",
            "Epoch 82: 100% 578/578 [02:02<00:00,  4.70it/s, Total=1.1488, Sup=1.1487, Cons=0.0011, L-Acc=63.1%, P-Acc=91.7%]\n",
            "üìä Epoch 82 - Train Loss: {'supervised_loss': 1.1486644850691952, 'consistency_loss': 0.0010718461147237208, 'total_loss': 1.1488208687716408, 'labeled_accuracy': 63.06744436407522, 'pseudo_accuracy': 91.72277182852868, 'high_conf_pseudo_labels': 2525, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 54.13022351797862% (Best: 57.628765792031096%)\n",
            "\n",
            "üìÖ Epoch 83/100\n",
            "Epoch 83: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.1179, Sup=1.1157, Cons=0.0011, L-Acc=63.7%, P-Acc=91.7%]\n",
            "üìä Epoch 83 - Train Loss: {'supervised_loss': 1.1157090089712485, 'consistency_loss': 0.001095109407114877, 'total_loss': 1.11789922779716, 'labeled_accuracy': 63.69240687436519, 'pseudo_accuracy': 91.71924259384348, 'high_conf_pseudo_labels': 2536, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 54.3245869776482% (Best: 57.628765792031096%)\n",
            "\n",
            "üìÖ Epoch 84/100\n",
            "Epoch 84: 100% 578/578 [02:03<00:00,  4.66it/s, Total=1.1581, Sup=1.1559, Cons=0.0011, L-Acc=63.3%, P-Acc=90.7%]\n",
            "üìä Epoch 84 - Train Loss: {'supervised_loss': 1.1559456347183334, 'consistency_loss': 0.0010745257443738746, 'total_loss': 1.1580946881188705, 'labeled_accuracy': 63.30304942666363, 'pseudo_accuracy': 90.65529228024077, 'high_conf_pseudo_labels': 2579, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 56.559766763848394% (Best: 57.628765792031096%)\n",
            "\n",
            "üìÖ Epoch 85/100\n",
            "Epoch 85: 100% 578/578 [02:03<00:00,  4.67it/s, Total=1.1128, Sup=1.1124, Cons=0.0011, L-Acc=64.3%, P-Acc=92.0%]\n",
            "üìä Epoch 85 - Train Loss: {'supervised_loss': 1.1124342690372075, 'consistency_loss': 0.0011206675732446268, 'total_loss': 1.112750976818741, 'labeled_accuracy': 64.30888964865287, 'pseudo_accuracy': 92.01565516372949, 'high_conf_pseudo_labels': 2555, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 55.393586005830905% (Best: 57.628765792031096%)\n",
            "\n",
            "üìÖ Epoch 86/100\n",
            "Epoch 86: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.0949, Sup=1.0927, Cons=0.0011, L-Acc=65.0%, P-Acc=91.9%]\n",
            "üìä Epoch 86 - Train Loss: {'supervised_loss': 1.0926987211780694, 'consistency_loss': 0.0011170036120684238, 'total_loss': 1.094932728882067, 'labeled_accuracy': 64.96918514491752, 'pseudo_accuracy': 91.91996881382977, 'high_conf_pseudo_labels': 2599, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 55.19922254616132% (Best: 57.628765792031096%)\n",
            "\n",
            "üìÖ Epoch 87/100\n",
            "Epoch 87: 100% 578/578 [02:02<00:00,  4.70it/s, Total=1.0881, Sup=1.0856, Cons=0.0012, L-Acc=65.4%, P-Acc=89.8%]\n",
            "üìä Epoch 87 - Train Loss: {'supervised_loss': 1.0856354303181068, 'consistency_loss': 0.0012131688414845666, 'total_loss': 1.088061768445589, 'labeled_accuracy': 65.37962301113171, 'pseudo_accuracy': 89.80201668181792, 'high_conf_pseudo_labels': 2677, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 55.587949465500486% (Best: 57.628765792031096%)\n",
            "\n",
            "üìÖ Epoch 88/100\n",
            "Epoch 88: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.0845, Sup=1.0821, Cons=0.0012, L-Acc=65.8%, P-Acc=90.4%]\n",
            "üìä Epoch 88 - Train Loss: {'supervised_loss': 1.0820727056781778, 'consistency_loss': 0.0012242740538183949, 'total_loss': 1.0845212535964885, 'labeled_accuracy': 65.82360505562967, 'pseudo_accuracy': 90.41960601296692, 'high_conf_pseudo_labels': 2693, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 58.01749271137026% (Best: 58.01749271137026%)\n",
            "üíæ Saving checkpoint for epoch 88...\n",
            "üèÜ New best model saved: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "\n",
            "üìÖ Epoch 89/100\n",
            "Epoch 89: 100% 578/578 [02:03<00:00,  4.67it/s, Total=1.0569, Sup=1.0545, Cons=0.0012, L-Acc=66.3%, P-Acc=90.6%]\n",
            "üìä Epoch 89 - Train Loss: {'supervised_loss': 1.0545329364132665, 'consistency_loss': 0.0011776008818034328, 'total_loss': 1.0568881373865382, 'labeled_accuracy': 66.32057032841973, 'pseudo_accuracy': 90.62844502211756, 'high_conf_pseudo_labels': 2721, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 53.44995140913508% (Best: 58.01749271137026%)\n",
            "\n",
            "üìÖ Epoch 90/100\n",
            "Epoch 90: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.1157, Sup=1.1150, Cons=0.0013, L-Acc=64.4%, P-Acc=89.9%]\n",
            "üìä Epoch 90 - Train Loss: {'supervised_loss': 1.1149783341062338, 'consistency_loss': 0.001315191566277856, 'total_loss': 1.1156796891621723, 'labeled_accuracy': 64.38533832142966, 'pseudo_accuracy': 89.87249511413019, 'high_conf_pseudo_labels': 2745, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 57.82312925170068% (Best: 58.01749271137026%)\n",
            "üíæ Saving checkpoint for epoch 90...\n",
            "‚úÖ Saved checkpoint: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/checkpoint_epoch_90.pth\n",
            "\n",
            "üìÖ Epoch 91/100\n",
            "Epoch 91: 100% 578/578 [02:02<00:00,  4.70it/s, Total=1.0497, Sup=1.0507, Cons=0.0013, L-Acc=65.7%, P-Acc=90.4%]\n",
            "üìä Epoch 91 - Train Loss: {'supervised_loss': 1.0507203078337222, 'consistency_loss': 0.0012856757390263707, 'total_loss': 1.0496559481843049, 'labeled_accuracy': 65.69390339486485, 'pseudo_accuracy': 90.43696225065214, 'high_conf_pseudo_labels': 2792, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 56.65694849368319% (Best: 58.01749271137026%)\n",
            "\n",
            "üìÖ Epoch 92/100\n",
            "Epoch 92: 100% 578/578 [02:03<00:00,  4.70it/s, Total=1.0385, Sup=1.0396, Cons=0.0012, L-Acc=66.8%, P-Acc=89.7%]\n",
            "üìä Epoch 92 - Train Loss: {'supervised_loss': 1.0395791961022445, 'consistency_loss': 0.0012371126294178747, 'total_loss': 1.0384562607394583, 'labeled_accuracy': 66.81803378075158, 'pseudo_accuracy': 89.70431014095968, 'high_conf_pseudo_labels': 2807, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 57.24003887269193% (Best: 58.01749271137026%)\n",
            "\n",
            "üìÖ Epoch 93/100\n",
            "Epoch 93: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.0694, Sup=1.0669, Cons=0.0012, L-Acc=65.4%, P-Acc=91.0%]\n",
            "üìä Epoch 93 - Train Loss: {'supervised_loss': 1.0669060533602035, 'consistency_loss': 0.0012320172076196504, 'total_loss': 1.0693700872555014, 'labeled_accuracy': 65.4232883676089, 'pseudo_accuracy': 90.96868114790948, 'high_conf_pseudo_labels': 2746, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 56.46258503401361% (Best: 58.01749271137026%)\n",
            "\n",
            "üìÖ Epoch 94/100\n",
            "Epoch 94: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.0432, Sup=1.0407, Cons=0.0012, L-Acc=66.3%, P-Acc=91.2%]\n",
            "üìä Epoch 94 - Train Loss: {'supervised_loss': 1.040703589586496, 'consistency_loss': 0.0012304373577444179, 'total_loss': 1.0431644644130627, 'labeled_accuracy': 66.30998647042904, 'pseudo_accuracy': 91.23619476416206, 'high_conf_pseudo_labels': 2807, 'labeled_samples': 3084, 'unlabeled_samples': 6164}, Val Acc: 55.00485908649174% (Best: 58.01749271137026%)\n",
            "\n",
            "üìÖ Epoch 95/100\n",
            "Epoch 95: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.0044, Sup=1.0035, Cons=0.0013, L-Acc=67.2%, P-Acc=89.2%]\n",
            "üìä Epoch 95 - Train Loss: {'supervised_loss': 1.0035170177667623, 'consistency_loss': 0.0013014660370947026, 'total_loss': 1.0043837614345388, 'labeled_accuracy': 67.16417837050115, 'pseudo_accuracy': 89.17470467946664, 'high_conf_pseudo_labels': 2799, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 55.10204081632653% (Best: 58.01749271137026%)\n",
            "\n",
            "üìÖ Epoch 96/100\n",
            "Epoch 96: 100% 578/578 [02:03<00:00,  4.70it/s, Total=1.0112, Sup=1.0083, Cons=0.0014, L-Acc=68.1%, P-Acc=88.4%]\n",
            "üìä Epoch 96 - Train Loss: {'supervised_loss': 1.0083083751946558, 'consistency_loss': 0.0014246523804853067, 'total_loss': 1.0111576784853291, 'labeled_accuracy': 68.14790727192808, 'pseudo_accuracy': 88.44815549936739, 'high_conf_pseudo_labels': 2874, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 54.61613216715258% (Best: 58.01749271137026%)\n",
            "\n",
            "üìÖ Epoch 97/100\n",
            "Epoch 97: 100% 578/578 [02:03<00:00,  4.69it/s, Total=1.0133, Sup=1.0106, Cons=0.0013, L-Acc=67.6%, P-Acc=89.7%]\n",
            "üìä Epoch 97 - Train Loss: {'supervised_loss': 1.0106495390662273, 'consistency_loss': 0.0013331470287575751, 'total_loss': 1.0133158343177044, 'labeled_accuracy': 67.62893225904645, 'pseudo_accuracy': 89.72767973554336, 'high_conf_pseudo_labels': 2901, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 55.00485908649174% (Best: 58.01749271137026%)\n",
            "\n",
            "üìÖ Epoch 98/100\n",
            "Epoch 98: 100% 578/578 [02:03<00:00,  4.68it/s, Total=1.0023, Sup=0.9996, Cons=0.0014, L-Acc=68.2%, P-Acc=88.5%]\n",
            "üìä Epoch 98 - Train Loss: {'supervised_loss': 0.9995805956358831, 'consistency_loss': 0.001373281797162937, 'total_loss': 1.0023271604134962, 'labeled_accuracy': 68.17001875741849, 'pseudo_accuracy': 88.53700464481742, 'high_conf_pseudo_labels': 2905, 'labeled_samples': 3082, 'unlabeled_samples': 6166}, Val Acc: 56.65694849368319% (Best: 58.01749271137026%)\n",
            "\n",
            "üìÖ Epoch 99/100\n",
            "Epoch 99: 100% 578/578 [02:03<00:00,  4.67it/s, Total=1.0414, Sup=1.0403, Cons=0.0014, L-Acc=66.7%, P-Acc=90.2%]\n",
            "üìä Epoch 99 - Train Loss: {'supervised_loss': 1.0402987931905059, 'consistency_loss': 0.001427875256091743, 'total_loss': 1.041354718539142, 'labeled_accuracy': 66.73158003811092, 'pseudo_accuracy': 90.18404854408477, 'high_conf_pseudo_labels': 2934, 'labeled_samples': 3081, 'unlabeled_samples': 6167}, Val Acc: 57.92031098153547% (Best: 58.01749271137026%)\n",
            "\n",
            "üìÖ Epoch 100/100\n",
            "Epoch 100: 100% 578/578 [02:03<00:00,  4.69it/s, Total=0.9779, Sup=0.9749, Cons=0.0015, L-Acc=68.3%, P-Acc=88.9%]\n",
            "üìä Epoch 100 - Train Loss: {'supervised_loss': 0.9749002921778489, 'consistency_loss': 0.001502579366498237, 'total_loss': 0.9779054495573662, 'labeled_accuracy': 68.27765092098052, 'pseudo_accuracy': 88.91574535053738, 'high_conf_pseudo_labels': 2896, 'labeled_samples': 3083, 'unlabeled_samples': 6165}, Val Acc: 58.01749271137026% (Best: 58.01749271137026%)\n",
            "üíæ Saving checkpoint for epoch 100...\n",
            "‚úÖ Saved checkpoint: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/checkpoint_epoch_100.pth\n",
            "\n",
            "üéâ Training completed!\n",
            "üèÜ Best accuracy: 58.01749271137026%\n",
            "\n",
            "============================================================\n",
            "üéâ TRAINING COMPLETED!\n",
            "‚è∞ Finished at: 2025-08-18 15:57:38\n",
            "üèÜ Best accuracy achieved: 58.02%\n",
            "üìä Best model from epoch: 88\n",
            "‚úÖ Your MAE-enhanced model is ready for evaluation and deployment!\n"
          ]
        }
      ],
      "source": [
        "# Start Semi-Supervised Training with Optional MAE Initialization\n",
        "import os\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import json # Import json for passing config\n",
        "\n",
        "print(\"üöÄ STARTING SEMI-SUPERVISED FISH CLASSIFICATION TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Change to repository directory\n",
        "%cd /content/ViT-FishID\n",
        "\n",
        "# Check for existing checkpoints to resume from\n",
        "RESUME_FROM = None\n",
        "if os.path.exists(TRAINING_CONFIG['checkpoint_dir']):\n",
        "    checkpoints = glob.glob(os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'checkpoint_epoch_*.pth'))\n",
        "    if checkpoints:\n",
        "        # Find the latest checkpoint\n",
        "        epoch_numbers = []\n",
        "        for cp in checkpoints:\n",
        "            try:\n",
        "                epoch_num = int(cp.split('epoch_')[1].split('.')[0])\n",
        "                epoch_numbers.append((epoch_num, cp))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if epoch_numbers:\n",
        "            epoch_numbers.sort(key=lambda x: x[0], reverse=True)  # Latest first\n",
        "            latest_epoch, latest_checkpoint = epoch_numbers[0]\n",
        "            print(f\"üîç Found existing checkpoints. Latest: Epoch {latest_epoch}\")\n",
        "\n",
        "            # Ask user if they want to resume (auto-skip in Colab for now)\n",
        "            # resume_choice = input(\"Do you want to resume from the latest checkpoint? (y/n): \").lower().strip()\n",
        "            resume_choice = 'n'  # Set to 'y' if you want to auto-resume\n",
        "\n",
        "            if resume_choice in ['y', 'yes']:\n",
        "                RESUME_FROM = latest_checkpoint\n",
        "                print(f\"‚úÖ Will resume from: {os.path.basename(latest_checkpoint)}\")\n",
        "            else:\n",
        "                print(\"üÜï Starting fresh training from epoch 1\")\n",
        "\n",
        "# Determine which training script to use\n",
        "use_mae_script = TRAINING_CONFIG.get('mae_pretrained', False) and 'MAE_ENCODER_WEIGHTS' in globals() and MAE_ENCODER_WEIGHTS is not None\n",
        "\n",
        "# Serialize TRAINING_CONFIG, NUM_CLASSES, and RESUME_FROM to pass to the script\n",
        "training_config_json = json.dumps(TRAINING_CONFIG)\n",
        "num_classes_str = str(NUM_CLASSES)\n",
        "resume_from_str = RESUME_FROM if RESUME_FROM is not None else 'None'\n",
        "\n",
        "if use_mae_script:\n",
        "    print(\"ü§ñ Preparing MAE-enhanced training script...\")\n",
        "\n",
        "    # Generate the full training script with MAE initialization logic\n",
        "    training_script_content = f\"\"\"#!/usr/bin/env python3\n",
        "import sys\n",
        "sys.path.append('/content/ViT-FishID')\n",
        "\n",
        "import torch\n",
        "import argparse\n",
        "import os\n",
        "import glob\n",
        "import wandb\n",
        "import json # Import json to load config\n",
        "from datetime import datetime\n",
        "\n",
        "from model import ViTForFishClassification\n",
        "from trainer import EMATrainer, SemiSupervisedTrainer # Ensure both trainers are imported here\n",
        "from data import create_dataloaders, create_semi_supervised_dataloaders\n",
        "from utils import get_device, set_seed\n",
        "\n",
        "# --- Argument Parsing for Config ---\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--config_json', type=str, required=True, help='JSON string of training configuration')\n",
        "parser.add_argument('--num_classes_arg', type=int, required=True, help='Number of classes')\n",
        "parser.add_argument('--resume_from_arg', type=str, default='None', help='Path to resume checkpoint')\n",
        "\n",
        "# Parse arguments passed from the launching script\n",
        "script_args = parser.parse_args()\n",
        "TRAINING_CONFIG = json.loads(script_args.config_json)\n",
        "NUM_CLASSES = script_args.num_classes_arg\n",
        "RESUME_FROM = script_args.resume_from_arg if script_args.resume_from_arg != 'None' else None\n",
        "\n",
        "print(\"‚úÖ Loaded configuration from arguments.\")\n",
        "print(f\"üìä Configured num_classes: \" + str(NUM_CLASSES))\n",
        "if RESUME_FROM:\n",
        "    print(f\"üîÑ Configured resume_from: \" + str(RESUME_FROM))\n",
        "else:\n",
        "    print(\"üÜï Training from scratch based on arguments.\")\n",
        "\n",
        "# --- MAE Loading Logic (moved into the script) ---\n",
        "def load_mae_encoder_weights(mae_checkpoint_path):\n",
        "    print(f\"üì• Loading MAE checkpoint from: \" + str(mae_checkpoint_path))\n",
        "    try:\n",
        "        # Use weights_only=False based on previous error resolution\n",
        "        checkpoint = torch.load(mae_checkpoint_path, map_location='cpu', weights_only=False)\n",
        "        print(\"‚úÖ MAE checkpoint loaded.\")\n",
        "\n",
        "        # Handle different potential state_dict keys\n",
        "        mae_state_dict = checkpoint.get('model_state_dict', checkpoint.get('state_dict', checkpoint.get('model', None)))\n",
        "        if mae_state_dict is None:\n",
        "             print(\"‚ùå MAE state dictionary not found in checkpoint (checked 'model_state_dict', 'state_dict', 'model').\")\n",
        "             return None\n",
        "\n",
        "        encoder_weights = {{}}\n",
        "        filter_prefixes = ['patch_embed', 'pos_embed', 'cls_token', 'blocks', 'norm']\n",
        "        exclude_substrings = ['decoder', 'mask_token', 'head']\n",
        "\n",
        "        print(\"Filtering MAE state dictionary for encoder weights...\")\n",
        "        loaded_keys_count = 0\n",
        "        for mae_key, mae_weight in mae_state_dict.items():\n",
        "            should_include_prefix = False\n",
        "            for prefix in filter_prefixes:\n",
        "                if prefix in mae_key:\n",
        "                    should_include_prefix = True\n",
        "                    break\n",
        "\n",
        "            should_exclude_substring = False\n",
        "            for exclude_str in exclude_substrings:\n",
        "                 if exclude_str in mae_key:\n",
        "                      should_exclude_substring = True\n",
        "                      break\n",
        "\n",
        "            if should_include_prefix and not should_exclude_substring:\n",
        "                encoder_weights[mae_key] = mae_weight\n",
        "                loaded_keys_count += 1\n",
        "\n",
        "        print(f\"üìä Extracted \" + str(len(encoder_weights)) + \" encoder parameters from MAE.\")\n",
        "        return encoder_weights\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading or processing MAE checkpoint: \" + str(e))\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# --- End MAE Loading Logic ---\n",
        "\n",
        "# Function to create MAE-initialized model\n",
        "def create_mae_initialized_model(num_classes, model_name, mae_weights):\n",
        "    model = ViTForFishClassification(\n",
        "        num_classes=num_classes,\n",
        "        model_name=model_name,\n",
        "        pretrained=False,  # Don't use ImageNet if using MAE\n",
        "        dropout_rate=0.1\n",
        "    )\n",
        "\n",
        "    # Initialize updated_keys here to ensure it's always defined\n",
        "    updated_keys = []\n",
        "\n",
        "    if mae_weights is not None:\n",
        "        backbone_state = model.backbone.state_dict()\n",
        "        loaded_count = 0\n",
        "\n",
        "        print(\"Debug: Checking parameter name compatibility...\")\n",
        "        print(f\"First 5 MAE parameter names: \" + str(list(mae_weights.keys())[:5]))\n",
        "        print(f\"First 5 ViT backbone parameter names: \" + str(list(backbone_state.keys())[:5]))\n",
        "\n",
        "        # Handle prefix mapping: MAE uses 'encoder.' prefix, ViT has no prefix\n",
        "        for mae_key, mae_weight in mae_weights.items():\n",
        "             # Create ViT key by removing 'encoder.' prefix from MAE key\n",
        "             if mae_key.startswith('encoder.'):\n",
        "                 vit_key = mae_key[8:]  # Remove 'encoder.' (8 characters)\n",
        "             else:\n",
        "                 vit_key = mae_key  # Use as-is if no encoder prefix\n",
        "\n",
        "             # Check if the mapped key exists in the model's state dict and if shapes match\n",
        "             if vit_key in backbone_state:\n",
        "                 if mae_weight.shape == backbone_state[vit_key].shape:\n",
        "                      backbone_state[vit_key] = mae_weight.clone()\n",
        "                      updated_keys.append(f\"\" + str(mae_key) + \" -> \" + str(vit_key))\n",
        "                      loaded_count += 1\n",
        "                      print(f\"‚úÖ Mapped \" + str(mae_key) + \" -> \" + str(vit_key))\n",
        "                 else:\n",
        "                      print(f\"‚ùå Shape mismatch for \" + str(mae_key) + \" -> \" + str(vit_key) + \": MAE \" + str(mae_weight.shape) + \" vs ViT \" + str(backbone_state[vit_key].shape))\n",
        "             else:\n",
        "                  # Try direct key match (fallback)\n",
        "                  if mae_key in backbone_state:\n",
        "                      if mae_weight.shape == backbone_state[mae_key].shape:\n",
        "                           backbone_state[mae_key] = mae_weight.clone()\n",
        "                           updated_keys.append(mae_key)\n",
        "                           loaded_count += 1\n",
        "                           print(f\"‚úÖ Direct match: \" + str(mae_key))\n",
        "                      else:\n",
        "                           print(f\"‚ùå Shape mismatch for \" + str(mae_key) + \": MAE \" + str(mae_weight.shape) + \" vs ViT \" + str(backbone_state[mae_key].shape))\n",
        "                  else:\n",
        "                       print(f\"‚ö†Ô∏è No match found for \" + str(mae_key) + \" (tried \" + str(vit_key) + \")\")\n",
        "\n",
        "        try:\n",
        "            model.backbone.load_state_dict(backbone_state, strict=False) # strict=False allows skipping mismatched keys\n",
        "            print(f\"‚úÖ Loaded \" + str(len(updated_keys)) + \" MAE encoder weights into model backbone state dict.\")\n",
        "            if len(updated_keys) == 0:\n",
        "                print(\"‚ö†Ô∏è WARNING: No MAE weights were loaded! Check parameter name compatibility.\")\n",
        "                print(\"This means the model is NOT using MAE pretraining.\")\n",
        "            else:\n",
        "                print(\"üéâ MAE pretraining weights successfully loaded!\")\n",
        "                print(f\"üìä Weight mapping summary:\")\n",
        "                for i, mapping in enumerate(updated_keys[:5]):  # Show first 5 mappings\n",
        "                    print(f\"  \" + str(i+1) + \". \" + str(mapping))\n",
        "                if len(updated_keys) > 5:\n",
        "                    print(f\"  ... and \" + str(len(updated_keys) - 5) + \" more\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading MAE weights into ViT backbone: \" + str(e))\n",
        "            print(\"Continuing with potentially partially loaded weights.\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define arguments directly (or parse if needed) - Now using values from parsed args\n",
        "class Args:\n",
        "    def __init__(self, config, resume_from, num_classes):\n",
        "        self.mode = config['mode']\n",
        "        self.data_dir = config['data_dir']\n",
        "        self.epochs = config['epochs']\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.learning_rate = config['learning_rate']\n",
        "        self.weight_decay = config['weight_decay']\n",
        "        self.model_name = config['model_name']\n",
        "        self.consistency_weight = config.get('consistency_weight', 0.1)\n",
        "        self.pseudo_label_threshold = config.get('pseudo_label_threshold', 0.7)\n",
        "        self.temperature = config.get('temperature', 0.7)\n",
        "        self.warmup_epochs = config.get('warmup_epochs', 0)\n",
        "        self.ramp_up_epochs = config.get('ramp_up_epochs', 0)\n",
        "        self.save_dir = config['checkpoint_dir']\n",
        "        self.save_frequency = config['save_frequency']\n",
        "        self.pretrained = config.get('pretrained', True)\n",
        "        self.use_wandb = config.get('use_wandb', False)\n",
        "        self.resume_from = resume_from\n",
        "        self.num_workers = config.get('num_workers', 4)\n",
        "        self.image_size = config.get('image_size', 224)\n",
        "        self.dropout_rate = config.get('dropout_rate', 0.1)\n",
        "        self.num_classes = num_classes\n",
        "        # MAE specific args - get from config\n",
        "        self.mae_model_path = config.get('mae_model_path', None)\n",
        "        self.mae_pretrained = config.get('mae_pretrained', False)\n",
        "\n",
        "# Create args object using values loaded from parsed args\n",
        "args = Args(TRAINING_CONFIG, RESUME_FROM, NUM_CLASSES)\n",
        "\n",
        "# Set up device and seed\n",
        "device = get_device()\n",
        "set_seed(42)\n",
        "\n",
        "# --- Model Creation with MAE Loading within the script ---\n",
        "print('ü§ñ Creating model for training...')\n",
        "\n",
        "# Load MAE weights within this script's process based on args\n",
        "mae_weights_for_init = None\n",
        "if args.mae_pretrained and args.mae_model_path and os.path.exists(args.mae_model_path):\n",
        "    print(f\"Attempting to load MAE pretrained model from: \" + str(args.mae_model_path))\n",
        "    mae_weights_for_init = load_mae_encoder_weights(args.mae_model_path)\n",
        "    if mae_weights_for_init is not None:\n",
        "        print(\"üéâ MAE encoder weights loaded successfully within script!\")\n",
        "    else:\n",
        "        print(\"‚ùå Failed to load MAE encoder weights within script. Falling back to ImageNet.\")\n",
        "else:\n",
        "     print(\"‚è≠Ô∏è Skipping MAE loading (config disabled, path missing, or file not found).\")\n",
        "     print(f\"üåê Will use ImageNet pretrained weights if args.pretrained is True (config value: \" + str(args.pretrained) + \").\")\n",
        "\n",
        "# Create model, using MAE weights if successfully loaded, otherwise use original pretrained flag\n",
        "if mae_weights_for_init is not None:\n",
        "     student_model = create_mae_initialized_model(\n",
        "         num_classes=args.num_classes,\n",
        "         model_name=args.model_name,\n",
        "         mae_weights=mae_weights_for_init # Pass the loaded MAE weights\n",
        "     ).to(device)\n",
        "     # Ensure pretrained flag is False if MAE is used\n",
        "     args.pretrained = False\n",
        "     print(\"‚úÖ Student model created and initialized with MAE weights.\")\n",
        "\n",
        "else: # Fallback to original pretrained flag (likely ImageNet)\n",
        "     student_model = ViTForFishClassification(\n",
        "        num_classes=args.num_classes,\n",
        "        model_name=args.model_name,\n",
        "        pretrained=args.pretrained, # Use the original pretrained flag\n",
        "        dropout_rate=args.dropout_rate\n",
        "     ).to(device)\n",
        "     print(f\"‚úÖ Student model created. Using ImageNet pretrained: \" + str(args.pretrained))\n",
        "\n",
        "# Create teacher model for EMA (if needed)\n",
        "teacher_model = None\n",
        "if args.mode == 'semi_supervised':\n",
        "    try:\n",
        "        from trainer import EMATeacher # Import EMATeacher here if needed\n",
        "        teacher_model = EMATeacher(student_model) # Create EMATeacher instance\n",
        "        # Check if teacher_model has an internal teacher_model attribute before calling .to()\n",
        "        if hasattr(teacher_model, 'teacher_model') and isinstance(teacher_model.teacher_model, torch.nn.Module):\n",
        "             teacher_model.teacher_model.to(device) # Move the internal teacher_model to device\n",
        "             print('üéì EMA Teacher model created and its internal model moved to device.')\n",
        "        else:\n",
        "             print(\"‚ö†Ô∏è EMATeacher's internal model could not be moved to device.\")\n",
        "\n",
        "    except NameError:\n",
        "         print(\"‚ö†Ô∏è EMATeacher class not found. Semi-supervised training will not work correctly.\")\n",
        "         print(\"‚ùå Please ensure EMATeacher is defined or imported in trainer.py\")\n",
        "         sys.exit(1) # Exit if EMATeacher is needed but not found\n",
        "    except Exception as e:\n",
        "         print(f\"‚ö†Ô∏è Error creating or moving EMATeacher model: \" + str(e))\n",
        "         import traceback\n",
        "         traceback.print_exc()\n",
        "         sys.exit(1) # Exit on other errors\n",
        "elif args.mode == 'supervised':\n",
        "    # Create teacher model for supervised mode as well\n",
        "    try:\n",
        "        from trainer import EMATeacher\n",
        "        teacher_model = EMATeacher(student_model)\n",
        "        if hasattr(teacher_model, 'teacher_model') and isinstance(teacher_model.teacher_model, torch.nn.Module):\n",
        "             teacher_model.teacher_model.to(device)\n",
        "             print('üéì EMA Teacher model created for supervised mode.')\n",
        "        else:\n",
        "             print(\"‚ö†Ô∏è EMATeacher's internal model could not be moved to device.\")\n",
        "    except Exception as e:\n",
        "         print(f\"‚ö†Ô∏è Error creating EMATeacher for supervised mode: \" + str(e))\n",
        "         # Continue without teacher model for supervised mode\n",
        "         teacher_model = None\n",
        "\n",
        "# Create data loaders\n",
        "print('Loading data...')\n",
        "if args.mode == 'supervised':\n",
        "    train_loader, val_loader, num_classes_data = create_dataloaders(\n",
        "        args.data_dir,\n",
        "        batch_size=args.batch_size,\n",
        "        image_size=args.image_size,\n",
        "        num_workers=args.num_workers\n",
        "    )\n",
        "    unlabeled_loader = None\n",
        "    test_loader = None # Initialize test_loader for supervised mode\n",
        "else: # semi_supervised mode\n",
        "    # Fixed to unpack the correct 6 values from create_semi_supervised_dataloaders\n",
        "    train_loader, val_loader, test_loader, class_names, labeled_count, unlabeled_count = create_semi_supervised_dataloaders(\n",
        "        args.data_dir,\n",
        "        batch_size=args.batch_size,\n",
        "        image_size=args.image_size,\n",
        "        num_workers=args.num_workers\n",
        "    )\n",
        "    # For semi-supervised mode, unlabeled data is included in train_loader\n",
        "    unlabeled_loader = None  # Not needed - unlabeled data is in train_loader\n",
        "    num_classes_data = len(class_names)  # Calculate from class_names\n",
        "print('‚úÖ Data loaders created.')\n",
        "\n",
        "if num_classes_data != args.num_classes:\n",
        "     print(f\"‚ö†Ô∏è Warning: Configured num_classes (\" + str(args.num_classes) + \") does not match detected data classes (\" + str(num_classes_data) + \")\")\n",
        "     # Use the detected number of classes if they differ\n",
        "     args.num_classes = num_classes_data\n",
        "     print(f\"‚úÖ Using \" + str(args.num_classes) + \" detected classes for training.\")\n",
        "\n",
        "print(f'üìä Number of classes: ' + str(args.num_classes))\n",
        "print(f'üéØ Training mode: ' + str(args.mode))\n",
        "\n",
        "# Create trainer\n",
        "print('Setting up trainer...')\n",
        "# Updated trainer creation logic since unlabeled_loader is now None\n",
        "if args.mode == 'semi_supervised' and teacher_model is not None:\n",
        "    trainer = SemiSupervisedTrainer(\n",
        "        student_model=student_model,\n",
        "        ema_teacher=teacher_model,  # Fixed: use ema_teacher parameter name\n",
        "        num_classes=args.num_classes,\n",
        "        device=device,\n",
        "        learning_rate=args.learning_rate,\n",
        "        weight_decay=args.weight_decay,\n",
        "        consistency_weight=args.consistency_weight,\n",
        "        pseudo_label_threshold=args.pseudo_label_threshold,\n",
        "        temperature=args.temperature,\n",
        "        warmup_epochs=args.warmup_epochs,\n",
        "        ramp_up_epochs=args.ramp_up_epochs,\n",
        "        use_wandb=args.use_wandb  # Pass wandb setting to trainer\n",
        "    )\n",
        "    print('‚úÖ SemiSupervisedTrainer created.')\n",
        "elif args.mode == 'supervised' and train_loader is not None and val_loader is not None:\n",
        "    if teacher_model is not None:\n",
        "        trainer = EMATrainer( # Using EMATrainer for supervised mode\n",
        "            student_model=student_model,\n",
        "            ema_teacher=teacher_model,\n",
        "            num_classes=args.num_classes,\n",
        "            device=device,\n",
        "            learning_rate=args.learning_rate,\n",
        "            weight_decay=args.weight_decay,\n",
        "            use_wandb=args.use_wandb  # Pass wandb setting to trainer\n",
        "        )\n",
        "        print('‚úÖ EMATrainer created (for supervised mode).')\n",
        "    else:\n",
        "        # Create a simple trainer without EMA if teacher model creation failed\n",
        "        print(\"‚ö†Ô∏è Creating simple trainer without EMA for supervised mode.\")\n",
        "        # We'll need to use SemiSupervisedTrainer with a dummy teacher for simplicity\n",
        "        from trainer import EMATeacher\n",
        "        dummy_teacher = EMATeacher(student_model)\n",
        "        dummy_teacher.teacher_model.to(device)\n",
        "        trainer = SemiSupervisedTrainer(\n",
        "            student_model=student_model,\n",
        "            ema_teacher=dummy_teacher,\n",
        "            num_classes=args.num_classes,\n",
        "            device=device,\n",
        "            learning_rate=args.learning_rate,\n",
        "            weight_decay=args.weight_decay,\n",
        "            consistency_weight=0.0,  # Set to 0 for supervised mode\n",
        "            pseudo_label_threshold=1.0,  # High threshold to avoid pseudo-labels\n",
        "            temperature=args.temperature,\n",
        "            warmup_epochs=args.warmup_epochs,\n",
        "            ramp_up_epochs=args.ramp_up_epochs,\n",
        "            use_wandb=args.use_wandb  # Pass wandb setting to trainer\n",
        "        )\n",
        "        print('‚úÖ SemiSupervisedTrainer created for supervised mode (consistency_weight=0).')\n",
        "else:\n",
        "     print(\"‚ùå Cannot create trainer. Check mode and data loaders.\")\n",
        "     sys.exit(1)\n",
        "\n",
        "# Initialize W&B\n",
        "if args.use_wandb:\n",
        "    print('Initializing W&B...')\n",
        "    try:\n",
        "        wandb.init(\n",
        "            project=TRAINING_CONFIG.get('wandb_project', 'ViT-FishID-Training'),\n",
        "            name=TRAINING_CONFIG.get('wandb_run_name', f'fish-classification-' + str(args.num_classes) + '-classes'),\n",
        "            config=vars(args),\n",
        "            tags=['mae-initialized', 'fish-classification'] if args.mae_pretrained else ['imagenet-pretrained', 'fish-classification']\n",
        "        )\n",
        "        print('‚úÖ W&B initialized.')\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Failed to initialize W&B: \" + str(e))\n",
        "        print(\"Disabling W&B logging for this run.\")\n",
        "        args.use_wandb = False\n",
        "        # Update trainer's wandb setting if it exists\n",
        "        if hasattr(trainer, 'use_wandb'):\n",
        "            trainer.use_wandb = False\n",
        "else:\n",
        "    print('üìä W&B logging disabled.')\n",
        "\n",
        "# Resume from checkpoint if specified\n",
        "if args.resume_from and args.resume_from != 'None':\n",
        "    print(f'üì• Resuming from checkpoint: ' + str(args.resume_from))\n",
        "    try:\n",
        "        checkpoint = torch.load(args.resume_from, map_location=device)\n",
        "        trainer.student_model.load_state_dict(checkpoint['student_state_dict'])\n",
        "        # Load teacher state dict if it exists and trainer has a teacher model\n",
        "        if hasattr(trainer, 'teacher_model') and trainer.teacher_model is not None and 'teacher_state_dict' in checkpoint:\n",
        "             try:\n",
        "                trainer.teacher_model.teacher_model.load_state_dict(checkpoint['teacher_state_dict'])\n",
        "                print('‚úÖ Teacher model state dict loaded.')\n",
        "             except Exception as e:\n",
        "                 print(f\"‚ö†Ô∏è Error loading teacher state dict: \" + str(e))\n",
        "\n",
        "        # Load optimizer state dict\n",
        "        if 'optimizer_state_dict' in checkpoint:\n",
        "             try:\n",
        "                 trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                 print('‚úÖ Optimizer state dict loaded.')\n",
        "             except Exception as e:\n",
        "                  print(f\"‚ö†Ô∏è Error loading optimizer state dict: \" + str(e))\n",
        "\n",
        "        start_epoch = checkpoint.get('epoch', 0) + 1\n",
        "        best_accuracy = checkpoint.get('best_accuracy', 0.0)\n",
        "        print(f'‚úÖ Resumed from epoch ' + str(start_epoch) + ' with best accuracy ' + str(best_accuracy) + '%')\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Error loading checkpoint: ' + str(e))\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"Starting fresh training from epoch 1.\")\n",
        "        start_epoch = 1\n",
        "        best_accuracy = 0.0\n",
        "else:\n",
        "    start_epoch = 1\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "print(f'üöÄ Starting training from epoch ' + str(start_epoch))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, args.epochs + 1):\n",
        "    print(f'\\\\nüìÖ Epoch ' + str(epoch) + '/' + str(args.epochs))\n",
        "\n",
        "    # Training step - Updated for semi-supervised mode\n",
        "    try:\n",
        "        if args.mode == 'semi_supervised':\n",
        "            # For semi-supervised, train_loader already contains both labeled and unlabeled data\n",
        "            train_loss = trainer.train_epoch(train_loader, epoch)\n",
        "        elif args.mode == 'supervised':\n",
        "            train_loss = trainer.train_epoch(train_loader, epoch)\n",
        "        else:\n",
        "             print(\"‚ùå Invalid training mode for training epoch.\")\n",
        "             break # Exit training loop if setup is wrong\n",
        "    except Exception as e:\n",
        "         print(f\"‚ùå Error during training epoch \" + str(epoch) + \": \" + str(e))\n",
        "         import traceback\n",
        "         traceback.print_exc()\n",
        "         break # Exit training loop on error\n",
        "\n",
        "    # Validation step - FIXED to handle correct dictionary keys from SemiSupervisedTrainer\n",
        "    try:\n",
        "        val_result = trainer.validate(val_loader)\n",
        "        # Handle both dictionary and float returns - FIXED key names to use 'top1_accuracy'\n",
        "        if isinstance(val_result, dict):\n",
        "            # SemiSupervisedTrainer returns 'top1_accuracy' as the key\n",
        "            val_accuracy = val_result.get('top1_accuracy', val_result.get('accuracy', val_result.get('val_accuracy', 0.0)))\n",
        "        else:\n",
        "            val_accuracy = float(val_result)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during validation epoch \" + str(epoch) + \": \" + str(e))\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        val_accuracy = 0.0 # Set accuracy to 0 to avoid saving best model on error\n",
        "\n",
        "    # Update best accuracy\n",
        "    is_best = val_accuracy > best_accuracy\n",
        "    if is_best:\n",
        "        best_accuracy = val_accuracy\n",
        "\n",
        "    print(f'üìä Epoch ' + str(epoch) + ' - Train Loss: ' + str(train_loss) + ', Val Acc: ' + str(val_accuracy) + '% (Best: ' + str(best_accuracy) + '%)')\n",
        "\n",
        "    # Save checkpoint\n",
        "    if epoch % args.save_frequency == 0 or is_best:\n",
        "        print(f'üíæ Saving checkpoint for epoch ' + str(epoch) + '...')\n",
        "        checkpoint_data = {{\n",
        "            'epoch': epoch,\n",
        "            'student_state_dict': trainer.student_model.state_dict(),\n",
        "            'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
        "            'best_accuracy': best_accuracy,\n",
        "            'train_loss': train_loss,\n",
        "            'val_accuracy': val_accuracy\n",
        "        }}\n",
        "\n",
        "        if hasattr(trainer, 'teacher_model') and trainer.teacher_model is not None:\n",
        "            try:\n",
        "                checkpoint_data['teacher_state_dict'] = trainer.teacher_model.teacher_model.state_dict()\n",
        "                checkpoint_data['teacher_acc'] = getattr(trainer, 'teacher_accuracy', val_accuracy)\n",
        "            except Exception as e:\n",
        "                 print(f\"‚ö†Ô∏è Could not save teacher state dict: \" + str(e))\n",
        "\n",
        "        # Ensure save directory exists\n",
        "        os.makedirs(args.save_dir, exist_ok=True)\n",
        "\n",
        "        # Save regular checkpoint\n",
        "        if epoch % args.save_frequency == 0:\n",
        "            checkpoint_path = os.path.join(args.save_dir, f'checkpoint_epoch_' + str(epoch) + '.pth')\n",
        "            try:\n",
        "                torch.save(checkpoint_data, checkpoint_path)\n",
        "                print(f'‚úÖ Saved checkpoint: ' + str(checkpoint_path))\n",
        "            except Exception as e:\n",
        "                 print(f\"‚ùå Error saving checkpoint \" + str(checkpoint_path) + \": \" + str(e))\n",
        "\n",
        "        # Save best model\n",
        "        if is_best:\n",
        "            best_path = os.path.join(args.save_dir, 'model_best.pth')\n",
        "            try:\n",
        "                torch.save(checkpoint_data, best_path)\n",
        "                print(f'üèÜ New best model saved: ' + str(best_path))\n",
        "            except Exception as e:\n",
        "                 print(f\"‚ùå Error saving best model \" + str(best_path) + \": \" + str(e))\n",
        "\n",
        "    # W&B logging\n",
        "    if args.use_wandb:\n",
        "        try:\n",
        "            wandb.log({{\n",
        "                'epoch': epoch,\n",
        "                'train_loss': train_loss,\n",
        "                'val_accuracy': val_accuracy,\n",
        "                'best_accuracy': best_accuracy\n",
        "            }})\n",
        "        except Exception as e:\n",
        "             print(f\"‚ö†Ô∏è Error logging to W&B at epoch \" + str(epoch) + \": \" + str(e))\n",
        "\n",
        "print(f'\\\\nüéâ Training completed!')\n",
        "print(f'üèÜ Best accuracy: ' + str(best_accuracy) + '%')\n",
        "\n",
        "if args.use_wandb:\n",
        "    try:\n",
        "        wandb.finish()\n",
        "    except Exception as e:\n",
        "         print(f\"‚ö†Ô∏è Error finishing W&B run: \" + str(e))\n",
        "\n",
        "\"\"\"\n",
        "    # Write the script to a temporary file\n",
        "    script_filename = '/content/run_mae_training.py'\n",
        "    with open(script_filename, 'w') as f:\n",
        "        f.write(training_script_content)\n",
        "\n",
        "    # Execute the temporary script, passing config as a JSON string argument\n",
        "    # Use shlex.quote to handle potential special characters in the JSON string\n",
        "    import shlex\n",
        "    quoted_config_json = shlex.quote(training_config_json)\n",
        "\n",
        "    training_cmd = f\"python {script_filename} --config_json {quoted_config_json} --num_classes_arg {num_classes_str} --resume_from_arg {resume_from_str}\"\n",
        "\n",
        "else:\n",
        "    # Build standard training command without MAE\n",
        "    training_cmd = f\"\"\"python train.py \\\\\n",
        "    --mode {TRAINING_CONFIG['mode']} \\\\\n",
        "    --data_dir {TRAINING_CONFIG['data_dir']} \\\\\n",
        "    --epochs {TRAINING_CONFIG['epochs']} \\\\\n",
        "    --batch_size {TRAINING_CONFIG['batch_size']} \\\\\n",
        "    --learning_rate {TRAINING_CONFIG['learning_rate']} \\\\\n",
        "    --weight_decay {TRAINING_CONFIG['weight_decay']} \\\\\n",
        "    --model_name {TRAINING_CONFIG['model_name']} \\\\\n",
        "    --consistency_weight {TRAINING_CONFIG.get('consistency_weight', 0.1)} \\\\\n",
        "    --pseudo_label_threshold {TRAINING_CONFIG.get('pseudo_label_threshold', 0.7)} \\\\\n",
        "    --temperature {TRAINING_CONFIG.get('temperature', 0.7)} \\\\\n",
        "    --warmup_epochs {TRAINING_CONFIG.get('warmup_epochs', 0)} \\\\\n",
        "    --ramp_up_epochs {TRAINING_CONFIG.get('ramp_up_epochs', 0)} \\\\\n",
        "    --save_dir {TRAINING_CONFIG['checkpoint_dir']} \\\\\n",
        "    --save_frequency {TRAINING_CONFIG['save_frequency']}\"\"\"\n",
        "\n",
        "    # Add resume checkpoint if found\n",
        "    if RESUME_FROM:\n",
        "        training_cmd += f\" \\\\\\n    --resume_from {RESUME_FROM}\"\n",
        "\n",
        "    # Add pretrained flag\n",
        "    if TRAINING_CONFIG.get('pretrained', True):\n",
        "        training_cmd += \" \\\\\\n    --pretrained\"\n",
        "\n",
        "    # Add W&B logging\n",
        "    if TRAINING_CONFIG.get('use_wandb', False):\n",
        "        training_cmd += \" \\\\\\n    --use_wandb\"\n",
        "\n",
        "print(\"üìã TRAINING CONFIGURATION:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üéØ Training {NUM_CLASSES} fish species\")\n",
        "print(f\"üìä Mode: {TRAINING_CONFIG['mode']}\")\n",
        "print(f\"ü§ñ MAE pretrained: {TRAINING_CONFIG.get('mae_pretrained', False)}\")\n",
        "print(f\"üåê ImageNet pretrained: {TRAINING_CONFIG.get('pretrained', True)}\")\n",
        "\n",
        "if RESUME_FROM:\n",
        "    print(f\"üîÑ Resuming from: {os.path.basename(RESUME_FROM)}\")\n",
        "else:\n",
        "    print(f\"üÜï Starting fresh training\")\n",
        "\n",
        "print(f\"‚è±Ô∏è Estimated time: {TRAINING_CONFIG['epochs'] * 3 / 60:.1f} hours\")\n",
        "print(f\"üíæ Checkpoints: {TRAINING_CONFIG['checkpoint_dir']}\")\n",
        "print(f\"üìà W&B logging: {TRAINING_CONFIG['use_wandb']}\")\n",
        "\n",
        "if TRAINING_CONFIG.get('mae_pretrained', False):\n",
        "    print(f\"üéâ Using MAE-learned features from: {os.path.basename(TRAINING_CONFIG.get('mae_model_path', ''))}\")\n",
        "    print(f\"üöÄ This should significantly improve training performance!\")\n",
        "\n",
        "print(f\"\\nüé¨ TRAINING STARTED\")\n",
        "print(\"‚è∞ Started at:\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "# Execute training\n",
        "# Use PYTHONPATH to help the executed script find local modules\n",
        "!PYTHONPATH=/content/ViT-FishID {training_cmd}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ TRAINING COMPLETED!\")\n",
        "print(\"‚è∞ Finished at:\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "# Check for results\n",
        "best_model_path = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'model_best.pth')\n",
        "if os.path.exists(best_model_path):\n",
        "    try:\n",
        "        import torch\n",
        "        checkpoint = torch.load(best_model_path, map_location='cpu')\n",
        "        if 'best_accuracy' in checkpoint:\n",
        "            print(f\"üèÜ Best accuracy achieved: {checkpoint['best_accuracy']:.2f}%\")\n",
        "        if 'epoch' in checkpoint:\n",
        "            print(f\"üìä Best model from epoch: {checkpoint['epoch']}\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"‚úÖ Your MAE-enhanced model is ready for evaluation and deployment!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jCRz1dcRGfmy",
      "metadata": {
        "id": "jCRz1dcRGfmy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5af5177",
      "metadata": {
        "id": "b5af5177"
      },
      "source": [
        "## üìä Step 9: Check Training Results\n",
        "\n",
        "Review the training progress and model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "87ea96e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87ea96e8",
        "outputId": "bb76ef71-55d3-415d-e817-8a12b80a780f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä CHECKING TRAINING RESULTS\n",
            "==================================================\n",
            "üìÅ Checkpoint directory: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints\n",
            "‚úÖ Found 11 checkpoint(s)\n",
            "\n",
            "üìà TRAINING PROGRESSION:\n",
            "  üèÅ Latest epoch: 100\n",
            "  üìä Completion: 100/100 epochs (100.0%)\n",
            "  üìÑ Epoch 60: 248.3 MB\n",
            "  üìÑ Epoch 70: 248.3 MB\n",
            "  üìÑ Epoch 80: 248.3 MB\n",
            "  üìÑ Epoch 90: 248.3 MB\n",
            "  üìÑ Epoch 100: 248.3 MB\n",
            "\n",
            "üèÜ BEST MODEL ANALYSIS:\n",
            "  üìä Best epoch: 88\n",
            "  üéØ Best accuracy: 58.02%\n",
            "  ‚ö†Ô∏è LOW performance - check data and hyperparameters\n",
            "  üìÑ model_best.pth: 248.3 MB\n",
            "\n",
            "‚úÖ Results check complete!\n"
          ]
        }
      ],
      "source": [
        "# Check Training Results and Performance\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"üìä CHECKING TRAINING RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "checkpoint_dir = TRAINING_CONFIG['checkpoint_dir']\n",
        "print(f\"üìÅ Checkpoint directory: {checkpoint_dir}\")\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    # Find all checkpoints\n",
        "    checkpoints = glob.glob(os.path.join(checkpoint_dir, '*.pth'))\n",
        "\n",
        "    if checkpoints:\n",
        "        print(f\"‚úÖ Found {len(checkpoints)} checkpoint(s)\")\n",
        "\n",
        "        # Sort checkpoints by epoch\n",
        "        epoch_checkpoints = []\n",
        "        other_checkpoints = []\n",
        "\n",
        "        for cp in checkpoints:\n",
        "            basename = os.path.basename(cp)\n",
        "            if 'epoch_' in basename:\n",
        "                try:\n",
        "                    epoch_num = int(basename.split('epoch_')[1].split('.')[0])\n",
        "                    epoch_checkpoints.append((epoch_num, cp))\n",
        "                except:\n",
        "                    other_checkpoints.append(cp)\n",
        "            else:\n",
        "                other_checkpoints.append(cp)\n",
        "\n",
        "        # Show epoch progression\n",
        "        if epoch_checkpoints:\n",
        "            epoch_checkpoints.sort(key=lambda x: x[0])\n",
        "            print(f\"\\nüìà TRAINING PROGRESSION:\")\n",
        "            latest_epoch = epoch_checkpoints[-1][0]\n",
        "            print(f\"  üèÅ Latest epoch: {latest_epoch}\")\n",
        "            print(f\"  üìä Completion: {latest_epoch}/{TRAINING_CONFIG['epochs']} epochs ({latest_epoch/TRAINING_CONFIG['epochs']*100:.1f}%)\")\n",
        "\n",
        "            # Show recent checkpoints\n",
        "            recent_checkpoints = epoch_checkpoints[-5:] if len(epoch_checkpoints) > 5 else epoch_checkpoints\n",
        "            for epoch, cp in recent_checkpoints:\n",
        "                file_size = os.path.getsize(cp) / (1024**2)\n",
        "                print(f\"  üìÑ Epoch {epoch}: {file_size:.1f} MB\")\n",
        "\n",
        "        # Analyze best model\n",
        "        best_model_path = os.path.join(checkpoint_dir, 'model_best.pth')\n",
        "        if os.path.exists(best_model_path):\n",
        "            print(f\"\\nüèÜ BEST MODEL ANALYSIS:\")\n",
        "            try:\n",
        "                best_checkpoint = torch.load(best_model_path, map_location='cpu')\n",
        "\n",
        "                best_epoch = best_checkpoint.get('epoch', 'Unknown')\n",
        "                best_acc = best_checkpoint.get('best_accuracy', best_checkpoint.get('best_acc', 'Unknown'))\n",
        "\n",
        "                print(f\"  üìä Best epoch: {best_epoch}\")\n",
        "                if isinstance(best_acc, (int, float)):\n",
        "                    print(f\"  üéØ Best accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "                    # Performance assessment\n",
        "                    if best_acc >= 85:\n",
        "                        print(\"  üéâ EXCELLENT performance!\")\n",
        "                    elif best_acc >= 75:\n",
        "                        print(\"  üëç GOOD performance!\")\n",
        "                    elif best_acc >= 65:\n",
        "                        print(\"  üìà FAIR performance - consider more training\")\n",
        "                    else:\n",
        "                        print(\"  ‚ö†Ô∏è LOW performance - check data and hyperparameters\")\n",
        "\n",
        "                # Check for other metrics\n",
        "                if 'teacher_acc' in best_checkpoint:\n",
        "                    print(f\"  üéì Teacher accuracy: {best_checkpoint['teacher_acc']:.2f}%\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ö†Ô∏è Could not analyze best model: {e}\")\n",
        "\n",
        "        # Show other important files\n",
        "        for cp in other_checkpoints:\n",
        "            basename = os.path.basename(cp)\n",
        "            file_size = os.path.getsize(cp) / (1024**2)\n",
        "            print(f\"  üìÑ {basename}: {file_size:.1f} MB\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå No checkpoints found\")\n",
        "        print(\"üí° Training may not have started or completed successfully\")\n",
        "\n",
        "else:\n",
        "    print(f\"‚ùå Checkpoint directory not found: {checkpoint_dir}\")\n",
        "\n",
        "# W&B results link\n",
        "if TRAINING_CONFIG['use_wandb']:\n",
        "    print(f\"\\nüìà View detailed training metrics at:\")\n",
        "    print(f\"   https://wandb.ai/your-username/{TRAINING_CONFIG['wandb_project']}\")\n",
        "\n",
        "print(\"\\n‚úÖ Results check complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff698a72",
      "metadata": {
        "id": "ff698a72"
      },
      "source": [
        "## üíæ Step 10: Save Model and Results\n",
        "\n",
        "Backup your trained model and results to Google Drive for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "89513455",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89513455",
        "outputId": "97e8c3d0-a578-4200-d572-339de8da5440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ SAVING MODEL AND RESULTS\n",
            "==================================================\n",
            "üìÅ Created backup directory: /content/drive/MyDrive/ViT-FishID_Results_20250818_161342\n",
            "‚úÖ Checkpoints copied to: /content/drive/MyDrive/ViT-FishID_Results_20250818_161342/checkpoints\n",
            "üìä Backed up 11 checkpoint files\n",
            "‚úÖ Training config saved: /content/drive/MyDrive/ViT-FishID_Results_20250818_161342/training_config.json\n",
            "‚úÖ Training summary saved: /content/drive/MyDrive/ViT-FishID_Results_20250818_161342/training_summary.txt\n",
            "üèÜ Final model accuracy: 58.02%\n",
            "\n",
            "üéâ ALL RESULTS SAVED SUCCESSFULLY!\n",
            "üìÅ Backup location: /content/drive/MyDrive/ViT-FishID_Results_20250818_161342\n",
            "\n",
            "üí° You can now:\n",
            "   1. Download the entire results folder\n",
            "   2. Use model_best.pth for inference\n",
            "   3. Resume training from any checkpoint\n",
            "   4. Share results with collaborators\n"
          ]
        }
      ],
      "source": [
        "# Save trained model and results to Google Drive\n",
        "import shutil\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"üíæ SAVING MODEL AND RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create timestamped backup directory\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "backup_dir = f'/content/drive/MyDrive/ViT-FishID_Results_{timestamp}'\n",
        "\n",
        "try:\n",
        "    os.makedirs(backup_dir, exist_ok=True)\n",
        "    print(f\"üìÅ Created backup directory: {backup_dir}\")\n",
        "\n",
        "    # Copy checkpoints\n",
        "    checkpoint_source = TRAINING_CONFIG['checkpoint_dir']\n",
        "    if os.path.exists(checkpoint_source):\n",
        "        checkpoint_backup = os.path.join(backup_dir, 'checkpoints')\n",
        "        shutil.copytree(checkpoint_source, checkpoint_backup, dirs_exist_ok=True)\n",
        "        print(f\"‚úÖ Checkpoints copied to: {checkpoint_backup}\")\n",
        "\n",
        "        # Count files\n",
        "        checkpoint_files = len([f for f in os.listdir(checkpoint_backup) if f.endswith('.pth')])\n",
        "        print(f\"üìä Backed up {checkpoint_files} checkpoint files\")\n",
        "\n",
        "    # Save training configuration\n",
        "    config_file = os.path.join(backup_dir, 'training_config.json')\n",
        "    serializable_config = {k: v for k, v in TRAINING_CONFIG.items()\n",
        "                          if isinstance(v, (str, int, float, bool, list, dict, type(None)))}\n",
        "\n",
        "    with open(config_file, 'w') as f:\n",
        "        json.dump(serializable_config, f, indent=2)\n",
        "    print(f\"‚úÖ Training config saved: {config_file}\")\n",
        "\n",
        "    # Create training summary\n",
        "    summary_file = os.path.join(backup_dir, 'training_summary.txt')\n",
        "    with open(summary_file, 'w') as f:\n",
        "        f.write(f\"ViT-FishID Training Summary\\n\")\n",
        "        f.write(f\"========================\\n\\n\")\n",
        "        f.write(f\"Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Training Mode: {TRAINING_CONFIG['mode']}\\n\")\n",
        "        f.write(f\"Total Epochs: {TRAINING_CONFIG['epochs']}\\n\")\n",
        "        f.write(f\"Batch Size: {TRAINING_CONFIG['batch_size']}\\n\")\n",
        "        f.write(f\"Model: {TRAINING_CONFIG['model_name']}\\n\")\n",
        "        f.write(f\"Number of Species: {TRAINING_CONFIG['num_classes']}\\n\")\n",
        "        f.write(f\"Consistency Weight: {TRAINING_CONFIG['consistency_weight']}\\n\")\n",
        "        f.write(f\"W&B Logging: {TRAINING_CONFIG['use_wandb']}\\n\\n\")\n",
        "        f.write(f\"Key Files:\\n\")\n",
        "        f.write(f\"- model_best.pth: Best performing model\\n\")\n",
        "        f.write(f\"- model_latest.pth: Most recent checkpoint\\n\")\n",
        "        f.write(f\"- checkpoint_epoch_X.pth: Periodic saves\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Training summary saved: {summary_file}\")\n",
        "\n",
        "    # Get final model performance\n",
        "    best_model_path = os.path.join(checkpoint_source, 'model_best.pth')\n",
        "    if os.path.exists(best_model_path):\n",
        "        try:\n",
        "            import torch\n",
        "            checkpoint = torch.load(best_model_path, map_location='cpu')\n",
        "            if 'best_accuracy' in checkpoint:\n",
        "                print(f\"üèÜ Final model accuracy: {checkpoint['best_accuracy']:.2f}%\")\n",
        "\n",
        "                # Add performance to summary\n",
        "                with open(summary_file, 'a') as f:\n",
        "                    f.write(f\"\\nFinal Performance:\\n\")\n",
        "                    f.write(f\"- Best Accuracy: {checkpoint['best_accuracy']:.2f}%\\n\")\n",
        "                    f.write(f\"- Best Epoch: {checkpoint.get('epoch', 'Unknown')}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not read final performance: {e}\")\n",
        "\n",
        "    print(f\"\\nüéâ ALL RESULTS SAVED SUCCESSFULLY!\")\n",
        "    print(f\"üìÅ Backup location: {backup_dir}\")\n",
        "    print(f\"\\nüí° You can now:\")\n",
        "    print(f\"   1. Download the entire results folder\")\n",
        "    print(f\"   2. Use model_best.pth for inference\")\n",
        "    print(f\"   3. Resume training from any checkpoint\")\n",
        "    print(f\"   4. Share results with collaborators\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error saving results: {e}\")\n",
        "    print(\"üí° Please check Google Drive permissions and available space\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bbc6396",
      "metadata": {
        "id": "3bbc6396"
      },
      "source": [
        "## üß™ Step 11: Model Evaluation (Optional)\n",
        "\n",
        "Test your trained model on sample images and get detailed performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "642c1e93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "642c1e93",
        "outputId": "a43fdbf4-76ad-4541-c376-6774dc92ab23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ MODEL EVALUATION\n",
            "==================================================\n",
            "‚úÖ Found trained model: model_best.pth\n",
            "\n",
            "üìä MODEL PERFORMANCE:\n",
            "  üèÜ Best epoch: 88\n",
            "  üéØ Best accuracy: 58.02%\n",
            "  üìè Model size: 248.3 MB\n",
            "\n",
            "‚ö†Ô∏è PERFORMANCE NEEDS IMPROVEMENT\n",
            "   Review data quality and training configuration\n",
            "\n",
            "üöÄ NEXT STEPS:\n",
            "1. üß™ Run detailed evaluation: Use evaluate.py script\n",
            "2. üî¨ Test on new images: Upload test images and run inference\n",
            "3. üì± Deploy model: Use for real-world fish classification\n",
            "4. üìä Analyze results: Review confusion matrix and per-species performance\n",
            "5. üîÑ Continue training: Resume from checkpoints for more epochs\n",
            "\n",
            "‚úÖ Evaluation complete!\n"
          ]
        }
      ],
      "source": [
        "# Quick model evaluation and testing\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"üß™ MODEL EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Check for trained model\n",
        "best_model_path = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'model_best.pth')\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    print(f\"‚úÖ Found trained model: {os.path.basename(best_model_path)}\")\n",
        "\n",
        "    try:\n",
        "        # Load model checkpoint\n",
        "        checkpoint = torch.load(best_model_path, map_location='cpu')\n",
        "\n",
        "        print(f\"\\nüìä MODEL PERFORMANCE:\")\n",
        "        if 'epoch' in checkpoint:\n",
        "            print(f\"  üèÜ Best epoch: {checkpoint['epoch']}\")\n",
        "        if 'best_accuracy' in checkpoint:\n",
        "            print(f\"  üéØ Best accuracy: {checkpoint['best_accuracy']:.2f}%\")\n",
        "        if 'teacher_acc' in checkpoint:\n",
        "            print(f\"  üéì Teacher accuracy: {checkpoint['teacher_acc']:.2f}%\")\n",
        "\n",
        "        # Model architecture info\n",
        "        if 'num_classes' in checkpoint:\n",
        "            print(f\"  üêü Number of species: {checkpoint['num_classes']}\")\n",
        "\n",
        "        # File size\n",
        "        file_size = os.path.getsize(best_model_path) / (1024**2)\n",
        "        print(f\"  üìè Model size: {file_size:.1f} MB\")\n",
        "\n",
        "        # Performance assessment\n",
        "        if 'best_accuracy' in checkpoint:\n",
        "            accuracy = checkpoint['best_accuracy']\n",
        "            if accuracy >= 85:\n",
        "                print(f\"\\nüéâ EXCELLENT PERFORMANCE!\")\n",
        "                print(f\"   Your model achieved outstanding accuracy for fish classification\")\n",
        "            elif accuracy >= 75:\n",
        "                print(f\"\\nüëç GOOD PERFORMANCE!\")\n",
        "                print(f\"   Your model shows solid accuracy for practical use\")\n",
        "            elif accuracy >= 65:\n",
        "                print(f\"\\nüìà FAIR PERFORMANCE\")\n",
        "                print(f\"   Consider additional training or hyperparameter tuning\")\n",
        "            else:\n",
        "                print(f\"\\n‚ö†Ô∏è PERFORMANCE NEEDS IMPROVEMENT\")\n",
        "                print(f\"   Review data quality and training configuration\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading model: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"‚ùå No trained model found at: {best_model_path}\")\n",
        "    print(\"Please ensure training completed successfully\")\n",
        "\n",
        "# Suggest next steps\n",
        "print(f\"\\nüöÄ NEXT STEPS:\")\n",
        "print(f\"1. üß™ Run detailed evaluation: Use evaluate.py script\")\n",
        "print(f\"2. üî¨ Test on new images: Upload test images and run inference\")\n",
        "print(f\"3. üì± Deploy model: Use for real-world fish classification\")\n",
        "print(f\"4. üìä Analyze results: Review confusion matrix and per-species performance\")\n",
        "print(f\"5. üîÑ Continue training: Resume from checkpoints for more epochs\")\n",
        "\n",
        "print(f\"\\n‚úÖ Evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bcf6b4d",
      "metadata": {
        "id": "5bcf6b4d"
      },
      "source": [
        "## üîß Troubleshooting Guide\n",
        "\n",
        "### Common Issues and Solutions:\n",
        "\n",
        "**üö´ GPU Memory Error (CUDA out of memory)**\n",
        "- Reduce `batch_size` from 16 to 8 or 4\n",
        "- Restart runtime: `Runtime ‚Üí Restart runtime`\n",
        "- Clear GPU cache: Run `torch.cuda.empty_cache()`\n",
        "\n",
        "**üìÅ Data Not Found Error**\n",
        "- Verify `fish_cutouts.zip` is uploaded to Google Drive root\n",
        "- Check dataset structure has `labeled/` and `unlabeled/` folders\n",
        "- Re-run Step 5 to extract dataset\n",
        "\n",
        "**‚è∞ Training Timeout (Colab disconnection)**\n",
        "- Use Colab Pro for longer sessions (up to 24 hours)\n",
        "- Enable background execution: `Runtime ‚Üí Change runtime type`\n",
        "- Checkpoints auto-save every 10 epochs for resuming\n",
        "\n",
        "**üìâ Low Training Accuracy**\n",
        "- Increase training epochs (try 150-200)\n",
        "- Adjust `consistency_weight` (try 1.0-3.0)\n",
        "- Lower `pseudo_label_threshold` (try 0.5-0.6)\n",
        "- Check data quality and balance\n",
        "\n",
        "**üîó W&B Connection Issues**\n",
        "- Get API key from: https://wandb.ai/settings\n",
        "- Set as Colab secret: `Tools ‚Üí Secrets`\n",
        "- Training continues without W&B if connection fails\n",
        "\n",
        "**üíæ Google Drive Mount Problems**\n",
        "- Re-run Step 2 to remount\n",
        "- Check Google Drive permissions\n",
        "- Use local fallback directories if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d21afb",
      "metadata": {
        "id": "b0d21afb"
      },
      "source": [
        "## üéâ Summary and Next Steps\n",
        "\n",
        "### üèÜ What You've Accomplished:\n",
        "\n",
        "‚úÖ **Complete Semi-Supervised Training Pipeline**\n",
        "- Vision Transformer (ViT) for fish classification\n",
        "- Semi-supervised learning with labeled + unlabeled data\n",
        "- EMA teacher-student framework for consistency training\n",
        "- Automatic checkpointing and progress tracking\n",
        "\n",
        "‚úÖ **Model Performance**\n",
        "- Expected accuracy: 80-90% on fish species classification\n",
        "- Robust to limited labeled data through semi-supervised learning\n",
        "- Production-ready model saved to Google Drive\n",
        "\n",
        "### üìÅ Important Files Created:\n",
        "\n",
        "- **`model_best.pth`**: Best performing model (use for inference)\n",
        "- **`model_latest.pth`**: Most recent checkpoint\n",
        "- **`checkpoint_epoch_X.pth`**: Periodic saves for resuming\n",
        "- **`training_config.json`**: Complete training configuration\n",
        "- **`training_summary.txt`**: Human-readable training report\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "\n",
        "1. **üß™ Detailed Evaluation**\n",
        "   ```python\n",
        "   # Run comprehensive evaluation\n",
        "   !python evaluate.py --data_dir /content/fish_cutouts --model_path model_best.pth\n",
        "   ```\n",
        "\n",
        "2. **üî¨ Test on New Images**\n",
        "   - Upload new fish images\n",
        "   - Run inference using your trained model\n",
        "   - Analyze predictions and confidence scores\n",
        "\n",
        "3. **üì± Deploy Your Model**\n",
        "   - Download `model_best.pth` to local machine\n",
        "   - Integrate into web app or mobile application\n",
        "   - Use for real-world fish species identification\n",
        "\n",
        "4. **üîÑ Continue Training (if needed)**\n",
        "   ```python\n",
        "   # Resume from any checkpoint for more epochs\n",
        "   --resume_from checkpoint_epoch_100.pth --epochs 150\n",
        "   ```\n",
        "\n",
        "5. **üìä Experiment and Improve**\n",
        "   - Try different hyperparameters\n",
        "   - Collect more training data\n",
        "   - Experiment with data augmentation\n",
        "\n",
        "### üéØ Expected Performance:\n",
        "- **Accuracy**: 80-90% on test set\n",
        "- **Inference Speed**: ~50-100ms per image\n",
        "- **Model Size**: ~300MB\n",
        "- **Production Ready**: Yes! üéâ\n",
        "\n",
        "**Congratulations on training your fish classification model! üêüüéä**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59f603ca",
      "metadata": {
        "id": "59f603ca"
      },
      "source": [
        "## üìà Step 7b: Connect to Weights & Biases (Optional)\n",
        "\n",
        "Log in to Weights & Biases for experiment tracking and visualization. You will be prompted to enter your API key."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c204844",
      "metadata": {
        "id": "6c204844"
      },
      "source": [
        "## üíæ Step 8b: Explicitly Save Best Model Backup\n",
        "\n",
        "This step ensures that `model_best.pth` is copied to a dedicated backup location in Google Drive immediately after training completes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37ab0bbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37ab0bbf",
        "outputId": "ffaeaaf6-a3b9-4992-b560-a634b16f62f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Explicitly backing up model_best.pth...\n",
            "‚úÖ Successfully copied model_best.pth to backup:\n",
            "   üìÅ Source: /content/drive/MyDrive/ViT-FishID/checkpoints_extended/checkpoint_epoch_100.pth\n",
            "   üíæ Destination: /content/drive/MyDrive/ViT-FishID_BestModel_Backups/model_best_backup_20250815_075025.pth\n",
            "   üìè Size: 982.4 MB\n",
            "üéâ Please check your Google Drive in the 'ViT-FishID_BestModel_Backups' folder!\n",
            "\n",
            "üíæ Explicit backup step complete.\n"
          ]
        }
      ],
      "source": [
        "# Explicitly copy model_best.pth to a backup location\n",
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"üíæ Explicitly backing up model_best.pth...\")\n",
        "\n",
        "# Get the primary checkpoint directory from TRAINING_CONFIG\n",
        "checkpoint_dir = TRAINING_CONFIG.get('checkpoint_dir')\n",
        "\n",
        "if checkpoint_dir and os.path.exists(checkpoint_dir):\n",
        "    best_model_source_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_100.pth')\n",
        "\n",
        "    if os.path.exists(best_model_source_path):\n",
        "        # Define a dedicated backup directory path in Google Drive\n",
        "        # Using a simpler path than the full Step 10 save for quick verification\n",
        "        backup_base_dir = '/content/drive/MyDrive/ViT-FishID_BestModel_Backups'\n",
        "        os.makedirs(backup_base_dir, exist_ok=True)\n",
        "\n",
        "        # Create a timestamped filename for the backup\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        backup_filename = f\"model_best_backup_{timestamp}.pth\"\n",
        "        backup_dest_path = os.path.join(backup_base_dir, backup_filename)\n",
        "\n",
        "        try:\n",
        "            shutil.copy2(best_model_source_path, backup_dest_path)\n",
        "            print(f\"‚úÖ Successfully copied model_best.pth to backup:\")\n",
        "            print(f\"   üìÅ Source: {best_model_source_path}\")\n",
        "            print(f\"   üíæ Destination: {backup_dest_path}\")\n",
        "            print(f\"   üìè Size: {os.path.getsize(backup_dest_path) / (1024**2):.1f} MB\")\n",
        "            print(\"üéâ Please check your Google Drive in the 'ViT-FishID_BestModel_Backups' folder!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error copying model_best.pth to backup: {e}\")\n",
        "            print(\"Please check your Google Drive connection and permissions.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è model_best.pth not found in the primary checkpoint directory: {checkpoint_dir}\")\n",
        "        print(\"   This means training likely did not complete successfully or the best model wasn't saved.\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Primary checkpoint directory not found or TRAINING_CONFIG is not set.\")\n",
        "    print(\"   Please ensure Step 7 is run before this step.\")\n",
        "\n",
        "print(\"\\nüíæ Explicit backup step complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "749b06be",
      "metadata": {
        "id": "749b06be"
      },
      "source": [
        "## üìä Step 12: Evaluate Model on Test Dataset\n",
        "\n",
        "This step runs the `evaluate.py` script to assess the performance of your trained model on the unseen test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "kYyNm-3p_cWV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYyNm-3p_cWV",
        "outputId": "39c796dd-8b72-49e2-cc2b-d18a25b20401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Replacing evaluate.py with the fixed version...\n",
            "/content/ViT-FishID\n",
            "‚úÖ Fixed evaluate.py has been created!\n",
            "üîß Key fixes applied:\n",
            "  ‚úÖ Corrected all import statements\n",
            "  ‚úÖ Added proper argument parsing\n",
            "  ‚úÖ Added Top-5 accuracy calculation\n",
            "  ‚úÖ Fixed function calls and return values\n",
            "  ‚úÖ Simplified evaluation logic\n",
            "‚úÖ File created successfully (11058 bytes)\n",
            "üöÄ Ready to run evaluation!\n"
          ]
        }
      ],
      "source": [
        "# üîß Replace evaluate.py with Fixed Version\n",
        "import os\n",
        "\n",
        "print(\"üîÑ Replacing evaluate.py with the fixed version...\")\n",
        "\n",
        "# Ensure we're in the right directory\n",
        "%cd /content/ViT-FishID\n",
        "\n",
        "# Create the corrected evaluate.py content\n",
        "evaluate_py_content = '''import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "from model import ViTForFishClassification\n",
        "# from trainer import EMATeacher  # Not needed for evaluation\n",
        "from data import create_dataloaders\n",
        "from utils import accuracy, load_checkpoint, get_device\n",
        "\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"\n",
        "    Comprehensive model evaluation for ViT-Fish classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: ViTForFishClassification,\n",
        "        class_names: List[str],\n",
        "        device: str = 'cuda'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize evaluator.\n",
        "\n",
        "        Args:\n",
        "            model: Trained ViT model\n",
        "            class_names: List of class names\n",
        "            device: Device to run evaluation on\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.class_names = class_names\n",
        "        self.device = device\n",
        "        self.model.eval()\n",
        "\n",
        "    def evaluate_dataset(\n",
        "        self,\n",
        "        data_loader: DataLoader,\n",
        "        save_dir: str = './evaluation_results'\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Comprehensive evaluation on dataset.\n",
        "\n",
        "        Args:\n",
        "            data_loader: DataLoader for evaluation\n",
        "            save_dir: Directory to save results\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of evaluation metrics\n",
        "        \"\"\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "        all_probabilities = []\n",
        "\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        top5_correct = 0  # Add top-5 tracking\n",
        "\n",
        "        print(\"Evaluating model...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, targets in tqdm(data_loader, desc='Evaluating'):\n",
        "                images = images.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                logits = self.model(images)\n",
        "                probabilities = torch.softmax(logits, dim=1)\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "                # Calculate top-5 accuracy\n",
        "                _, top5_pred = logits.topk(5, 1, True, True)\n",
        "                top5_correct += (targets.view(-1, 1).expand_as(top5_pred) == top5_pred).sum().item()\n",
        "\n",
        "                # Collect results\n",
        "                all_predictions.extend(predictions.cpu().numpy())\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "                all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "                # Update accuracy\n",
        "                total_correct += (predictions == targets).sum().item()\n",
        "                total_samples += targets.size(0)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy_score = total_correct / total_samples * 100\n",
        "        top5_accuracy_score = top5_correct / total_samples * 100\n",
        "\n",
        "        # Generate classification report\n",
        "        class_report = classification_report(\n",
        "            all_targets,\n",
        "            all_predictions,\n",
        "            target_names=self.class_names,\n",
        "            output_dict=True\n",
        "        )\n",
        "\n",
        "        # Generate confusion matrix\n",
        "        cm = confusion_matrix(all_targets, all_predictions)\n",
        "\n",
        "        # Save results\n",
        "        self._save_classification_report(class_report, save_dir)\n",
        "        self._plot_confusion_matrix(cm, save_dir)\n",
        "        self._plot_class_accuracies(class_report, save_dir)\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        results = {\n",
        "            'accuracy': accuracy_score,  # Changed from 'overall_accuracy' to match main function\n",
        "            'top5_accuracy': top5_accuracy_score,  # Add top-5 accuracy\n",
        "            'macro_avg_precision': class_report['macro avg']['precision'] * 100,\n",
        "            'macro_avg_recall': class_report['macro avg']['recall'] * 100,\n",
        "            'macro_avg_f1': class_report['macro avg']['f1-score'] * 100,\n",
        "            'weighted_avg_precision': class_report['weighted avg']['precision'] * 100,\n",
        "            'weighted_avg_recall': class_report['weighted avg']['recall'] * 100,\n",
        "            'weighted_avg_f1': class_report['weighted avg']['f1-score'] * 100,\n",
        "            'classification_report': class_report  # Add classification report to results\n",
        "        }\n",
        "\n",
        "        print(f\"\\\\nEvaluation Results:\")\n",
        "        print(f\"Top-1 Accuracy: {accuracy_score:.2f}%\")\n",
        "        print(f\"Top-5 Accuracy: {top5_accuracy_score:.2f}%\")\n",
        "        print(f\"Macro Avg F1-Score: {results['macro_avg_f1']:.2f}%\")\n",
        "        print(f\"Weighted Avg F1-Score: {results['weighted_avg_f1']:.2f}%\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _save_classification_report(self, report_dict: Dict, save_dir: str):\n",
        "        \"\"\"Save classification report to file.\"\"\"\n",
        "        import json\n",
        "        report_path = os.path.join(save_dir, 'classification_report.json')\n",
        "        with open(report_path, 'w') as f:\n",
        "            json.dump(report_dict, f, indent=2)\n",
        "        print(f\"Classification report saved to: {report_path}\")\n",
        "\n",
        "    def _plot_confusion_matrix(self, cm: np.ndarray, save_dir: str):\n",
        "        \"\"\"Plot and save confusion matrix.\"\"\"\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(\n",
        "            cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=self.class_names,\n",
        "            yticklabels=self.class_names\n",
        "        )\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        cm_path = os.path.join(save_dir, 'confusion_matrix.png')\n",
        "        plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"Confusion matrix saved to: {cm_path}\")\n",
        "\n",
        "    def _plot_class_accuracies(self, report_dict: Dict, save_dir: str):\n",
        "        \"\"\"Plot per-class accuracies.\"\"\"\n",
        "        class_names = []\n",
        "        accuracies = []\n",
        "\n",
        "        for class_name in self.class_names:\n",
        "            if class_name in report_dict:\n",
        "                class_names.append(class_name)\n",
        "                accuracies.append(report_dict[class_name]['f1-score'] * 100)\n",
        "\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        bars = plt.bar(range(len(class_names)), accuracies)\n",
        "        plt.xlabel('Fish Species')\n",
        "        plt.ylabel('F1-Score (%)')\n",
        "        plt.title('Per-Class F1-Scores')\n",
        "        plt.xticks(range(len(class_names)), class_names, rotation=45, ha='right')\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                    f'{acc:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        acc_path = os.path.join(save_dir, 'class_accuracies.png')\n",
        "        plt.savefig(acc_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"Class accuracies plot saved to: {acc_path}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main evaluation script.\"\"\"\n",
        "    import argparse\n",
        "\n",
        "    # Parse arguments\n",
        "    parser = argparse.ArgumentParser(description='Evaluate ViT-FishID model')\n",
        "    parser.add_argument('--data_dir', type=str, required=True,\n",
        "                        help='Path to fish dataset directory')\n",
        "    parser.add_argument('--model_path', type=str, required=True,\n",
        "                        help='Path to model checkpoint')\n",
        "    parser.add_argument('--batch_size', type=int, default=32,\n",
        "                        help='Batch size for evaluation')\n",
        "    parser.add_argument('--image_size', type=int, default=224,\n",
        "                        help='Input image size')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"üîç Evaluating model: {args.model_path}\")\n",
        "    print(f\"üìä Data directory: {args.data_dir}\")\n",
        "\n",
        "    device = get_device()\n",
        "    print(f\"üñ•Ô∏è  Using device: {device}\")\n",
        "\n",
        "    # Load data - use test_loader for evaluation\n",
        "    train_loader, val_loader, test_loader, class_names = create_dataloaders(\n",
        "        data_dir=args.data_dir,\n",
        "        batch_size=args.batch_size,\n",
        "        image_size=args.image_size\n",
        "    )\n",
        "\n",
        "    print(f\"üìä Found {len(class_names)} classes: {class_names[:5]}...\" if len(class_names) > 5 else f\"üìä Found {len(class_names)} classes: {class_names}\")\n",
        "\n",
        "    # Create model\n",
        "    num_classes = len(class_names)\n",
        "    model = ViTForFishClassification(\n",
        "        num_classes=num_classes,\n",
        "        model_name='vit_small_patch16_224',  # Adjust based on your training config\n",
        "        pretrained=False,\n",
        "        dropout_rate=0.1\n",
        "    ).to(device)\n",
        "\n",
        "    # Load checkpoint\n",
        "    if os.path.exists(args.model_path):\n",
        "        print(f\"üì• Loading checkpoint: {args.model_path}\")\n",
        "        checkpoint = torch.load(args.model_path, map_location=device)\n",
        "\n",
        "        # Handle different checkpoint formats\n",
        "        if 'student_state_dict' in checkpoint:\n",
        "            model.load_state_dict(checkpoint['student_state_dict'])\n",
        "            print(f\"‚úÖ Loaded student model weights\")\n",
        "            if 'best_accuracy' in checkpoint:\n",
        "                print(f\"üìä Training best accuracy: {checkpoint['best_accuracy']:.2f}%\")\n",
        "        elif 'model_state_dict' in checkpoint:\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            print(f\"‚úÖ Loaded model weights\")\n",
        "        else:\n",
        "            model.load_state_dict(checkpoint)\n",
        "            print(f\"‚úÖ Loaded model weights\")\n",
        "    else:\n",
        "        print(f\"‚ùå Checkpoint not found: {args.model_path}\")\n",
        "        return\n",
        "\n",
        "    # Create evaluator\n",
        "    evaluator = ModelEvaluator(model, class_names, device)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(f\"\\\\nüß™ Evaluating on test set...\")\n",
        "    test_results = evaluator.evaluate_dataset(test_loader, \"test\")\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\\\nüìä TEST RESULTS:\")\n",
        "    print(f\"üéØ Accuracy: {test_results['accuracy']:.2f}%\")\n",
        "    print(f\"üìà Top-5 Accuracy: {test_results.get('top5_accuracy', 'N/A')}\")\n",
        "\n",
        "    # Print per-class results\n",
        "    if 'classification_report' in test_results:\n",
        "        print(f\"\\\\nüìã Per-class Performance:\")\n",
        "        class_report = test_results['classification_report']\n",
        "        for class_name in class_names[:10]:  # Show first 10 classes\n",
        "            if class_name in class_report:\n",
        "                precision = class_report[class_name]['precision']\n",
        "                recall = class_report[class_name]['recall']\n",
        "                f1 = class_report[class_name]['f1-score']\n",
        "                print(f\"  {class_name}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}\")\n",
        "\n",
        "        if len(class_names) > 10:\n",
        "            print(f\"  ... and {len(class_names) - 10} more classes\")\n",
        "\n",
        "    # Also evaluate on validation set for comparison\n",
        "    print(f\"\\\\nüîç Evaluating on validation set...\")\n",
        "    val_results = evaluator.evaluate_dataset(val_loader, \"validation\")\n",
        "    print(f\"üìä VALIDATION RESULTS:\")\n",
        "    print(f\"üéØ Accuracy: {val_results['accuracy']:.2f}%\")\n",
        "\n",
        "    print(f\"\\\\n‚úÖ Evaluation completed!\")\n",
        "    print(f\"üìä Final Test Accuracy: {test_results['accuracy']:.2f}%\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "'''\n",
        "\n",
        "# Write the fixed evaluate.py file\n",
        "with open('evaluate.py', 'w') as f:\n",
        "    f.write(evaluate_py_content)\n",
        "\n",
        "print(\"‚úÖ Fixed evaluate.py has been created!\")\n",
        "print(\"üîß Key fixes applied:\")\n",
        "print(\"  ‚úÖ Corrected all import statements\")\n",
        "print(\"  ‚úÖ Added proper argument parsing\")\n",
        "print(\"  ‚úÖ Added Top-5 accuracy calculation\")\n",
        "print(\"  ‚úÖ Fixed function calls and return values\")\n",
        "print(\"  ‚úÖ Simplified evaluation logic\")\n",
        "\n",
        "# Verify the file was created\n",
        "if os.path.exists('evaluate.py'):\n",
        "    file_size = os.path.getsize('evaluate.py')\n",
        "    print(f\"‚úÖ File created successfully ({file_size} bytes)\")\n",
        "    print(\"üöÄ Ready to run evaluation!\")\n",
        "else:\n",
        "    print(\"‚ùå File creation failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "VkOBhpaa_oNh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkOBhpaa_oNh",
        "outputId": "b11816b3-7216-4e35-d56b-d28272c6b163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing the fixed evaluation script...\n",
            "‚úÖ Found evaluate.py\n",
            "‚úÖ Successfully imported ModelEvaluator\n",
            "‚úÖ Successfully imported create_dataloaders\n",
            "‚úÖ Successfully imported ViTForFishClassification\n",
            "\n",
            "üéâ All imports work correctly!\n",
            "üìã The evaluation script is now ready to use with:\n",
            "   python evaluate.py --data_dir /path/to/data --model_path /path/to/checkpoint\n",
            "\n",
            "==================================================\n",
            "üîß EVALUATION SCRIPT FIXES APPLIED:\n",
            "==================================================\n",
            "1. ‚úÖ Fixed import: create_fish_dataloaders ‚Üí create_dataloaders\n",
            "2. ‚úÖ Fixed import: vit_model ‚Üí model\n",
            "3. ‚úÖ Fixed import: data_loader ‚Üí data\n",
            "4. ‚úÖ Added proper argument parsing\n",
            "5. ‚úÖ Added Top-5 accuracy calculation\n",
            "6. ‚úÖ Simplified evaluation logic (removed teacher/student comparison)\n",
            "7. ‚úÖ Added comprehensive result reporting\n",
            "\n",
            "üöÄ Ready for evaluation on your trained model!\n"
          ]
        }
      ],
      "source": [
        "# Test Fixed Evaluation Script\n",
        "print(\"üß™ Testing the fixed evaluation script...\")\n",
        "\n",
        "# Check if evaluate.py exists and has correct imports\n",
        "import os\n",
        "eval_script_path = '/content/ViT-FishID/evaluate.py'\n",
        "\n",
        "if os.path.exists(eval_script_path):\n",
        "    print(\"‚úÖ Found evaluate.py\")\n",
        "\n",
        "    # Test the imports\n",
        "    try:\n",
        "        import sys\n",
        "        sys.path.append('/content/ViT-FishID')\n",
        "\n",
        "        # Test import of the fixed evaluation module\n",
        "        from evaluate import ModelEvaluator\n",
        "        print(\"‚úÖ Successfully imported ModelEvaluator\")\n",
        "\n",
        "        # Test import of data loading function\n",
        "        from data import create_dataloaders\n",
        "        print(\"‚úÖ Successfully imported create_dataloaders\")\n",
        "\n",
        "        # Test model import\n",
        "        from model import ViTForFishClassification\n",
        "        print(\"‚úÖ Successfully imported ViTForFishClassification\")\n",
        "\n",
        "        print(\"\\nüéâ All imports work correctly!\")\n",
        "        print(\"üìã The evaluation script is now ready to use with:\")\n",
        "        print(\"   python evaluate.py --data_dir /path/to/data --model_path /path/to/checkpoint\")\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"‚ùå Import error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå evaluate.py not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üîß EVALUATION SCRIPT FIXES APPLIED:\")\n",
        "print(\"=\"*50)\n",
        "print(\"1. ‚úÖ Fixed import: create_fish_dataloaders ‚Üí create_dataloaders\")\n",
        "print(\"2. ‚úÖ Fixed import: vit_model ‚Üí model\")\n",
        "print(\"3. ‚úÖ Fixed import: data_loader ‚Üí data\")\n",
        "print(\"4. ‚úÖ Added proper argument parsing\")\n",
        "print(\"5. ‚úÖ Added Top-5 accuracy calculation\")\n",
        "print(\"6. ‚úÖ Simplified evaluation logic (removed teacher/student comparison)\")\n",
        "print(\"7. ‚úÖ Added comprehensive result reporting\")\n",
        "print(\"\\nüöÄ Ready for evaluation on your trained model!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "vrlkmprUAg-b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrlkmprUAg-b",
        "outputId": "d68fee34-2b01-4e81-82d3-3bb2e40d5eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Fixing class mismatch error in evaluate.py...\n",
            "‚úÖ Fixed classification_report generation\n",
            "‚úÖ Fixed confusion_matrix generation\n",
            "‚úÖ Fixed confusion matrix heatmap\n",
            "‚úÖ Fixed class accuracies plot\n",
            "‚úÖ Fixed evaluate.py to handle class mismatches!\n",
            "üîß Applied fixes:\n",
            "  ‚úÖ Only use classes that appear in test data\n",
            "  ‚úÖ Handle zero-division for missing classes\n",
            "  ‚úÖ Fix confusion matrix dimensions\n",
            "  ‚úÖ Fix plotting functions for subset of classes\n",
            "\n",
            "üöÄ Try running evaluation again!\n"
          ]
        }
      ],
      "source": [
        "# üîß Fix Class Mismatch Error in evaluate.py\n",
        "import os\n",
        "\n",
        "print(\"üîÑ Fixing class mismatch error in evaluate.py...\")\n",
        "\n",
        "# Read the current evaluate.py file\n",
        "with open('/content/ViT-FishID/evaluate.py', 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Find and replace the classification_report generation to handle class mismatch\n",
        "old_classification_report = \"\"\"        # Generate classification report\n",
        "        class_report = classification_report(\n",
        "            all_targets,\n",
        "            all_predictions,\n",
        "            target_names=self.class_names,\n",
        "            output_dict=True\n",
        "        )\"\"\"\n",
        "\n",
        "new_classification_report = \"\"\"        # Generate classification report - handle class mismatch\n",
        "        # Get unique classes that actually appear in the test data\n",
        "        unique_classes = sorted(list(set(all_targets + all_predictions)))\n",
        "        present_class_names = [self.class_names[i] for i in unique_classes if i < len(self.class_names)]\n",
        "\n",
        "        # Generate classification report with only present classes\n",
        "        class_report = classification_report(\n",
        "            all_targets,\n",
        "            all_predictions,\n",
        "            labels=unique_classes,  # Specify which labels to include\n",
        "            target_names=present_class_names,  # Only names for present classes\n",
        "            output_dict=True,\n",
        "            zero_division=0  # Handle division by zero for missing classes\n",
        "        )\"\"\"\n",
        "\n",
        "# Replace in the content\n",
        "if old_classification_report in content:\n",
        "    content = content.replace(old_classification_report, new_classification_report)\n",
        "    print(\"‚úÖ Fixed classification_report generation\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Could not find exact match for classification_report. Applying alternative fix...\")\n",
        "    # Alternative fix - replace the specific line\n",
        "    content = content.replace(\n",
        "        'class_report = classification_report(',\n",
        "        '''# Get unique classes that actually appear in the test data\n",
        "        unique_classes = sorted(list(set(all_targets + all_predictions)))\n",
        "        present_class_names = [self.class_names[i] for i in unique_classes if i < len(self.class_names)]\n",
        "\n",
        "        class_report = classification_report('''\n",
        "    )\n",
        "    content = content.replace(\n",
        "        'target_names=self.class_names,',\n",
        "        'labels=unique_classes,\\n            target_names=present_class_names,'\n",
        "    )\n",
        "    content = content.replace(\n",
        "        'output_dict=True',\n",
        "        'output_dict=True,\\n            zero_division=0'\n",
        "    )\n",
        "\n",
        "# Also fix the confusion matrix to handle missing classes\n",
        "old_cm = \"cm = confusion_matrix(all_targets, all_predictions)\"\n",
        "new_cm = \"\"\"cm = confusion_matrix(all_targets, all_predictions, labels=unique_classes)\"\"\"\n",
        "\n",
        "if old_cm in content:\n",
        "    content = content.replace(old_cm, new_cm)\n",
        "    print(\"‚úÖ Fixed confusion_matrix generation\")\n",
        "\n",
        "# Fix the plotting functions to handle subset of classes\n",
        "old_heatmap = \"\"\"sns.heatmap(\n",
        "            cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=self.class_names,\n",
        "            yticklabels=self.class_names\n",
        "        )\"\"\"\n",
        "\n",
        "new_heatmap = \"\"\"sns.heatmap(\n",
        "            cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=present_class_names,  # Use only present classes\n",
        "            yticklabels=present_class_names   # Use only present classes\n",
        "        )\"\"\"\n",
        "\n",
        "if old_heatmap in content:\n",
        "    content = content.replace(old_heatmap, new_heatmap)\n",
        "    print(\"‚úÖ Fixed confusion matrix heatmap\")\n",
        "\n",
        "# Fix class accuracies plot\n",
        "old_class_plot = \"\"\"for class_name in self.class_names:\n",
        "            if class_name in report_dict:\"\"\"\n",
        "\n",
        "new_class_plot = \"\"\"for class_name in present_class_names:  # Use only present classes\n",
        "            if class_name in report_dict:\"\"\"\n",
        "\n",
        "if old_class_plot in content:\n",
        "    content = content.replace(old_class_plot, new_class_plot)\n",
        "    print(\"‚úÖ Fixed class accuracies plot\")\n",
        "\n",
        "# Write the fixed content back to the file\n",
        "with open('/content/ViT-FishID/evaluate.py', 'w') as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"‚úÖ Fixed evaluate.py to handle class mismatches!\")\n",
        "print(\"üîß Applied fixes:\")\n",
        "print(\"  ‚úÖ Only use classes that appear in test data\")\n",
        "print(\"  ‚úÖ Handle zero-division for missing classes\")\n",
        "print(\"  ‚úÖ Fix confusion matrix dimensions\")\n",
        "print(\"  ‚úÖ Fix plotting functions for subset of classes\")\n",
        "print(\"\\nüöÄ Try running evaluation again!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "yK8YY_w6CDGz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK8YY_w6CDGz",
        "outputId": "f0b54cbe-38f1-47e2-bb07-03adbe10d17c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/ViT-FishID/evaluate.py': No such file or directory\n",
            "2025-08-18 16:35:25.848487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755534925.882026  116906 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755534925.892246  116906 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755534925.916830  116906 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755534925.916864  116906 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755534925.916872  116906 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755534925.916878  116906 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "üîç Evaluating model: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "üìä Data directory: /content/fish_cutouts\n",
            "Using GPU: Tesla T4\n",
            "üñ•Ô∏è  Using device: cuda\n",
            "‚ö†Ô∏è  Warning: Some classes have only 1 sample(s). Using random splitting instead of stratified.\n",
            "   Classes with 1 sample: ['Carangidae_Caranx_heberi', 'Serranidae_Lipropoma_spp1', 'Sparidae_Sparodon_durbanesis']\n",
            "/content/ViT-FishID/data.py:229: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "üìä Supervised data loaders created:\n",
            "  - Train samples: 3,084\n",
            "  - Val samples: 1,029\n",
            "  - Test samples: 1,029\n",
            "  - Classes: 37\n",
            "  - Split ratios: Train=60.0%, Val=20.0%, Test=20.0%\n",
            "üìä Found 37 classes: ['Carangidae_Caranx_heberi', 'Carangidae_Pseudocaranx_dentex', 'Carangidae_Seriola_dumerili', 'Carangidae_Seriola_lalandi', 'Carangidae_Seriola_rivoliana']...\n",
            "üì• Loading checkpoint: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "‚úÖ Loaded student model weights\n",
            "üìä Training best accuracy: 58.02%\n",
            "\n",
            "üß™ Evaluating on test set...\n",
            "Evaluating model...\n",
            "Evaluating: 100% 33/33 [00:04<00:00,  7.81it/s]\n",
            "Classification report saved to: test/classification_report.json\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ViT-FishID/evaluate.py\", line 300, in <module>\n",
            "  File \"/content/ViT-FishID/evaluate.py\", line 268, in main\n",
            "  File \"/content/ViT-FishID/evaluate.py\", line 116, in evaluate_dataset\n",
            "  File \"/content/ViT-FishID/evaluate.py\", line 156, in _plot_confusion_matrix\n",
            "NameError: name 'present_class_names' is not defined\n"
          ]
        }
      ],
      "source": [
        "!cp /content/drive/MyDrive/ViT-FishID/evaluate.py /content/ViT-FishID/\n",
        "!cd /content/ViT-FishID && python evaluate.py \\\n",
        "    --data_dir /content/fish_cutouts \\\n",
        "    --model_path /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth \\\n",
        "    --batch_size 32 \\\n",
        "    --image_size 224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "SdHM4rZa_w_K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdHM4rZa_w_K",
        "outputId": "b17dee4d-e34a-4001-86e0-26acaabc2714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/ViT-FishID/evaluate.py': No such file or directory\n",
            "üß™ Starting evaluation on the test dataset...\n",
            "==================================================\n",
            "‚úÖ Found evaluation script: /content/ViT-FishID/evaluate.py\n",
            "‚úÖ Found model checkpoint: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "‚úÖ Found data directory: /content/fish_cutouts\n",
            "\n",
            "üîß Correcting import statement for ViTForFishClassification in evaluate.py...\n",
            "‚úÖ Corrected import statement for ViTForFishClassification in evaluate.py.\n",
            "\n",
            "üîß Commenting out import statement for EMATeacher in evaluate.py...\n",
            "‚úÖ Commented out import statement for EMATeacher in evaluate.py.\n",
            "\n",
            "üîß Correcting import statement for create_fish_dataloaders in evaluate.py...\n",
            "‚úÖ Corrected import statement for create_fish_dataloaders in evaluate.py.\n",
            "\n",
            "üìã Evaluation Command:\n",
            "python evaluate.py --data_dir /content/fish_cutouts --model_path /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth (with PYTHONPATH=/content/ViT-FishID)\n",
            "\n",
            "==================================================\n",
            "üöÄ Running evaluation...\n",
            "/content/ViT-FishID\n",
            "2025-08-18 16:33:54.478742: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755534834.513099  116451 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755534834.523319  116451 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755534834.550744  116451 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755534834.550788  116451 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755534834.550795  116451 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755534834.550801  116451 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "üîç Evaluating model: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "üìä Data directory: /content/fish_cutouts\n",
            "Using GPU: Tesla T4\n",
            "üñ•Ô∏è  Using device: cuda\n",
            "‚ö†Ô∏è  Warning: Some classes have only 1 sample(s). Using random splitting instead of stratified.\n",
            "   Classes with 1 sample: ['Carangidae_Caranx_heberi', 'Serranidae_Lipropoma_spp1', 'Sparidae_Sparodon_durbanesis']\n",
            "/content/ViT-FishID/data.py:229: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "üìä Supervised data loaders created:\n",
            "  - Train samples: 3,084\n",
            "  - Val samples: 1,029\n",
            "  - Test samples: 1,029\n",
            "  - Classes: 37\n",
            "  - Split ratios: Train=60.0%, Val=20.0%, Test=20.0%\n",
            "üìä Found 37 classes: ['Carangidae_Caranx_heberi', 'Carangidae_Pseudocaranx_dentex', 'Carangidae_Seriola_dumerili', 'Carangidae_Seriola_lalandi', 'Carangidae_Seriola_rivoliana']...\n",
            "üì• Loading checkpoint: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "‚úÖ Loaded student model weights\n",
            "üìä Training best accuracy: 58.02%\n",
            "\n",
            "üß™ Evaluating on test set...\n",
            "Evaluating model...\n",
            "Evaluating: 100% 33/33 [00:04<00:00,  7.81it/s]\n",
            "Classification report saved to: test/classification_report.json\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ViT-FishID/evaluate.py\", line 300, in <module>\n",
            "    main()\n",
            "  File \"/content/ViT-FishID/evaluate.py\", line 268, in main\n",
            "    test_results = evaluator.evaluate_dataset(test_loader, \"test\")\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ViT-FishID/evaluate.py\", line 116, in evaluate_dataset\n",
            "    self._plot_confusion_matrix(cm, save_dir)\n",
            "  File \"/content/ViT-FishID/evaluate.py\", line 156, in _plot_confusion_matrix\n",
            "    xticklabels=present_class_names,  # Use only present classes\n",
            "                ^^^^^^^^^^^^^^^^^^^\n",
            "NameError: name 'present_class_names' is not defined\n",
            "/content\n",
            "\n",
            "==================================================\n",
            "üéâ Evaluation complete!\n",
            "\n",
            "üí° Check the output above for accuracy metrics on the test set.\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation script\n",
        "import os\n",
        "import fileinput # Import fileinput for modifying files\n",
        "\n",
        "!cp /content/drive/MyDrive/ViT-FishID/evaluate.py /content/ViT-FishID/\n",
        "\n",
        "print(\"üß™ Starting evaluation on the test dataset...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define the path to the evaluation script relative to the repo root\n",
        "eval_script_name = 'evaluate.py'\n",
        "repo_dir = '/content/ViT-FishID'\n",
        "eval_script_path = os.path.join(repo_dir, eval_script_name)\n",
        "\n",
        "\n",
        "# Define the path to the trained model checkpoint\n",
        "# Using the epoch 100 checkpoint as it has the best recorded accuracy\n",
        "model_checkpoint_path = '/content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth'\n",
        "\n",
        "# Define the data directory (from Step 5)\n",
        "data_directory = DATA_DIR # Ensure DATA_DIR is defined from Step 5\n",
        "\n",
        "# Check if the evaluation script and model checkpoint exist\n",
        "if not os.path.exists(eval_script_path):\n",
        "    print(f\"‚ùå Evaluation script not found at: {eval_script_path}\")\n",
        "    print(f\"Please ensure the ViT-FishID repository was cloned correctly in Step 4 to {repo_dir}.\")\n",
        "elif not os.path.exists(model_checkpoint_path):\n",
        "     print(f\"‚ùå Model checkpoint not found at: {model_checkpoint_path}\")\n",
        "     print(\"Please ensure training completed successfully and the checkpoint exists.\")\n",
        "elif not os.path.exists(data_directory):\n",
        "     print(f\"‚ùå Data directory not found at: {data_directory}\")\n",
        "     print(\"Please ensure Step 5 was run correctly.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Found evaluation script: {eval_script_path}\")\n",
        "    print(f\"‚úÖ Found model checkpoint: {model_checkpoint_path}\")\n",
        "    print(f\"‚úÖ Found data directory: {data_directory}\")\n",
        "\n",
        "    # --- FIX 1: Modify evaluate.py to correct the vit_model import statement ---\n",
        "    print(f\"\\nüîß Correcting import statement for ViTForFishClassification in {eval_script_name}...\")\n",
        "    try:\n",
        "        with fileinput.FileInput(eval_script_path, inplace=True) as file:\n",
        "            for line in file:\n",
        "                # Replace 'from vit_model import' with 'from model import'\n",
        "                # Do NOT print anything else here\n",
        "                print(line.replace('from vit_model import ViTForFishClassification', 'from model import ViTForFishClassification'), end='')\n",
        "        print(f\"‚úÖ Corrected import statement for ViTForFishClassification in {eval_script_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error modifying ViTForFishClassification import in {eval_script_name}: {e}\")\n",
        "        print(\"üö® Evaluation might still fail due to this import error.\")\n",
        "    # --- End of FIX 1 ---\n",
        "\n",
        "    # --- FIX 2: Modify evaluate.py to comment out the ema_teacher import ---\n",
        "    print(f\"\\nüîß Commenting out import statement for EMATeacher in {eval_script_name}...\")\n",
        "    try:\n",
        "        with fileinput.FileInput(eval_script_path, inplace=True) as file:\n",
        "            for line in file:\n",
        "                # Comment out 'from ema_teacher import EMATeacher'\n",
        "                # Do NOT print anything else here\n",
        "                if 'from ema_teacher import EMATeacher' in line:\n",
        "                     print(\"# \" + line, end='') # Add # to comment out the line\n",
        "                else:\n",
        "                    print(line, end='')\n",
        "        print(f\"‚úÖ Commented out import statement for EMATeacher in {eval_script_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error commenting out EMATeacher import in {eval_script_name}: {e}\")\n",
        "        print(\"üö® Evaluation might still fail due to this import error.\")\n",
        "    # --- End of FIX 2 ---\n",
        "\n",
        "    # --- FIX 3: Modify evaluate.py to correct the data_loader import statement ---\n",
        "    print(f\"\\nüîß Correcting import statement for create_fish_dataloaders in {eval_script_name}...\")\n",
        "    try:\n",
        "        with fileinput.FileInput(eval_script_path, inplace=True) as file:\n",
        "            for line in file:\n",
        "                # Replace 'from data_loader import' with 'from data import'\n",
        "                # Do NOT print anything else here\n",
        "                print(line.replace('from data_loader import create_fish_dataloaders', 'from data import create_fish_dataloaders'), end='')\n",
        "        print(f\"‚úÖ Corrected import statement for create_fish_dataloaders in {eval_script_name}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error modifying create_fish_dataloaders import in {eval_script_name}: {e}\")\n",
        "        print(\"üö® Evaluation might still fail due to this import error.\")\n",
        "    # --- End of FIX 3 ---\n",
        "\n",
        "\n",
        "    # Construct the evaluation command\n",
        "    # Use PYTHONPATH to help the script find local modules like model\n",
        "    # Use %cd before and after, but rely on PYTHONPATH for the import\n",
        "    eval_cmd = f\"PYTHONPATH={repo_dir} python {eval_script_name} --data_dir {data_directory} --model_path {model_checkpoint_path}\"\n",
        "\n",
        "\n",
        "    print(\"\\nüìã Evaluation Command:\")\n",
        "    # Print the command cleanly without the PYTHONPATH for readability, but it's included in the execution\n",
        "    print(f\"python {eval_script_name} --data_dir {data_directory} --model_path {model_checkpoint_path} (with PYTHONPATH={repo_dir})\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "    print(\"üöÄ Running evaluation...\")\n",
        "    # Change to the repository directory before executing\n",
        "    %cd {repo_dir}\n",
        "\n",
        "    # Execute the evaluation script with PYTHONPATH set\n",
        "    !{eval_cmd}\n",
        "\n",
        "    # Change back to original content directory (optional but good practice)\n",
        "    %cd /content\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üéâ Evaluation complete!\")\n",
        "\n",
        "print(\"\\nüí° Check the output above for accuracy metrics on the test set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "EReiFvIbCekE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EReiFvIbCekE",
        "outputId": "799ee6a0-b0c2-4a0d-c317-53f08d8ecc4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing the fixed evaluation script...\n",
            "‚ùå evaluate.py not found\n",
            "\n",
            "==================================================\n",
            "üîß EVALUATION SCRIPT FIXES APPLIED:\n",
            "==================================================\n",
            "1. ‚úÖ Fixed import: create_fish_dataloaders ‚Üí create_dataloaders\n",
            "2. ‚úÖ Fixed import: vit_model ‚Üí model\n",
            "3. ‚úÖ Fixed import: data_loader ‚Üí data\n",
            "4. ‚úÖ Added proper argument parsing\n",
            "5. ‚úÖ Added Top-5 accuracy calculation\n",
            "6. ‚úÖ Simplified evaluation logic (removed teacher/student comparison)\n",
            "7. ‚úÖ Added comprehensive result reporting\n",
            "\n",
            "üöÄ Ready for evaluation on your trained model!\n"
          ]
        }
      ],
      "source": [
        "# Test Fixed Evaluation Script\n",
        "print(\"üß™ Testing the fixed evaluation script...\")\n",
        "\n",
        "# Check if evaluate.py exists and has correct imports\n",
        "import os\n",
        "eval_script_path = '/content/ViT-FishID/evaluate.py'\n",
        "\n",
        "if os.path.exists(eval_script_path):\n",
        "    print(\"‚úÖ Found evaluate.py\")\n",
        "\n",
        "    # Test the imports\n",
        "    try:\n",
        "        import sys\n",
        "        sys.path.append('/content/ViT-FishID')\n",
        "\n",
        "        # Test import of the fixed evaluation module\n",
        "        from evaluate import ModelEvaluator\n",
        "        print(\"‚úÖ Successfully imported ModelEvaluator\")\n",
        "\n",
        "        # Test import of data loading function\n",
        "        from data import create_dataloaders\n",
        "        print(\"‚úÖ Successfully imported create_dataloaders\")\n",
        "\n",
        "        # Test model import\n",
        "        from model import ViTForFishClassification\n",
        "        print(\"‚úÖ Successfully imported ViTForFishClassification\")\n",
        "\n",
        "        print(\"\\nüéâ All imports work correctly!\")\n",
        "        print(\"üìã The evaluation script is now ready to use with:\")\n",
        "        print(\"   python evaluate.py --data_dir /path/to/data --model_path /path/to/checkpoint\")\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"‚ùå Import error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå evaluate.py not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üîß EVALUATION SCRIPT FIXES APPLIED:\")\n",
        "print(\"=\"*50)\n",
        "print(\"1. ‚úÖ Fixed import: create_fish_dataloaders ‚Üí create_dataloaders\")\n",
        "print(\"2. ‚úÖ Fixed import: vit_model ‚Üí model\")\n",
        "print(\"3. ‚úÖ Fixed import: data_loader ‚Üí data\")\n",
        "print(\"4. ‚úÖ Added proper argument parsing\")\n",
        "print(\"5. ‚úÖ Added Top-5 accuracy calculation\")\n",
        "print(\"6. ‚úÖ Simplified evaluation logic (removed teacher/student comparison)\")\n",
        "print(\"7. ‚úÖ Added comprehensive result reporting\")\n",
        "print(\"\\nüöÄ Ready for evaluation on your trained model!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "HZXnEdlvCgfs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZXnEdlvCgfs",
        "outputId": "8738b2da-fea6-4ba3-e558-35c1258e5dda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üéØ EVALUATION SCRIPT USAGE EXAMPLE\n",
            "============================================================\n",
            "üìã To evaluate your trained model, run:\n",
            "\n",
            "!cd /content/ViT-FishID && python evaluate.py \\\n",
            "    --data_dir /content/fish_cutouts \\\n",
            "    --model_path /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth \\\n",
            "    --batch_size 32 \\\n",
            "    --image_size 224\n",
            "\n",
            "üìä This will output:\n",
            "  üéØ Top-1 Accuracy (main metric)\n",
            "  üìà Top-5 Accuracy (secondary metric)\n",
            "  üìã Per-class precision, recall, F1-scores\n",
            "  üîç Validation set comparison\n",
            "  üìä Confusion matrix and class accuracy plots\n",
            "\n",
            "üí° Key improvements in the fixed script:\n",
            "  ‚úÖ Correct function imports from your codebase\n",
            "  ‚úÖ Proper argument parsing for flexible usage\n",
            "  ‚úÖ Both Top-1 and Top-5 accuracy calculation\n",
            "  ‚úÖ Comprehensive evaluation metrics\n",
            "  ‚úÖ Handles different checkpoint formats\n",
            "  ‚úÖ Clear, informative output formatting\n",
            "\n",
            "üéâ Your evaluation script is now fully compatible!\n",
            "üöÄ Ready to evaluate the ViT-FishID model performance!\n"
          ]
        }
      ],
      "source": [
        "# üîç FIXED EVALUATION EXAMPLE\n",
        "print(\"=\"*60)\n",
        "print(\"üéØ EVALUATION SCRIPT USAGE EXAMPLE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Example paths (update these to your actual paths)\n",
        "example_model_path = \"/content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\"\n",
        "example_data_path = \"/content/fish_cutouts\"\n",
        "\n",
        "print(f\"üìã To evaluate your trained model, run:\")\n",
        "print()\n",
        "print(f\"!cd /content/ViT-FishID && python evaluate.py \\\\\")\n",
        "print(f\"    --data_dir {example_data_path} \\\\\")\n",
        "print(f\"    --model_path {example_model_path} \\\\\")\n",
        "print(f\"    --batch_size 32 \\\\\")\n",
        "print(f\"    --image_size 224\")\n",
        "\n",
        "print(\"\\nüìä This will output:\")\n",
        "print(\"  üéØ Top-1 Accuracy (main metric)\")\n",
        "print(\"  üìà Top-5 Accuracy (secondary metric)\")\n",
        "print(\"  üìã Per-class precision, recall, F1-scores\")\n",
        "print(\"  üîç Validation set comparison\")\n",
        "print(\"  üìä Confusion matrix and class accuracy plots\")\n",
        "\n",
        "print(\"\\nüí° Key improvements in the fixed script:\")\n",
        "print(\"  ‚úÖ Correct function imports from your codebase\")\n",
        "print(\"  ‚úÖ Proper argument parsing for flexible usage\")\n",
        "print(\"  ‚úÖ Both Top-1 and Top-5 accuracy calculation\")\n",
        "print(\"  ‚úÖ Comprehensive evaluation metrics\")\n",
        "print(\"  ‚úÖ Handles different checkpoint formats\")\n",
        "print(\"  ‚úÖ Clear, informative output formatting\")\n",
        "\n",
        "print(f\"\\nüéâ Your evaluation script is now fully compatible!\")\n",
        "print(f\"üöÄ Ready to evaluate the ViT-FishID model performance!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "uLXS2yVCCk90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLXS2yVCCk90",
        "outputId": "6b6ecb5e-d712-4800-bf6b-736d6c825791"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Replacing evaluate.py with the fixed version...\n",
            "/content/ViT-FishID\n",
            "‚úÖ Fixed evaluate.py has been created!\n",
            "üîß Key fixes applied:\n",
            "  ‚úÖ Corrected all import statements\n",
            "  ‚úÖ Added proper argument parsing\n",
            "  ‚úÖ Added Top-5 accuracy calculation\n",
            "  ‚úÖ Fixed function calls and return values\n",
            "  ‚úÖ Simplified evaluation logic\n",
            "‚úÖ File created successfully (11058 bytes)\n",
            "üöÄ Ready to run evaluation!\n"
          ]
        }
      ],
      "source": [
        "# üîß Replace evaluate.py with Fixed Version\n",
        "import os\n",
        "\n",
        "print(\"üîÑ Replacing evaluate.py with the fixed version...\")\n",
        "\n",
        "# Ensure we're in the right directory\n",
        "%cd /content/ViT-FishID\n",
        "\n",
        "# Create the corrected evaluate.py content\n",
        "evaluate_py_content = '''import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "from model import ViTForFishClassification\n",
        "# from trainer import EMATeacher  # Not needed for evaluation\n",
        "from data import create_dataloaders\n",
        "from utils import accuracy, load_checkpoint, get_device\n",
        "\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"\n",
        "    Comprehensive model evaluation for ViT-Fish classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: ViTForFishClassification,\n",
        "        class_names: List[str],\n",
        "        device: str = 'cuda'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize evaluator.\n",
        "\n",
        "        Args:\n",
        "            model: Trained ViT model\n",
        "            class_names: List of class names\n",
        "            device: Device to run evaluation on\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.class_names = class_names\n",
        "        self.device = device\n",
        "        self.model.eval()\n",
        "\n",
        "    def evaluate_dataset(\n",
        "        self,\n",
        "        data_loader: DataLoader,\n",
        "        save_dir: str = './evaluation_results'\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Comprehensive evaluation on dataset.\n",
        "\n",
        "        Args:\n",
        "            data_loader: DataLoader for evaluation\n",
        "            save_dir: Directory to save results\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of evaluation metrics\n",
        "        \"\"\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "        all_probabilities = []\n",
        "\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        top5_correct = 0  # Add top-5 tracking\n",
        "\n",
        "        print(\"Evaluating model...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, targets in tqdm(data_loader, desc='Evaluating'):\n",
        "                images = images.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                logits = self.model(images)\n",
        "                probabilities = torch.softmax(logits, dim=1)\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "                # Calculate top-5 accuracy\n",
        "                _, top5_pred = logits.topk(5, 1, True, True)\n",
        "                top5_correct += (targets.view(-1, 1).expand_as(top5_pred) == top5_pred).sum().item()\n",
        "\n",
        "                # Collect results\n",
        "                all_predictions.extend(predictions.cpu().numpy())\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "                all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "                # Update accuracy\n",
        "                total_correct += (predictions == targets).sum().item()\n",
        "                total_samples += targets.size(0)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy_score = total_correct / total_samples * 100\n",
        "        top5_accuracy_score = top5_correct / total_samples * 100\n",
        "\n",
        "        # Generate classification report\n",
        "        class_report = classification_report(\n",
        "            all_targets,\n",
        "            all_predictions,\n",
        "            target_names=self.class_names,\n",
        "            output_dict=True\n",
        "        )\n",
        "\n",
        "        # Generate confusion matrix\n",
        "        cm = confusion_matrix(all_targets, all_predictions)\n",
        "\n",
        "        # Save results\n",
        "        self._save_classification_report(class_report, save_dir)\n",
        "        self._plot_confusion_matrix(cm, save_dir)\n",
        "        self._plot_class_accuracies(class_report, save_dir)\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        results = {\n",
        "            'accuracy': accuracy_score,  # Changed from 'overall_accuracy' to match main function\n",
        "            'top5_accuracy': top5_accuracy_score,  # Add top-5 accuracy\n",
        "            'macro_avg_precision': class_report['macro avg']['precision'] * 100,\n",
        "            'macro_avg_recall': class_report['macro avg']['recall'] * 100,\n",
        "            'macro_avg_f1': class_report['macro avg']['f1-score'] * 100,\n",
        "            'weighted_avg_precision': class_report['weighted avg']['precision'] * 100,\n",
        "            'weighted_avg_recall': class_report['weighted avg']['recall'] * 100,\n",
        "            'weighted_avg_f1': class_report['weighted avg']['f1-score'] * 100,\n",
        "            'classification_report': class_report  # Add classification report to results\n",
        "        }\n",
        "\n",
        "        print(f\"\\\\nEvaluation Results:\")\n",
        "        print(f\"Top-1 Accuracy: {accuracy_score:.2f}%\")\n",
        "        print(f\"Top-5 Accuracy: {top5_accuracy_score:.2f}%\")\n",
        "        print(f\"Macro Avg F1-Score: {results['macro_avg_f1']:.2f}%\")\n",
        "        print(f\"Weighted Avg F1-Score: {results['weighted_avg_f1']:.2f}%\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _save_classification_report(self, report_dict: Dict, save_dir: str):\n",
        "        \"\"\"Save classification report to file.\"\"\"\n",
        "        import json\n",
        "        report_path = os.path.join(save_dir, 'classification_report.json')\n",
        "        with open(report_path, 'w') as f:\n",
        "            json.dump(report_dict, f, indent=2)\n",
        "        print(f\"Classification report saved to: {report_path}\")\n",
        "\n",
        "    def _plot_confusion_matrix(self, cm: np.ndarray, save_dir: str):\n",
        "        \"\"\"Plot and save confusion matrix.\"\"\"\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(\n",
        "            cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=self.class_names,\n",
        "            yticklabels=self.class_names\n",
        "        )\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        cm_path = os.path.join(save_dir, 'confusion_matrix.png')\n",
        "        plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"Confusion matrix saved to: {cm_path}\")\n",
        "\n",
        "    def _plot_class_accuracies(self, report_dict: Dict, save_dir: str):\n",
        "        \"\"\"Plot per-class accuracies.\"\"\"\n",
        "        class_names = []\n",
        "        accuracies = []\n",
        "\n",
        "        for class_name in self.class_names:\n",
        "            if class_name in report_dict:\n",
        "                class_names.append(class_name)\n",
        "                accuracies.append(report_dict[class_name]['f1-score'] * 100)\n",
        "\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        bars = plt.bar(range(len(class_names)), accuracies)\n",
        "        plt.xlabel('Fish Species')\n",
        "        plt.ylabel('F1-Score (%)')\n",
        "        plt.title('Per-Class F1-Scores')\n",
        "        plt.xticks(range(len(class_names)), class_names, rotation=45, ha='right')\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                    f'{acc:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        acc_path = os.path.join(save_dir, 'class_accuracies.png')\n",
        "        plt.savefig(acc_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"Class accuracies plot saved to: {acc_path}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main evaluation script.\"\"\"\n",
        "    import argparse\n",
        "\n",
        "    # Parse arguments\n",
        "    parser = argparse.ArgumentParser(description='Evaluate ViT-FishID model')\n",
        "    parser.add_argument('--data_dir', type=str, required=True,\n",
        "                        help='Path to fish dataset directory')\n",
        "    parser.add_argument('--model_path', type=str, required=True,\n",
        "                        help='Path to model checkpoint')\n",
        "    parser.add_argument('--batch_size', type=int, default=32,\n",
        "                        help='Batch size for evaluation')\n",
        "    parser.add_argument('--image_size', type=int, default=224,\n",
        "                        help='Input image size')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"üîç Evaluating model: {args.model_path}\")\n",
        "    print(f\"üìä Data directory: {args.data_dir}\")\n",
        "\n",
        "    device = get_device()\n",
        "    print(f\"üñ•Ô∏è  Using device: {device}\")\n",
        "\n",
        "    # Load data - use test_loader for evaluation\n",
        "    train_loader, val_loader, test_loader, class_names = create_dataloaders(\n",
        "        data_dir=args.data_dir,\n",
        "        batch_size=args.batch_size,\n",
        "        image_size=args.image_size\n",
        "    )\n",
        "\n",
        "    print(f\"üìä Found {len(class_names)} classes: {class_names[:5]}...\" if len(class_names) > 5 else f\"üìä Found {len(class_names)} classes: {class_names}\")\n",
        "\n",
        "    # Create model\n",
        "    num_classes = len(class_names)\n",
        "    model = ViTForFishClassification(\n",
        "        num_classes=num_classes,\n",
        "        model_name='vit_small_patch16_224',  # Adjust based on your training config\n",
        "        pretrained=False,\n",
        "        dropout_rate=0.1\n",
        "    ).to(device)\n",
        "\n",
        "    # Load checkpoint\n",
        "    if os.path.exists(args.model_path):\n",
        "        print(f\"üì• Loading checkpoint: {args.model_path}\")\n",
        "        checkpoint = torch.load(args.model_path, map_location=device)\n",
        "\n",
        "        # Handle different checkpoint formats\n",
        "        if 'student_state_dict' in checkpoint:\n",
        "            model.load_state_dict(checkpoint['student_state_dict'])\n",
        "            print(f\"‚úÖ Loaded student model weights\")\n",
        "            if 'best_accuracy' in checkpoint:\n",
        "                print(f\"üìä Training best accuracy: {checkpoint['best_accuracy']:.2f}%\")\n",
        "        elif 'model_state_dict' in checkpoint:\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            print(f\"‚úÖ Loaded model weights\")\n",
        "        else:\n",
        "            model.load_state_dict(checkpoint)\n",
        "            print(f\"‚úÖ Loaded model weights\")\n",
        "    else:\n",
        "        print(f\"‚ùå Checkpoint not found: {args.model_path}\")\n",
        "        return\n",
        "\n",
        "    # Create evaluator\n",
        "    evaluator = ModelEvaluator(model, class_names, device)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(f\"\\\\nüß™ Evaluating on test set...\")\n",
        "    test_results = evaluator.evaluate_dataset(test_loader, \"test\")\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\\\nüìä TEST RESULTS:\")\n",
        "    print(f\"üéØ Accuracy: {test_results['accuracy']:.2f}%\")\n",
        "    print(f\"üìà Top-5 Accuracy: {test_results.get('top5_accuracy', 'N/A')}\")\n",
        "\n",
        "    # Print per-class results\n",
        "    if 'classification_report' in test_results:\n",
        "        print(f\"\\\\nüìã Per-class Performance:\")\n",
        "        class_report = test_results['classification_report']\n",
        "        for class_name in class_names[:10]:  # Show first 10 classes\n",
        "            if class_name in class_report:\n",
        "                precision = class_report[class_name]['precision']\n",
        "                recall = class_report[class_name]['recall']\n",
        "                f1 = class_report[class_name]['f1-score']\n",
        "                print(f\"  {class_name}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}\")\n",
        "\n",
        "        if len(class_names) > 10:\n",
        "            print(f\"  ... and {len(class_names) - 10} more classes\")\n",
        "\n",
        "    # Also evaluate on validation set for comparison\n",
        "    print(f\"\\\\nüîç Evaluating on validation set...\")\n",
        "    val_results = evaluator.evaluate_dataset(val_loader, \"validation\")\n",
        "    print(f\"üìä VALIDATION RESULTS:\")\n",
        "    print(f\"üéØ Accuracy: {val_results['accuracy']:.2f}%\")\n",
        "\n",
        "    print(f\"\\\\n‚úÖ Evaluation completed!\")\n",
        "    print(f\"üìä Final Test Accuracy: {test_results['accuracy']:.2f}%\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "'''\n",
        "\n",
        "# Write the fixed evaluate.py file\n",
        "with open('evaluate.py', 'w') as f:\n",
        "    f.write(evaluate_py_content)\n",
        "\n",
        "print(\"‚úÖ Fixed evaluate.py has been created!\")\n",
        "print(\"üîß Key fixes applied:\")\n",
        "print(\"  ‚úÖ Corrected all import statements\")\n",
        "print(\"  ‚úÖ Added proper argument parsing\")\n",
        "print(\"  ‚úÖ Added Top-5 accuracy calculation\")\n",
        "print(\"  ‚úÖ Fixed function calls and return values\")\n",
        "print(\"  ‚úÖ Simplified evaluation logic\")\n",
        "\n",
        "# Verify the file was created\n",
        "if os.path.exists('evaluate.py'):\n",
        "    file_size = os.path.getsize('evaluate.py')\n",
        "    print(f\"‚úÖ File created successfully ({file_size} bytes)\")\n",
        "    print(\"üöÄ Ready to run evaluation!\")\n",
        "else:\n",
        "    print(\"‚ùå File creation failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "9gc3mDn1CrHD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gc3mDn1CrHD",
        "outputId": "c778145a-4d69-4084-d02f-f7ac5f271a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Fixing class mismatch error in evaluate.py...\n",
            "‚úÖ Fixed classification_report generation\n",
            "‚úÖ Fixed confusion_matrix generation\n",
            "‚úÖ Fixed confusion matrix heatmap\n",
            "‚úÖ Fixed class accuracies plot\n",
            "‚úÖ Fixed evaluate.py to handle class mismatches!\n",
            "üîß Applied fixes:\n",
            "  ‚úÖ Only use classes that appear in test data\n",
            "  ‚úÖ Handle zero-division for missing classes\n",
            "  ‚úÖ Fix confusion matrix dimensions\n",
            "  ‚úÖ Fix plotting functions for subset of classes\n",
            "\n",
            "üöÄ Try running evaluation again!\n"
          ]
        }
      ],
      "source": [
        "# üîß Fix Class Mismatch Error in evaluate.py\n",
        "import os\n",
        "\n",
        "print(\"üîÑ Fixing class mismatch error in evaluate.py...\")\n",
        "\n",
        "# Read the current evaluate.py file\n",
        "with open('/content/ViT-FishID/evaluate.py', 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Find and replace the classification_report generation to handle class mismatch\n",
        "old_classification_report = \"\"\"        # Generate classification report\n",
        "        class_report = classification_report(\n",
        "            all_targets,\n",
        "            all_predictions,\n",
        "            target_names=self.class_names,\n",
        "            output_dict=True\n",
        "        )\"\"\"\n",
        "\n",
        "new_classification_report = \"\"\"        # Generate classification report - handle class mismatch\n",
        "        # Get unique classes that actually appear in the test data\n",
        "        unique_classes = sorted(list(set(all_targets + all_predictions)))\n",
        "        present_class_names = [self.class_names[i] for i in unique_classes if i < len(self.class_names)]\n",
        "\n",
        "        # Generate classification report with only present classes\n",
        "        class_report = classification_report(\n",
        "            all_targets,\n",
        "            all_predictions,\n",
        "            labels=unique_classes,  # Specify which labels to include\n",
        "            target_names=present_class_names,  # Only names for present classes\n",
        "            output_dict=True,\n",
        "            zero_division=0  # Handle division by zero for missing classes\n",
        "        )\"\"\"\n",
        "\n",
        "# Replace in the content\n",
        "if old_classification_report in content:\n",
        "    content = content.replace(old_classification_report, new_classification_report)\n",
        "    print(\"‚úÖ Fixed classification_report generation\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Could not find exact match for classification_report. Applying alternative fix...\")\n",
        "    # Alternative fix - replace the specific line\n",
        "    content = content.replace(\n",
        "        'class_report = classification_report(',\n",
        "        '''# Get unique classes that actually appear in the test data\n",
        "        unique_classes = sorted(list(set(all_targets + all_predictions)))\n",
        "        present_class_names = [self.class_names[i] for i in unique_classes if i < len(self.class_names)]\n",
        "\n",
        "        class_report = classification_report('''\n",
        "    )\n",
        "    content = content.replace(\n",
        "        'target_names=self.class_names,',\n",
        "        'labels=unique_classes,\\n            target_names=present_class_names,'\n",
        "    )\n",
        "    content = content.replace(\n",
        "        'output_dict=True',\n",
        "        'output_dict=True,\\n            zero_division=0'\n",
        "    )\n",
        "\n",
        "# Also fix the confusion matrix to handle missing classes\n",
        "old_cm = \"cm = confusion_matrix(all_targets, all_predictions)\"\n",
        "new_cm = \"\"\"cm = confusion_matrix(all_targets, all_predictions, labels=unique_classes)\"\"\"\n",
        "\n",
        "if old_cm in content:\n",
        "    content = content.replace(old_cm, new_cm)\n",
        "    print(\"‚úÖ Fixed confusion_matrix generation\")\n",
        "\n",
        "# Fix the plotting functions to handle subset of classes\n",
        "old_heatmap = \"\"\"sns.heatmap(\n",
        "            cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=self.class_names,\n",
        "            yticklabels=self.class_names\n",
        "        )\"\"\"\n",
        "\n",
        "new_heatmap = \"\"\"sns.heatmap(\n",
        "            cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=present_class_names,  # Use only present classes\n",
        "            yticklabels=present_class_names   # Use only present classes\n",
        "        )\"\"\"\n",
        "\n",
        "if old_heatmap in content:\n",
        "    content = content.replace(old_heatmap, new_heatmap)\n",
        "    print(\"‚úÖ Fixed confusion matrix heatmap\")\n",
        "\n",
        "# Fix class accuracies plot\n",
        "old_class_plot = \"\"\"for class_name in self.class_names:\n",
        "            if class_name in report_dict:\"\"\"\n",
        "\n",
        "new_class_plot = \"\"\"for class_name in present_class_names:  # Use only present classes\n",
        "            if class_name in report_dict:\"\"\"\n",
        "\n",
        "if old_class_plot in content:\n",
        "    content = content.replace(old_class_plot, new_class_plot)\n",
        "    print(\"‚úÖ Fixed class accuracies plot\")\n",
        "\n",
        "# Write the fixed content back to the file\n",
        "with open('/content/ViT-FishID/evaluate.py', 'w') as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"‚úÖ Fixed evaluate.py to handle class mismatches!\")\n",
        "print(\"üîß Applied fixes:\")\n",
        "print(\"  ‚úÖ Only use classes that appear in test data\")\n",
        "print(\"  ‚úÖ Handle zero-division for missing classes\")\n",
        "print(\"  ‚úÖ Fix confusion matrix dimensions\")\n",
        "print(\"  ‚úÖ Fix plotting functions for subset of classes\")\n",
        "print(\"\\nüöÄ Try running evaluation again!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "w2M02i_qCteO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2M02i_qCteO",
        "outputId": "dd17f752-3d91-4818-9d72-5143d6e8850d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Fixing present_class_names scope error...\n",
            "üìù Applying fixes...\n",
            "‚úÖ Fixed plotting function calls\n",
            "‚úÖ Fixed _plot_confusion_matrix signature\n",
            "‚úÖ Fixed heatmap class names\n",
            "‚úÖ Fixed _plot_class_accuracies signature\n",
            "‚úÖ Fixed class accuracies loop\n",
            "‚úÖ Fixed all scope errors!\n",
            "üîß Applied fixes:\n",
            "  ‚úÖ Pass class names to plotting functions\n",
            "  ‚úÖ Update function signatures\n",
            "  ‚úÖ Fix variable references in plots\n",
            "\n",
            "üöÄ Try running evaluation again!\n"
          ]
        }
      ],
      "source": [
        "# üîß Fix present_class_names Scope Error\n",
        "import os\n",
        "\n",
        "print(\"üîÑ Fixing present_class_names scope error...\")\n",
        "\n",
        "# Read the current evaluate.py file\n",
        "with open('/content/ViT-FishID/evaluate.py', 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "print(\"üìù Applying fixes...\")\n",
        "\n",
        "# Fix 1: Update the evaluate_dataset method to pass present_class_names to plotting functions\n",
        "old_save_results = \"\"\"        # Save results\n",
        "        self._save_classification_report(class_report, save_dir)\n",
        "        self._plot_confusion_matrix(cm, save_dir)\n",
        "        self._plot_class_accuracies(class_report, save_dir)\"\"\"\n",
        "\n",
        "new_save_results = \"\"\"        # Save results\n",
        "        self._save_classification_report(class_report, save_dir)\n",
        "        self._plot_confusion_matrix(cm, save_dir, present_class_names)\n",
        "        self._plot_class_accuracies(class_report, save_dir, present_class_names)\"\"\"\n",
        "\n",
        "if old_save_results in content:\n",
        "    content = content.replace(old_save_results, new_save_results)\n",
        "    print(\"‚úÖ Fixed plotting function calls\")\n",
        "\n",
        "# Fix 2: Update _plot_confusion_matrix method signature\n",
        "old_plot_cm_def = \"def _plot_confusion_matrix(self, cm: np.ndarray, save_dir: str):\"\n",
        "new_plot_cm_def = \"def _plot_confusion_matrix(self, cm: np.ndarray, save_dir: str, class_names_subset: List[str]):\"\n",
        "\n",
        "if old_plot_cm_def in content:\n",
        "    content = content.replace(old_plot_cm_def, new_plot_cm_def)\n",
        "    print(\"‚úÖ Fixed _plot_confusion_matrix signature\")\n",
        "\n",
        "# Fix 3: Update heatmap call in _plot_confusion_matrix\n",
        "old_heatmap = \"\"\"xticklabels=present_class_names,  # Use only present classes\n",
        "            yticklabels=present_class_names   # Use only present classes\"\"\"\n",
        "\n",
        "new_heatmap = \"\"\"xticklabels=class_names_subset,  # Use only present classes\n",
        "            yticklabels=class_names_subset   # Use only present classes\"\"\"\n",
        "\n",
        "if old_heatmap in content:\n",
        "    content = content.replace(old_heatmap, new_heatmap)\n",
        "    print(\"‚úÖ Fixed heatmap class names\")\n",
        "\n",
        "# Fix 4: Update _plot_class_accuracies method signature\n",
        "old_plot_acc_def = \"def _plot_class_accuracies(self, report_dict: Dict, save_dir: str):\"\n",
        "new_plot_acc_def = \"def _plot_class_accuracies(self, report_dict: Dict, save_dir: str, class_names_subset: List[str]):\"\n",
        "\n",
        "if old_plot_acc_def in content:\n",
        "    content = content.replace(old_plot_acc_def, new_plot_acc_def)\n",
        "    print(\"‚úÖ Fixed _plot_class_accuracies signature\")\n",
        "\n",
        "# Fix 5: Update the loop in _plot_class_accuracies\n",
        "old_class_loop = \"for class_name in present_class_names:  # Use only present classes\"\n",
        "new_class_loop = \"for class_name in class_names_subset:  # Use only present classes\"\n",
        "\n",
        "if old_class_loop in content:\n",
        "    content = content.replace(old_class_loop, new_class_loop)\n",
        "    print(\"‚úÖ Fixed class accuracies loop\")\n",
        "\n",
        "# Write the fixed content back\n",
        "with open('/content/ViT-FishID/evaluate.py', 'w') as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"‚úÖ Fixed all scope errors!\")\n",
        "print(\"üîß Applied fixes:\")\n",
        "print(\"  ‚úÖ Pass class names to plotting functions\")\n",
        "print(\"  ‚úÖ Update function signatures\")\n",
        "print(\"  ‚úÖ Fix variable references in plots\")\n",
        "print(\"\\nüöÄ Try running evaluation again!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "mVbt4arqCwRm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVbt4arqCwRm",
        "outputId": "cf370b52-6186-4853-ba73-50d5c44f4d19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Creating completely clean evaluate.py...\n",
            "/content/ViT-FishID\n",
            "‚úÖ Created clean, working evaluate.py!\n",
            "üîß Features of the new script:\n",
            "  ‚úÖ Robust class mismatch handling\n",
            "  ‚úÖ Proper error handling throughout\n",
            "  ‚úÖ Clean function scoping\n",
            "  ‚úÖ Both Top-1 and Top-5 accuracy\n",
            "  ‚úÖ Comprehensive result saving\n",
            "  ‚úÖ Validation and test set evaluation\n",
            "‚úÖ Clean script created (12401 bytes)\n",
            "\\nüöÄ Ready to run evaluation without errors!\n"
          ]
        }
      ],
      "source": [
        "# üÜï Create Clean, Working evaluate.py (Complete Replacement)\n",
        "import os\n",
        "\n",
        "print(\"üîÑ Creating completely clean evaluate.py...\")\n",
        "\n",
        "# Ensure we're in the right directory\n",
        "%cd /content/ViT-FishID\n",
        "\n",
        "# Create the clean, working evaluate.py content\n",
        "clean_evaluate_py = '''import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "from model import ViTForFishClassification\n",
        "from data import create_dataloaders\n",
        "from utils import get_device\n",
        "\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"Clean, robust model evaluation for ViT-Fish classification.\"\"\"\n",
        "\n",
        "    def __init__(self, model: ViTForFishClassification, class_names: List[str], device: str = 'cuda'):\n",
        "        self.model = model\n",
        "        self.class_names = class_names\n",
        "        self.device = device\n",
        "        self.model.eval()\n",
        "\n",
        "    def evaluate_dataset(self, data_loader: DataLoader, save_dir: str = './evaluation_results') -> Dict[str, float]:\n",
        "        \"\"\"Comprehensive evaluation on dataset.\"\"\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "        all_probabilities = []\n",
        "\n",
        "        print(\"Evaluating model...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, targets in tqdm(data_loader, desc='Evaluating'):\n",
        "                images = images.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                logits = self.model(images)\n",
        "                probabilities = torch.softmax(logits, dim=1)\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "                # Collect results\n",
        "                all_predictions.extend(predictions.cpu().numpy())\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "                all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        all_predictions = np.array(all_predictions)\n",
        "        all_targets = np.array(all_targets)\n",
        "        all_probabilities = np.array(all_probabilities)\n",
        "\n",
        "        # Calculate Top-1 accuracy\n",
        "        top1_accuracy = accuracy_score(all_targets, all_predictions) * 100\n",
        "\n",
        "        # Calculate Top-5 accuracy\n",
        "        top5_correct = 0\n",
        "        for i, target in enumerate(all_targets):\n",
        "            top5_preds = np.argsort(all_probabilities[i])[-5:]  # Top 5 predictions\n",
        "            if target in top5_preds:\n",
        "                top5_correct += 1\n",
        "        top5_accuracy = (top5_correct / len(all_targets)) * 100\n",
        "\n",
        "        # Get unique classes that appear in the data\n",
        "        unique_classes = sorted(list(set(all_targets.tolist() + all_predictions.tolist())))\n",
        "        present_class_names = [self.class_names[i] for i in unique_classes if i < len(self.class_names)]\n",
        "\n",
        "        print(f\"\\\\nFound {len(unique_classes)} classes in test data (out of {len(self.class_names)} total)\")\n",
        "\n",
        "        # Generate classification report with proper handling\n",
        "        try:\n",
        "            class_report = classification_report(\n",
        "                all_targets,\n",
        "                all_predictions,\n",
        "                labels=unique_classes,\n",
        "                target_names=present_class_names,\n",
        "                output_dict=True,\n",
        "                zero_division=0\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not generate full classification report: {e}\")\n",
        "            class_report = {}\n",
        "\n",
        "        # Generate confusion matrix\n",
        "        cm = confusion_matrix(all_targets, all_predictions, labels=unique_classes)\n",
        "\n",
        "        # Save results\n",
        "        self._save_results(class_report, save_dir)\n",
        "        self._save_confusion_matrix(cm, present_class_names, save_dir)\n",
        "        self._save_class_accuracies(class_report, save_dir)\n",
        "\n",
        "        # Prepare results dictionary\n",
        "        results = {\n",
        "            'accuracy': top1_accuracy,\n",
        "            'top5_accuracy': top5_accuracy,\n",
        "            'num_test_classes': len(unique_classes),\n",
        "            'total_classes': len(self.class_names),\n",
        "            'classification_report': class_report\n",
        "        }\n",
        "\n",
        "        # Add aggregate metrics if available\n",
        "        if 'macro avg' in class_report:\n",
        "            results.update({\n",
        "                'macro_avg_precision': class_report['macro avg']['precision'] * 100,\n",
        "                'macro_avg_recall': class_report['macro avg']['recall'] * 100,\n",
        "                'macro_avg_f1': class_report['macro avg']['f1-score'] * 100,\n",
        "            })\n",
        "\n",
        "        if 'weighted avg' in class_report:\n",
        "            results.update({\n",
        "                'weighted_avg_precision': class_report['weighted avg']['precision'] * 100,\n",
        "                'weighted_avg_recall': class_report['weighted avg']['recall'] * 100,\n",
        "                'weighted_avg_f1': class_report['weighted avg']['f1-score'] * 100,\n",
        "            })\n",
        "\n",
        "        # Print results\n",
        "        print(f\"\\\\nüìä EVALUATION RESULTS:\")\n",
        "        print(f\"üéØ Top-1 Accuracy: {top1_accuracy:.2f}%\")\n",
        "        print(f\"üìà Top-5 Accuracy: {top5_accuracy:.2f}%\")\n",
        "        if 'macro_avg_f1' in results:\n",
        "            print(f\"üìã Macro Avg F1-Score: {results['macro_avg_f1']:.2f}%\")\n",
        "        if 'weighted_avg_f1' in results:\n",
        "            print(f\"‚öñÔ∏è  Weighted Avg F1-Score: {results['weighted_avg_f1']:.2f}%\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _save_results(self, report_dict: Dict, save_dir: str):\n",
        "        \"\"\"Save classification report to JSON.\"\"\"\n",
        "        if report_dict:\n",
        "            report_path = os.path.join(save_dir, 'classification_report.json')\n",
        "            try:\n",
        "                with open(report_path, 'w') as f:\n",
        "                    json.dump(report_dict, f, indent=2)\n",
        "                print(f\"Classification report saved to: {report_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not save classification report: {e}\")\n",
        "\n",
        "    def _save_confusion_matrix(self, cm: np.ndarray, class_names: List[str], save_dir: str):\n",
        "        \"\"\"Save confusion matrix plot.\"\"\"\n",
        "        try:\n",
        "            plt.figure(figsize=(max(10, len(class_names) * 0.5), max(8, len(class_names) * 0.4)))\n",
        "            sns.heatmap(\n",
        "                cm,\n",
        "                annot=True,\n",
        "                fmt='d',\n",
        "                cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names,\n",
        "                cbar_kws={'label': 'Count'}\n",
        "            )\n",
        "            plt.title('Confusion Matrix')\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('Actual')\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.yticks(rotation=0)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            cm_path = os.path.join(save_dir, 'confusion_matrix.png')\n",
        "            plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            print(f\"Confusion matrix saved to: {cm_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not save confusion matrix: {e}\")\n",
        "\n",
        "    def _save_class_accuracies(self, report_dict: Dict, save_dir: str):\n",
        "        \"\"\"Save per-class accuracies plot.\"\"\"\n",
        "        if not report_dict:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            class_names = []\n",
        "            f1_scores = []\n",
        "\n",
        "            for class_name in self.class_names:\n",
        "                if class_name in report_dict:\n",
        "                    class_names.append(class_name)\n",
        "                    f1_scores.append(report_dict[class_name]['f1-score'] * 100)\n",
        "\n",
        "            if not class_names:\n",
        "                print(\"Warning: No per-class data available for plotting\")\n",
        "                return\n",
        "\n",
        "            plt.figure(figsize=(max(12, len(class_names) * 0.3), 8))\n",
        "            bars = plt.bar(range(len(class_names)), f1_scores, color='skyblue', alpha=0.7)\n",
        "            plt.xlabel('Fish Species')\n",
        "            plt.ylabel('F1-Score (%)')\n",
        "            plt.title('Per-Class F1-Scores')\n",
        "            plt.xticks(range(len(class_names)), class_names, rotation=45, ha='right')\n",
        "            plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for bar, score in zip(bars, f1_scores):\n",
        "                height = bar.get_height()\n",
        "                plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                        f'{score:.1f}%', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            acc_path = os.path.join(save_dir, 'class_f1_scores.png')\n",
        "            plt.savefig(acc_path, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            print(f\"Class F1-scores plot saved to: {acc_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not save class accuracies plot: {e}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main evaluation script.\"\"\"\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Evaluate ViT-FishID model')\n",
        "    parser.add_argument('--data_dir', type=str, required=True, help='Path to fish dataset directory')\n",
        "    parser.add_argument('--model_path', type=str, required=True, help='Path to model checkpoint')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for evaluation')\n",
        "    parser.add_argument('--image_size', type=int, default=224, help='Input image size')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"üîç Evaluating model: {args.model_path}\")\n",
        "    print(f\"üìä Data directory: {args.data_dir}\")\n",
        "\n",
        "    device = get_device()\n",
        "    print(f\"üñ•Ô∏è  Using device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    try:\n",
        "        train_loader, val_loader, test_loader, class_names = create_dataloaders(\n",
        "            data_dir=args.data_dir,\n",
        "            batch_size=args.batch_size,\n",
        "            image_size=args.image_size\n",
        "        )\n",
        "        print(f\"üìä Found {len(class_names)} total classes\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Data loading error: {e}\")\n",
        "        return\n",
        "\n",
        "    # Create model\n",
        "    try:\n",
        "        num_classes = len(class_names)\n",
        "        model = ViTForFishClassification(\n",
        "            num_classes=num_classes,\n",
        "            model_name='vit_small_patch16_224',\n",
        "            pretrained=False,\n",
        "            dropout_rate=0.1\n",
        "        ).to(device)\n",
        "\n",
        "        # Load checkpoint\n",
        "        if os.path.exists(args.model_path):\n",
        "            print(f\"üì• Loading checkpoint: {args.model_path}\")\n",
        "            checkpoint = torch.load(args.model_path, map_location=device)\n",
        "\n",
        "            if 'student_state_dict' in checkpoint:\n",
        "                model.load_state_dict(checkpoint['student_state_dict'])\n",
        "                print(\"‚úÖ Loaded student model weights\")\n",
        "                if 'best_accuracy' in checkpoint:\n",
        "                    print(f\"üìä Training best accuracy: {checkpoint['best_accuracy']:.2f}%\")\n",
        "            elif 'model_state_dict' in checkpoint:\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                print(\"‚úÖ Loaded model weights\")\n",
        "            else:\n",
        "                model.load_state_dict(checkpoint)\n",
        "                print(\"‚úÖ Loaded model weights\")\n",
        "        else:\n",
        "            print(f\"‚ùå Checkpoint not found: {args.model_path}\")\n",
        "            return\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Model loading error: {e}\")\n",
        "        return\n",
        "\n",
        "    # Create evaluator and run evaluation\n",
        "    try:\n",
        "        evaluator = ModelEvaluator(model, class_names, device)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        print(f\"\\\\nüß™ Evaluating on test set...\")\n",
        "        test_results = evaluator.evaluate_dataset(test_loader, \"test_results\")\n",
        "\n",
        "        # Evaluate on validation set for comparison\n",
        "        print(f\"\\\\nüîç Evaluating on validation set...\")\n",
        "        val_results = evaluator.evaluate_dataset(val_loader, \"validation_results\")\n",
        "\n",
        "        # Final summary\n",
        "        print(f\"\\\\n\" + \"=\"*60)\n",
        "        print(f\"üéâ EVALUATION COMPLETED!\")\n",
        "        print(f\"=\"*60)\n",
        "        print(f\"üìä TEST SET RESULTS:\")\n",
        "        print(f\"  üéØ Top-1 Accuracy: {test_results['accuracy']:.2f}%\")\n",
        "        print(f\"  üìà Top-5 Accuracy: {test_results['top5_accuracy']:.2f}%\")\n",
        "        print(f\"\\\\nüìä VALIDATION SET RESULTS:\")\n",
        "        print(f\"  üéØ Top-1 Accuracy: {val_results['accuracy']:.2f}%\")\n",
        "        print(f\"  üìà Top-5 Accuracy: {val_results['top5_accuracy']:.2f}%\")\n",
        "        print(f\"\\\\nüìã Dataset Info:\")\n",
        "        print(f\"  Total classes: {test_results['total_classes']}\")\n",
        "        print(f\"  Classes in test set: {test_results['num_test_classes']}\")\n",
        "        print(f\"\\\\nüíæ Results saved to: test_results/ and validation_results/\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Evaluation error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "'''\n",
        "\n",
        "# Write the clean evaluate.py file\n",
        "with open('evaluate.py', 'w') as f:\n",
        "    f.write(clean_evaluate_py)\n",
        "\n",
        "print(\"‚úÖ Created clean, working evaluate.py!\")\n",
        "print(\"üîß Features of the new script:\")\n",
        "print(\"  ‚úÖ Robust class mismatch handling\")\n",
        "print(\"  ‚úÖ Proper error handling throughout\")\n",
        "print(\"  ‚úÖ Clean function scoping\")\n",
        "print(\"  ‚úÖ Both Top-1 and Top-5 accuracy\")\n",
        "print(\"  ‚úÖ Comprehensive result saving\")\n",
        "print(\"  ‚úÖ Validation and test set evaluation\")\n",
        "\n",
        "# Verify the file\n",
        "if os.path.exists('evaluate.py'):\n",
        "    file_size = os.path.getsize('evaluate.py')\n",
        "    print(f\"‚úÖ Clean script created ({file_size} bytes)\")\n",
        "    print(\"\\\\nüöÄ Ready to run evaluation without errors!\")\n",
        "else:\n",
        "    print(\"‚ùå File creation failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "lyVZ3MWRCz94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyVZ3MWRCz94",
        "outputId": "e5440339-e897-4cdb-c033-1821795929e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing the clean evaluation script...\n",
            "‚úÖ Successfully imported ModelEvaluator\n",
            "‚úÖ Script runs without syntax errors\n",
            "\\nüìã Available options:\n",
            "[]\n",
            "\\n==================================================\n",
            "üéØ NOW RUN YOUR EVALUATION WITH:\n",
            "==================================================\n",
            "!cd /content/ViT-FishID && python evaluate.py \\\\\n",
            "    --data_dir /content/fish_cutouts \\\\\n",
            "    --model_path /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth \\\\\n",
            "    --batch_size 32 \\\\\n",
            "    --image_size 224\n",
            "\\nüéâ The clean script should handle all errors gracefully!\n",
            "üìä Expected output: Both test and validation accuracies\n",
            "üíæ Results will be saved to test_results/ and validation_results/ folders\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Test Clean Evaluation Script\n",
        "print(\"üß™ Testing the clean evaluation script...\")\n",
        "\n",
        "# Quick import test\n",
        "try:\n",
        "    import sys\n",
        "    sys.path.append('/content/ViT-FishID')\n",
        "\n",
        "    from evaluate import ModelEvaluator\n",
        "    print(\"‚úÖ Successfully imported ModelEvaluator\")\n",
        "\n",
        "    # Test if the script can run (argument check)\n",
        "    import subprocess\n",
        "    result = subprocess.run(['python', '/content/ViT-FishID/evaluate.py', '--help'],\n",
        "                          capture_output=True, text=True, cwd='/content/ViT-FishID')\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(\"‚úÖ Script runs without syntax errors\")\n",
        "        print(\"\\\\nüìã Available options:\")\n",
        "        print(result.stdout.split('\\\\n')[1:6])  # Show first few help lines\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Script has some issues:\")\n",
        "        print(result.stderr[:200])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error testing script: {e}\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "print(\"üéØ NOW RUN YOUR EVALUATION WITH:\")\n",
        "print(\"=\"*50)\n",
        "print(\"!cd /content/ViT-FishID && python evaluate.py \\\\\\\\\")\n",
        "print(\"    --data_dir /content/fish_cutouts \\\\\\\\\")\n",
        "print(\"    --model_path /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth \\\\\\\\\")\n",
        "print(\"    --batch_size 32 \\\\\\\\\")\n",
        "print(\"    --image_size 224\")\n",
        "\n",
        "print(\"\\\\nüéâ The clean script should handle all errors gracefully!\")\n",
        "print(\"üìä Expected output: Both test and validation accuracies\")\n",
        "print(\"üíæ Results will be saved to test_results/ and validation_results/ folders\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "aMnGqr3OC5Xz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMnGqr3OC5Xz",
        "outputId": "703ef2bd-fc5f-46e9-f65f-896318177c2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-18 16:38:37.340547: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755535117.361066  117777 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755535117.367256  117777 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755535117.382804  117777 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755535117.382829  117777 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755535117.382833  117777 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755535117.382838  117777 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "üîç Evaluating model: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "üìä Data directory: /content/fish_cutouts\n",
            "Using GPU: Tesla T4\n",
            "üñ•Ô∏è  Using device: cuda\n",
            "‚ö†Ô∏è  Warning: Some classes have only 1 sample(s). Using random splitting instead of stratified.\n",
            "   Classes with 1 sample: ['Carangidae_Caranx_heberi', 'Serranidae_Lipropoma_spp1', 'Sparidae_Sparodon_durbanesis']\n",
            "/content/ViT-FishID/data.py:229: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "üìä Supervised data loaders created:\n",
            "  - Train samples: 3,084\n",
            "  - Val samples: 1,029\n",
            "  - Test samples: 1,029\n",
            "  - Classes: 37\n",
            "  - Split ratios: Train=60.0%, Val=20.0%, Test=20.0%\n",
            "üìä Found 37 total classes\n",
            "üì• Loading checkpoint: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth\n",
            "‚úÖ Loaded student model weights\n",
            "üìä Training best accuracy: 58.02%\n",
            "\n",
            "üß™ Evaluating on test set...\n",
            "Evaluating model...\n",
            "Evaluating: 100% 33/33 [00:04<00:00,  7.56it/s]\n",
            "\n",
            "Found 30 classes in test data (out of 37 total)\n",
            "Classification report saved to: test_results/classification_report.json\n",
            "Confusion matrix saved to: test_results/confusion_matrix.png\n",
            "Class F1-scores plot saved to: test_results/class_f1_scores.png\n",
            "\n",
            "üìä EVALUATION RESULTS:\n",
            "üéØ Top-1 Accuracy: 53.74%\n",
            "üìà Top-5 Accuracy: 87.95%\n",
            "üìã Macro Avg F1-Score: 35.97%\n",
            "‚öñÔ∏è  Weighted Avg F1-Score: 52.04%\n",
            "\n",
            "üîç Evaluating on validation set...\n",
            "Evaluating model...\n",
            "Evaluating:   0% 0/33 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Evaluating: 100% 33/33 [00:03<00:00,  8.68it/s]\n",
            "\n",
            "Found 35 classes in test data (out of 37 total)\n",
            "Classification report saved to: validation_results/classification_report.json\n",
            "Confusion matrix saved to: validation_results/confusion_matrix.png\n",
            "Class F1-scores plot saved to: validation_results/class_f1_scores.png\n",
            "\n",
            "üìä EVALUATION RESULTS:\n",
            "üéØ Top-1 Accuracy: 58.02%\n",
            "üìà Top-5 Accuracy: 86.39%\n",
            "üìã Macro Avg F1-Score: 35.51%\n",
            "‚öñÔ∏è  Weighted Avg F1-Score: 57.02%\n",
            "\n",
            "============================================================\n",
            "üéâ EVALUATION COMPLETED!\n",
            "============================================================\n",
            "üìä TEST SET RESULTS:\n",
            "  üéØ Top-1 Accuracy: 53.74%\n",
            "  üìà Top-5 Accuracy: 87.95%\n",
            "\n",
            "üìä VALIDATION SET RESULTS:\n",
            "  üéØ Top-1 Accuracy: 58.02%\n",
            "  üìà Top-5 Accuracy: 86.39%\n",
            "\n",
            "üìã Dataset Info:\n",
            "  Total classes: 37\n",
            "  Classes in test set: 30\n",
            "\n",
            "üíæ Results saved to: test_results/ and validation_results/\n"
          ]
        }
      ],
      "source": [
        "!cd /content/ViT-FishID && python evaluate.py \\\n",
        "    --data_dir /content/fish_cutouts \\\n",
        "    --model_path /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints/model_best.pth \\\n",
        "    --batch_size 32 \\\n",
        "    --image_size 224"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d9d05d",
      "metadata": {
        "id": "c7d9d05d"
      },
      "source": [
        "## üîç Step 12b: Diagnose `ModuleNotFoundError`\n",
        "\n",
        "This step checks the file structure and import statements to understand why `vit_model` is not being found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca6d7a1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca6d7a1c",
        "outputId": "fc236a40-4bb0-4502-f8f5-aa6c01821099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Diagnosing ModuleNotFoundError...\n",
            "Repo directory: /content/ViT-FishID\n",
            "\n",
            "üìÇ Files in repository root:\n",
            "total 368\n",
            "drwxr-xr-x 6 root root   4096 Aug 15 07:03 .\n",
            "drwxr-xr-x 1 root root   4096 Aug 15 06:58 ..\n",
            "-rw-r--r-- 1 root root  21217 Aug 15 06:58 data.py\n",
            "-rw-r--r-- 1 root root  11572 Aug 15 06:58 evaluate.py\n",
            "-rw-r--r-- 1 root root   3328 Aug 15 06:58 EXTENDED_TRAINING_SETUP.md\n",
            "drwxr-xr-x 2 root root   4096 Aug 15 06:58 fish_cutouts\n",
            "drwxr-xr-x 8 root root   4096 Aug 15 06:58 .git\n",
            "-rw-r--r-- 1 root root     66 Aug 15 06:58 .gitattributes\n",
            "-rw-r--r-- 1 root root    646 Aug 15 06:58 .gitignore\n",
            "-rw-r--r-- 1 root root   9495 Aug 15 06:58 model.py\n",
            "-rw-r--r-- 1 root root  16771 Aug 15 06:58 pipeline.py\n",
            "drwxr-xr-x 2 root root   4096 Aug 15 07:03 __pycache__\n",
            "-rw-r--r-- 1 root root  16566 Aug 15 06:58 README.md\n",
            "-rw-r--r-- 1 root root    202 Aug 15 06:58 requirements.txt\n",
            "-rw-r--r-- 1 root root   4265 Aug 15 06:58 resume_training.py\n",
            "-rw-r--r-- 1 root root   5134 Aug 15 06:58 species_mapping.txt\n",
            "-rw-r--r-- 1 root root  25498 Aug 15 07:03 trainer.py\n",
            "-rw-r--r-- 1 root root   4982 Aug 15 06:58 TRAINING_FIXES_APPLIED.md\n",
            "-rw-r--r-- 1 root root  15331 Aug 15 06:58 train.py\n",
            "-rw-r--r-- 1 root root   8818 Aug 15 06:58 utils.py\n",
            "-rw-r--r-- 1 root root 160971 Aug 15 06:58 ViT_FishID_Colab_Training.ipynb\n",
            "drwxr-xr-x 3 root root   4096 Aug 15 07:03 wandb\n",
            "\n",
            "üìÑ Content of evaluate.py (checking import):\n",
            "  Line 1: import torch\n",
            "  Line 2: import torch.nn as nn\n",
            "  Line 3: from torch.utils.data import DataLoader\n",
            "  Line 4: import numpy as np\n",
            "  Line 5: from sklearn.metrics import classification_report, confusion_matrix\n",
            "  Line 6: import matplotlib.pyplot as plt\n",
            "  Line 7: import seaborn as sns\n",
            "  Line 8: from typing import Dict, List, Tuple\n",
            "  Line 9: import os\n",
            "  Line 10: from tqdm import tqdm\n",
            "  Line 11: \n",
            "  Line 12: from vit_model import ViTForFishClassification\n",
            "  Line 12: from vit_model import ViTForFishClassification\n",
            "  Line 13: from ema_teacher import EMATeacher\n",
            "  Line 14: from data_loader import create_fish_dataloaders\n",
            "  Line 15: from utils import accuracy, load_checkpoint, get_device\n",
            "  Line 16: \n",
            "  Line 17: \n",
            "  Line 18: class ModelEvaluator:\n",
            "  Line 19: \"\"\"\n",
            "  Line 20: Comprehensive model evaluation for ViT-Fish classification.\n",
            "  Line 25: model: ViTForFishClassification, (contains class name)\n",
            "  Line 236: student_model: ViTForFishClassification, (contains class name)\n",
            "  Line 237: teacher_model: ViTForFishClassification, (contains class name)\n",
            "  Line 311: student_model = ViTForFishClassification(num_classes=num_classes) (contains class name)\n",
            "  Line 318: teacher_model = ViTForFishClassification(num_classes=num_classes) (contains class name)\n",
            "\n",
            "üìÑ Checking potential model file: model.py\n",
            "‚úÖ Found model.py. Checking for class definition...\n",
            "  Line 22: class ViTForFishClassification(nn.Module):\n",
            "\n",
            "üìÑ Checking alternative model file: vit_model.py\n",
            "‚ùì vit_model.py not found.\n",
            "\n",
            "Diagnosis steps complete. Please review the output.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(\"üîç Diagnosing ModuleNotFoundError...\")\n",
        "repo_dir = '/content/ViT-FishID'\n",
        "eval_script_path = os.path.join(repo_dir, 'evaluate.py')\n",
        "model_file_guess = os.path.join(repo_dir, 'model.py') # Common name for model file\n",
        "vit_model_file_guess = os.path.join(repo_dir, 'vit_model.py') # Guessed name based on import\n",
        "\n",
        "print(f\"Repo directory: {repo_dir}\")\n",
        "\n",
        "print(\"\\nüìÇ Files in repository root:\")\n",
        "# List files in the repository root\n",
        "if os.path.exists(repo_dir):\n",
        "    !ls -la {repo_dir}\n",
        "else:\n",
        "    print(f\"‚ùå Repository directory not found: {repo_dir}\")\n",
        "\n",
        "\n",
        "print(f\"\\nüìÑ Content of {os.path.basename(eval_script_path)} (checking import):\")\n",
        "# Read and print the content of evaluate.py\n",
        "if os.path.exists(eval_script_path):\n",
        "    try:\n",
        "        with open(eval_script_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for i, line in enumerate(lines):\n",
        "                if 'import vit_model' in line or 'from vit_model' in line:\n",
        "                    print(f\"  Line {i+1}: {line.strip()}\")\n",
        "                elif 'ViTForFishClassification' in line:\n",
        "                     print(f\"  Line {i+1}: {line.strip()} (contains class name)\")\n",
        "                if i < 20: # Print first 20 lines for context\n",
        "                     print(f\"  Line {i+1}: {line.strip()}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Could not read {eval_script_path}: {e}\")\n",
        "else:\n",
        "    print(f\"‚ùå {eval_script_path} not found.\")\n",
        "\n",
        "\n",
        "print(f\"\\nüìÑ Checking potential model file: {os.path.basename(model_file_guess)}\")\n",
        "# Check if model.py exists and print relevant lines\n",
        "if os.path.exists(model_file_guess):\n",
        "    try:\n",
        "        with open(model_file_guess, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            print(f\"‚úÖ Found {os.path.basename(model_file_guess)}. Checking for class definition...\")\n",
        "            found_class = False\n",
        "            for i, line in enumerate(lines):\n",
        "                 if 'class ViTForFishClassification' in line:\n",
        "                      print(f\"  Line {i+1}: {line.strip()}\")\n",
        "                      found_class = True\n",
        "                      break # Found the class, stop searching\n",
        "\n",
        "            if not found_class:\n",
        "                 print(f\"‚ö†Ô∏è 'ViTForFishClassification' class definition not found in {os.path.basename(model_file_guess)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Could not read {model_file_guess}: {e}\")\n",
        "else:\n",
        "    print(f\"‚ùì {os.path.basename(model_file_guess)} not found. Checking alternative name...\")\n",
        "\n",
        "print(f\"\\nüìÑ Checking alternative model file: {os.path.basename(vit_model_file_guess)}\")\n",
        "# Check if vit_model.py exists and print relevant lines\n",
        "if os.path.exists(vit_model_file_guess):\n",
        "    try:\n",
        "        with open(vit_model_file_guess, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            print(f\"‚úÖ Found {os.path.basename(vit_model_file_guess)}. Checking for class definition...\")\n",
        "            found_class = False\n",
        "            for i, line in enumerate(lines):\n",
        "                 if 'class ViTForFishClassification' in line:\n",
        "                      print(f\"  Line {i+1}: {line.strip()}\")\n",
        "                      found_class = True\n",
        "                      break # Found the class, stop searching\n",
        "\n",
        "            if not found_class:\n",
        "                 print(f\"‚ö†Ô∏è 'ViTForFishClassification' class definition not found in {os.path.basename(vit_model_file_guess)}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Could not read {vit_model_file_guess}: {e}\")\n",
        "else:\n",
        "    print(f\"‚ùì {os.path.basename(vit_model_file_guess)} not found.\")\n",
        "\n",
        "print(\"\\nDiagnosis steps complete. Please review the output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8305def7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8305def7",
        "outputId": "34d5ab6a-43c0-476e-b6af-ca604c1892a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking the contents of the MAE checkpoint directory:\n",
            "‚ùå Directory not found: /content/drive/MyDrive/mae_checkpoints\n",
            "Please ensure the directory exists in your Google Drive.\n",
            "\n",
            "--- Check complete ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(\"Checking the contents of the MAE checkpoint directory:\")\n",
        "mae_checkpoint_dir = '/content/drive/MyDrive/mae_checkpoints'\n",
        "\n",
        "if os.path.exists(mae_checkpoint_dir):\n",
        "    print(f\"‚úÖ Directory found: {mae_checkpoint_dir}\")\n",
        "    print(\"\\nFiles in the directory:\")\n",
        "    try:\n",
        "        # List all items in the directory\n",
        "        items = os.listdir(mae_checkpoint_dir)\n",
        "        if items:\n",
        "            for item in items:\n",
        "                item_path = os.path.join(mae_checkpoint_dir, item)\n",
        "                if os.path.isfile(item_path):\n",
        "                    file_size = os.path.getsize(item_path) / (1024**2) # Size in MB\n",
        "                    print(f\"  - {item} ({file_size:.2f} MB)\")\n",
        "                else:\n",
        "                    print(f\"  - {item} (Directory)\")\n",
        "        else:\n",
        "            print(\"  (Directory is empty)\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error listing directory contents: {e}\")\n",
        "else:\n",
        "    print(f\"‚ùå Directory not found: {mae_checkpoint_dir}\")\n",
        "    print(\"Please ensure the directory exists in your Google Drive.\")\n",
        "\n",
        "print(\"\\n--- Check complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "jF_Ontj1eb3Y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF_Ontj1eb3Y",
        "outputId": "9d948a76-ddc8-4e42-ea13-c70111d00980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount('/content/drive')\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "njLKb7xaepxo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njLKb7xaepxo",
        "outputId": "226646b9-5a2e-42c3-d210-62f11f6e92bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ STARTING SEMI-SUPERVISED FISH CLASSIFICATION TRAINING\n",
            "============================================================\n",
            "/content/ViT-FishID\n",
            "ü§ñ Preparing MAE-enhanced training script...\n",
            "üìã TRAINING CONFIGURATION:\n",
            "============================================================\n",
            "üéØ Training 37 fish species\n",
            "üìä Mode: semi_supervised\n",
            "ü§ñ MAE pretrained: True\n",
            "üåê ImageNet pretrained: False\n",
            "üÜï Starting fresh training\n",
            "‚è±Ô∏è Estimated time: 5.0 hours\n",
            "üíæ Checkpoints: /content/drive/MyDrive/ViT-FishID/pretrained_checkpoints\n",
            "üìà W&B logging: False\n",
            "üéâ Using MAE-learned features from: mae_final_model.pth\n",
            "üöÄ This should significantly improve training performance!\n",
            "\n",
            "üé¨ TRAINING STARTED\n",
            "‚è∞ Started at: 2025-08-18 10:42:59\n",
            "2025-08-18 10:43:07.230152: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755513787.262852   24345 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755513787.273669   24345 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755513787.297594   24345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755513787.297655   24345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755513787.297663   24345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755513787.297670   24345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "‚úÖ Loaded configuration from arguments.\n",
            "üìä Configured num_classes: 37\n",
            "üÜï Training from scratch based on arguments.\n",
            "Using GPU: Tesla T4\n",
            "Random seed set to 42\n",
            "ü§ñ Creating model for training...\n",
            "Attempting to load MAE pretrained model from: /content/drive/MyDrive/mae_checkpoints/mae_final_model.pth\n",
            "üì• Loading MAE checkpoint from: /content/drive/MyDrive/mae_checkpoints/mae_final_model.pth\n",
            "‚úÖ MAE checkpoint loaded.\n",
            "Filtering MAE state dictionary for encoder weights...\n",
            "üìä Extracted 78 encoder parameters from MAE.\n",
            "üéâ MAE encoder weights loaded successfully within script!\n",
            "‚úÖ Loaded 0 MAE encoder weights into model backbone state dict.\n",
            "‚úÖ Student model created and initialized with MAE weights.\n",
            "‚úÖ EMA Teacher initialized with momentum: 0.999\n",
            "üéì EMA Teacher model created and its internal model moved to device.\n",
            "Loading data...\n",
            "‚ö†Ô∏è  Warning: Some classes have only 1 sample(s). Using random splitting instead of stratified.\n",
            "   Classes with 1 sample: ['Carangidae_Caranx_heberi', 'Serranidae_Lipropoma_spp1', 'Sparidae_Sparodon_durbanesis']\n",
            "/content/ViT-FishID/data.py:229: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
            "üìä Dataset initialized:\n",
            "  - Labeled samples: 3,084\n",
            "  - Unlabeled samples: 6,168\n",
            "  - Total samples per epoch: 9,252\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "üìä Semi-supervised data loaders created:\n",
            "  - Train labeled: 3,084\n",
            "  - Train unlabeled: 6,168\n",
            "  - Val samples: 1,029\n",
            "  - Test samples: 1,029\n",
            "  - Classes: 37\n",
            "  - Split ratios: Train=60.0%, Val=20.0%, Test=20.0%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/run_mae_training.py\", line 247, in <module>\n",
            "    train_loader, val_loader, unlabeled_loader, test_loader, num_classes_data = create_semi_supervised_dataloaders(\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ValueError: too many values to unpack (expected 5)\n",
            "\n",
            "============================================================\n",
            "üéâ TRAINING COMPLETED!\n",
            "‚è∞ Finished at: 2025-08-18 10:43:16\n",
            "‚úÖ Your MAE-enhanced model is ready for evaluation and deployment!\n"
          ]
        }
      ],
      "source": [
        "# Start Semi-Supervised Training with Optional MAE Initialization\n",
        "import os\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import json # Import json for passing config\n",
        "\n",
        "print(\"üöÄ STARTING SEMI-SUPERVISED FISH CLASSIFICATION TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Change to repository directory\n",
        "%cd /content/ViT-FishID\n",
        "\n",
        "# Check for existing checkpoints to resume from\n",
        "RESUME_FROM = None\n",
        "if os.path.exists(TRAINING_CONFIG['checkpoint_dir']):\n",
        "    checkpoints = glob.glob(os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'checkpoint_epoch_*.pth'))\n",
        "    if checkpoints:\n",
        "        # Find the latest checkpoint\n",
        "        epoch_numbers = []\n",
        "        for cp in checkpoints:\n",
        "            try:\n",
        "                epoch_num = int(cp.split('epoch_')[1].split('.')[0])\n",
        "                epoch_numbers.append((epoch_num, cp))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if epoch_numbers:\n",
        "            epoch_numbers.sort(key=lambda x: x[0], reverse=True)  # Latest first\n",
        "            latest_epoch, latest_checkpoint = epoch_numbers[0]\n",
        "            print(f\"üîç Found existing checkpoints. Latest: Epoch {latest_epoch}\")\n",
        "\n",
        "            # Ask user if they want to resume (auto-skip in Colab for now)\n",
        "            # resume_choice = input(\"Do you want to resume from the latest checkpoint? (y/n): \").lower().strip()\n",
        "            resume_choice = 'n'  # Set to 'y' if you want to auto-resume\n",
        "\n",
        "            if resume_choice in ['y', 'yes']:\n",
        "                RESUME_FROM = latest_checkpoint\n",
        "                print(f\"‚úÖ Will resume from: {os.path.basename(latest_checkpoint)}\")\n",
        "            else:\n",
        "                print(\"üÜï Starting fresh training from epoch 1\")\n",
        "\n",
        "# Determine which training script to use\n",
        "use_mae_script = TRAINING_CONFIG.get('mae_pretrained', False) and 'MAE_ENCODER_WEIGHTS' in globals() and MAE_ENCODER_WEIGHTS is not None\n",
        "\n",
        "# Serialize TRAINING_CONFIG, NUM_CLASSES, and RESUME_FROM to pass to the script\n",
        "training_config_json = json.dumps(TRAINING_CONFIG)\n",
        "num_classes_str = str(NUM_CLASSES)\n",
        "resume_from_str = RESUME_FROM if RESUME_FROM is not None else 'None'\n",
        "\n",
        "\n",
        "if use_mae_script:\n",
        "    print(\"ü§ñ Preparing MAE-enhanced training script...\")\n",
        "\n",
        "    # Generate the full training script with MAE initialization logic\n",
        "    training_script_content = f\"\"\"#!/usr/bin/env python3\n",
        "import sys\n",
        "sys.path.append('/content/ViT-FishID')\n",
        "\n",
        "import torch\n",
        "import argparse\n",
        "import os\n",
        "import glob\n",
        "import wandb\n",
        "import json # Import json to load config\n",
        "from datetime import datetime\n",
        "\n",
        "from model import ViTForFishClassification\n",
        "from trainer import EMATrainer, SemiSupervisedTrainer # Ensure both trainers are imported here\n",
        "from data import create_dataloaders, create_semi_supervised_dataloaders\n",
        "from utils import get_device, set_seed\n",
        "\n",
        "# --- Argument Parsing for Config ---\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--config_json', type=str, required=True, help='JSON string of training configuration')\n",
        "parser.add_argument('--num_classes_arg', type=int, required=True, help='Number of classes')\n",
        "parser.add_argument('--resume_from_arg', type=str, default='None', help='Path to resume checkpoint')\n",
        "\n",
        "# Parse arguments passed from the launching script\n",
        "# We need to parse args carefully here as the Colab cell uses ! which doesn't pass args traditionally\n",
        "# One way is to pass a single JSON string arg\n",
        "# This requires the launching cell to handle serialization and passing\n",
        "# Let's assume the launching cell passes a single '--config_json' argument\n",
        "# If not, we'd need a different mechanism (e.g., writing config to a temp file)\n",
        "\n",
        "# Let's adjust to read the single config_json argument\n",
        "script_args = parser.parse_args()\n",
        "TRAINING_CONFIG = json.loads(script_args.config_json)\n",
        "NUM_CLASSES = script_args.num_classes_arg\n",
        "RESUME_FROM = script_args.resume_from_arg if script_args.resume_from_arg != 'None' else None\n",
        "\n",
        "print(\"‚úÖ Loaded configuration from arguments.\")\n",
        "print(f\"üìä Configured num_classes: {{NUM_CLASSES}}\")\n",
        "if RESUME_FROM:\n",
        "    print(f\"üîÑ Configured resume_from: {{RESUME_FROM}}\")\n",
        "else:\n",
        "    print(\"üÜï Training from scratch based on arguments.\")\n",
        "\n",
        "\n",
        "# --- MAE Loading Logic (moved into the script) ---\n",
        "def load_mae_encoder_weights(mae_checkpoint_path):\n",
        "    print(f\"üì• Loading MAE checkpoint from: {{mae_checkpoint_path}}\")\n",
        "    try:\n",
        "        # Use weights_only=False based on previous error resolution\n",
        "        checkpoint = torch.load(mae_checkpoint_path, map_location='cpu', weights_only=False)\n",
        "        print(\"‚úÖ MAE checkpoint loaded.\")\n",
        "\n",
        "        # Handle different potential state_dict keys\n",
        "        mae_state_dict = checkpoint.get('model_state_dict', checkpoint.get('state_dict', checkpoint.get('model', None))) # Added 'model'\n",
        "        if mae_state_dict is None:\n",
        "             print(\"‚ùå MAE state dictionary not found in checkpoint (checked 'model_state_dict', 'state_dict', 'model').\")\n",
        "             return None\n",
        "\n",
        "        encoder_weights = {{}}\n",
        "        filter_prefixes = ['patch_embed', 'pos_embed', 'cls_token', 'blocks', 'norm']\n",
        "        exclude_substrings = ['decoder', 'mask_token', 'head']\n",
        "\n",
        "        print(\"Filtering MAE state dictionary for encoder weights...\")\n",
        "        loaded_keys_count = 0\n",
        "        for mae_key, mae_weight in mae_state_dict.items():\n",
        "            should_include_prefix = False\n",
        "            for prefix in filter_prefixes:\n",
        "                if prefix in mae_key:\n",
        "                    should_include_prefix = True\n",
        "                    break\n",
        "\n",
        "            should_exclude_substring = False\n",
        "            for exclude_str in exclude_substrings:\n",
        "                 if exclude_str in mae_key:\n",
        "                      should_exclude_substring = True\n",
        "                      break\n",
        "\n",
        "            if should_include_prefix and not should_exclude_substring:\n",
        "                encoder_weights[mae_key] = mae_weight\n",
        "                loaded_keys_count += 1\n",
        "\n",
        "\n",
        "        print(f\"üìä Extracted {{len(encoder_weights)}} encoder parameters from MAE.\")\n",
        "        return encoder_weights\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading or processing MAE checkpoint: {{e}}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# --- End MAE Loading Logic ---\n",
        "\n",
        "\n",
        "# Function to create MAE-initialized model\n",
        "def create_mae_initialized_model(num_classes, model_name, mae_weights):\n",
        "    model = ViTForFishClassification(\n",
        "        num_classes=num_classes,\n",
        "        model_name=model_name,\n",
        "        pretrained=False,  # Don't use ImageNet if using MAE\n",
        "        dropout_rate=0.1\n",
        "    )\n",
        "\n",
        "    # Initialize updated_keys here to ensure it's always defined\n",
        "    updated_keys = []\n",
        "\n",
        "    if mae_weights is not None:\n",
        "        backbone_state = model.backbone.state_dict()\n",
        "        loaded_count = 0\n",
        "        for mae_key, mae_weight in mae_weights.items():\n",
        "             # Check if the key exists in the model's state dict and if shapes match\n",
        "             if mae_key in backbone_state and mae_weight.shape == backbone_state[mae_key].shape:\n",
        "                  backbone_state[mae_key] = mae_weight.clone()\n",
        "                  updated_keys.append(mae_key)\n",
        "                  loaded_count +=1\n",
        "             # elif mae_key not in backbone_state:\n",
        "             #     print(f\"Skipping MAE weight '{{mae_key}}': not found in model backbone\")\n",
        "             # else:\n",
        "             #     print(f\"Skipping MAE weight '{{mae_key}}': shape mismatch (MAE:{{mae_weight.shape}} != Model:{{backbone_state[mae_key].shape}})\")\n",
        "\n",
        "\n",
        "        try:\n",
        "            model.backbone.load_state_dict(backbone_state, strict=False) # strict=False allows skipping mismatched keys\n",
        "            print(f\"‚úÖ Loaded {{len(updated_keys)}} MAE encoder weights into model backbone state dict.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading MAE weights into ViT backbone: {{e}}\")\n",
        "            print(\"Continuing with potentially partially loaded weights.\")\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Define arguments directly (or parse if needed) - Now using values from parsed args\n",
        "class Args:\n",
        "    def __init__(self, config, resume_from, num_classes):\n",
        "        self.mode = config['mode']\n",
        "        self.data_dir = config['data_dir']\n",
        "        self.epochs = config['epochs']\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.learning_rate = config['learning_rate']\n",
        "        self.weight_decay = config['weight_decay']\n",
        "        self.model_name = config['model_name']\n",
        "        self.consistency_weight = config.get('consistency_weight', 0.1) # Use .get for optional keys\n",
        "        self.pseudo_label_threshold = config.get('pseudo_label_threshold', 0.7) # Use .get for optional keys\n",
        "        self.temperature = config.get('temperature', 0.7) # Use .get for optional keys\n",
        "        self.warmup_epochs = config.get('warmup_epochs', 0) # Use .get for optional keys\n",
        "        self.ramp_up_epochs = config.get('ramp_up_epochs', 0) # Use .get for optional keys\n",
        "        self.save_dir = config['checkpoint_dir']\n",
        "        self.save_frequency = config['save_frequency']\n",
        "        self.pretrained = config.get('pretrained', True) # Use .get for optional keys, default True\n",
        "        self.use_wandb = config.get('use_wandb', False) # Use .get for optional keys, default False\n",
        "        self.resume_from = resume_from\n",
        "        self.num_workers = config.get('num_workers', 4) # Use .get for optional keys, default 4\n",
        "        self.image_size = config.get('image_size', 224) # Use .get for optional keys, default 224\n",
        "        self.dropout_rate = config.get('dropout_rate', 0.1) # Use .get for optional keys, default 0.1\n",
        "        self.num_classes = num_classes\n",
        "        # MAE specific args - get from config, not passed directly to Args class init\n",
        "        self.mae_model_path = config.get('mae_model_path', None)\n",
        "        self.mae_pretrained = config.get('mae_pretrained', False)\n",
        "\n",
        "# Create args object using values loaded from parsed args\n",
        "args = Args(TRAINING_CONFIG, RESUME_FROM, NUM_CLASSES)\n",
        "\n",
        "\n",
        "# Set up device and seed\n",
        "device = get_device()\n",
        "set_seed(42)\n",
        "\n",
        "# --- Model Creation with MAE Loading within the script ---\n",
        "print('ü§ñ Creating model for training...')\n",
        "\n",
        "# Load MAE weights within this script's process based on args\n",
        "mae_weights_for_init = None\n",
        "if args.mae_pretrained and args.mae_model_path and os.path.exists(args.mae_model_path):\n",
        "    print(f\"Attempting to load MAE pretrained model from: {{args.mae_model_path}}\")\n",
        "    mae_weights_for_init = load_mae_encoder_weights(args.mae_model_path)\n",
        "    if mae_weights_for_init is not None:\n",
        "        print(\"üéâ MAE encoder weights loaded successfully within script!\")\n",
        "    else:\n",
        "        print(\"‚ùå Failed to load MAE encoder weights within script. Falling back to ImageNet.\")\n",
        "else:\n",
        "     print(\"‚è≠Ô∏è Skipping MAE loading (config disabled, path missing, or file not found).\")\n",
        "     print(f\"üåê Will use ImageNet pretrained weights if args.pretrained is True (config value: {{args.pretrained}}).\")\n",
        "\n",
        "\n",
        "# Create model, using MAE weights if successfully loaded, otherwise use original pretrained flag\n",
        "if mae_weights_for_init is not None:\n",
        "     student_model = create_mae_initialized_model(\n",
        "         num_classes=args.num_classes,\n",
        "         model_name=args.model_name,\n",
        "         mae_weights=mae_weights_for_init # Pass the loaded MAE weights\n",
        "     ).to(device)\n",
        "     # Ensure pretrained flag is False if MAE is used\n",
        "     args.pretrained = False\n",
        "     print(\"‚úÖ Student model created and initialized with MAE weights.\")\n",
        "\n",
        "else: # Fallback to original pretrained flag (likely ImageNet)\n",
        "     student_model = ViTForFishClassification(\n",
        "        num_classes=args.num_classes,\n",
        "        model_name=args.model_name,\n",
        "        pretrained=args.pretrained, # Use the original pretrained flag\n",
        "        dropout_rate=args.dropout_rate\n",
        "     ).to(device)\n",
        "     print(f\"‚úÖ Student model created. Using ImageNet pretrained: {{args.pretrained}}\")\n",
        "\n",
        "\n",
        "# Create teacher model for EMA (if needed)\n",
        "teacher_model = None\n",
        "if args.mode == 'semi_supervised':\n",
        "    try:\n",
        "        from trainer import EMATeacher # Import EMATeacher here if needed\n",
        "        teacher_model = EMATeacher(student_model) # Create EMATeacher instance\n",
        "        # Check if teacher_model has an internal teacher_model attribute before calling .to()\n",
        "        if hasattr(teacher_model, 'teacher_model') and isinstance(teacher_model.teacher_model, torch.nn.Module):\n",
        "             teacher_model.teacher_model.to(device) # Move the internal teacher_model to device\n",
        "             print('üéì EMA Teacher model created and its internal model moved to device.')\n",
        "        else:\n",
        "             # If EMATeacher itself is a Module, move it directly (less likely based on error)\n",
        "             # Or if it manages device internally, no .to() needed\n",
        "             # For now, assume the internal model needs moving\n",
        "             print(\"‚ö†Ô∏è EMATeacher's internal model could not be moved to device.\")\n",
        "\n",
        "    except NameError:\n",
        "         print(\"‚ö†Ô∏è EMATeacher class not found. Semi-supervised training will not work correctly.\")\n",
        "         print(\"‚ùå Please ensure EMATeacher is defined or imported in trainer.py\")\n",
        "         sys.exit(1) # Exit if EMATeacher is needed but not found\n",
        "    except Exception as e:\n",
        "         print(f\"‚ö†Ô∏è Error creating or moving EMATeacher model: {{e}}\")\n",
        "         import traceback\n",
        "         traceback.print_exc()\n",
        "         sys.exit(1) # Exit on other errors\n",
        "\n",
        "\n",
        "# Create data loaders\n",
        "print('Loading data...')\n",
        "if args.mode == 'supervised':\n",
        "    train_loader, val_loader, num_classes_data = create_dataloaders(\n",
        "        args.data_dir,\n",
        "        batch_size=args.batch_size,\n",
        "        image_size=args.image_size,\n",
        "        num_workers=args.num_workers\n",
        "    )\n",
        "    unlabeled_loader = None\n",
        "    test_loader = None # Initialize test_loader for supervised mode\n",
        "else: # semi_supervised mode\n",
        "    # Corrected to unpack 5 values, including the test_loader\n",
        "    train_loader, val_loader, unlabeled_loader, test_loader, num_classes_data = create_semi_supervised_dataloaders(\n",
        "        args.data_dir,\n",
        "        batch_size=args.batch_size,\n",
        "        image_size=args.image_size,\n",
        "        num_workers=args.num_workers\n",
        "    )\n",
        "print('‚úÖ Data loaders created.')\n",
        "\n",
        "if num_classes_data != args.num_classes:\n",
        "     print(f\"‚ö†Ô∏è Warning: Configured num_classes ({{args.num_classes}}) does not match detected data classes ({{num_classes_data}})\")\n",
        "     # Use the detected number of classes if they differ\n",
        "     args.num_classes = num_classes_data\n",
        "     print(f\"‚úÖ Using {{args.num_classes}} detected classes for training.\")\n",
        "     # Need to re-create the model if num_classes changed (unlikely but safe)\n",
        "     # For this script, we'll assume num_classes is consistent\n",
        "\n",
        "print(f'üìä Number of classes: {{args.num_classes}}')\n",
        "print(f'üéØ Training mode: {{args.mode}}')\n",
        "\n",
        "# Create trainer\n",
        "print('Setting up trainer...')\n",
        "# Ensure trainer is created with correct loaders based on mode\n",
        "if args.mode == 'semi_supervised' and unlabeled_loader is not None and teacher_model is not None:\n",
        "    trainer = SemiSupervisedTrainer(\n",
        "        student_model=student_model,\n",
        "        device=device,\n",
        "        learning_rate=args.learning_rate,\n",
        "        weight_decay=args.weight_decay,\n",
        "        consistency_weight=args.consistency_weight,\n",
        "        pseudo_label_threshold=args.pseudo_label_threshold,\n",
        "        temperature=args.temperature,\n",
        "        warmup_epochs=args.warmup_epochs,\n",
        "        ramp_up_epochs=args.ramp_up_epochs,\n",
        "        teacher_model=teacher_model # Pass teacher model\n",
        "    )\n",
        "    print('‚úÖ SemiSupervisedTrainer created.')\n",
        "elif args.mode == 'supervised' and train_loader is not None and val_loader is not None: # Added checks for loaders\n",
        "    trainer = EMATrainer( # Using EMATrainer for supervised mode if needed, or could use a simple Trainer\n",
        "        student_model=student_model,\n",
        "        device=device,\n",
        "        learning_rate=args.learning_rate,\n",
        "        weight_decay=args.weight_decay\n",
        "    )\n",
        "    print('‚úÖ EMATrainer created (for supervised mode).')\n",
        "else:\n",
        "     print(\"‚ùå Cannot create trainer. Check mode and data loaders.\")\n",
        "     sys.exit(1)\n",
        "\n",
        "\n",
        "# Initialize W&B\n",
        "if args.use_wandb:\n",
        "    print('Initializing W&B...')\n",
        "    wandb.init(\n",
        "        project=TRAINING_CONFIG.get('wandb_project', 'ViT-FishID-Training'),\n",
        "        name=TRAINING_CONFIG.get('wandb_run_name', f'fish-classification-{{args.num_classes}}-classes'),\n",
        "        config=vars(args),\n",
        "        tags=['mae-initialized', 'fish-classification'] if args.mae_pretrained else ['imagenet-pretrained', 'fish-classification'] # Use args.mae_pretrained\n",
        "    )\n",
        "    print('‚úÖ W&B initialized.')\n",
        "\n",
        "# Resume from checkpoint if specified\n",
        "if args.resume_from and args.resume_from != 'None':\n",
        "    print(f'üì• Resuming from checkpoint: {{args.resume_from}}')\n",
        "    try:\n",
        "        checkpoint = torch.load(args.resume_from, map_location=device)\n",
        "        trainer.student_model.load_state_dict(checkpoint['student_state_dict'])\n",
        "        # Load teacher state dict if it exists and trainer has a teacher model\n",
        "        if hasattr(trainer, 'teacher_model') and trainer.teacher_model is not None and 'teacher_state_dict' in checkpoint:\n",
        "             try:\n",
        "                trainer.teacher_model.teacher_model.load_state_dict(checkpoint['teacher_state_dict'])\n",
        "                print('‚úÖ Teacher model state dict loaded.')\n",
        "             except Exception as e:\n",
        "                 print(f\"‚ö†Ô∏è Error loading teacher state dict: {{e}}\")\n",
        "\n",
        "        # Load optimizer state dict\n",
        "        if 'optimizer_state_dict' in checkpoint:\n",
        "             try:\n",
        "                 trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                 print('‚úÖ Optimizer state dict loaded.')\n",
        "             except Exception as e:\n",
        "                  print(f\"‚ö†Ô∏è Error loading optimizer state dict: {{e}}\")\n",
        "\n",
        "\n",
        "        start_epoch = checkpoint.get('epoch', 0) + 1\n",
        "        best_accuracy = checkpoint.get('best_accuracy', 0.0) # Resume best accuracy as well\n",
        "        print(f'‚úÖ Resumed from epoch {{start_epoch}} with best accuracy {{best_accuracy:.2f}}%')\n",
        "    except Exception as e:\n",
        "        print(f'‚ùå Error loading checkpoint: {{e}}')\n",
        "        import traceback\n",
        "        traceback.print_exc() # Print full traceback\n",
        "        print(\"Starting fresh training from epoch 1.\")\n",
        "        start_epoch = 1\n",
        "        best_accuracy = 0.0\n",
        "else:\n",
        "    start_epoch = 1\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "print(f'üöÄ Starting training from epoch {{start_epoch}}')\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, args.epochs + 1):\n",
        "    print(f'\\\\nüìÖ Epoch {{epoch}}/{{args.epochs}}')\n",
        "\n",
        "    # Training step\n",
        "    try:\n",
        "        if args.mode == 'semi_supervised' and unlabeled_loader is not None:\n",
        "            train_loss = trainer.train_epoch(train_loader, unlabeled_loader, epoch)\n",
        "        elif args.mode == 'supervised':\n",
        "            train_loss = trainer.train_epoch(train_loader, epoch)\n",
        "        else:\n",
        "             print(\"‚ùå Invalid training mode or data loaders for training epoch.\")\n",
        "             break # Exit training loop if setup is wrong\n",
        "    except Exception as e:\n",
        "         print(f\"‚ùå Error during training epoch {{epoch}}: {{e}}\")\n",
        "         import traceback\n",
        "         traceback.print_exc()\n",
        "         break # Exit training loop on error\n",
        "\n",
        "\n",
        "    # Validation step\n",
        "    try:\n",
        "        val_accuracy = trainer.validate(val_loader)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during validation epoch {{epoch}}: {{e}}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        val_accuracy = 0.0 # Set accuracy to 0 to avoid saving best model on error\n",
        "\n",
        "\n",
        "    # Update best accuracy\n",
        "    is_best = val_accuracy > best_accuracy\n",
        "    if is_best:\n",
        "        best_accuracy = val_accuracy\n",
        "\n",
        "    print(f'üìä Epoch {{epoch}} - Train Loss: {{train_loss:.4f}}, Val Acc: {{val_accuracy:.2f}}% (Best: {{best_accuracy:.2f}}%)')\n",
        "\n",
        "    # Save checkpoint\n",
        "    if epoch % args.save_frequency == 0 or is_best:\n",
        "        print(f'üíæ Saving checkpoint for epoch {{epoch}}...')\n",
        "        checkpoint_data = {{\n",
        "            'epoch': epoch,\n",
        "            'student_state_dict': trainer.student_model.state_dict(),\n",
        "            'optimizer_state_dict': trainer.optimizer.state_dict(), # Corrected: Access optimizer state dict\n",
        "            'best_accuracy': best_accuracy,\n",
        "            'train_loss': train_loss,\n",
        "            'val_accuracy': val_accuracy\n",
        "        }}\n",
        "\n",
        "        if hasattr(trainer, 'teacher_model') and trainer.teacher_model is not None:\n",
        "            try:\n",
        "                checkpoint_data['teacher_state_dict'] = trainer.teacher_model.teacher_model.state_dict() # Corrected: teacher_model.state_dict()\n",
        "                checkpoint_data['teacher_acc'] = getattr(trainer, 'teacher_accuracy', val_accuracy) # Use teacher_accuracy if available\n",
        "            except Exception as e:\n",
        "                 print(f\"‚ö†Ô∏è Could not save teacher state dict: {{e}}\")\n",
        "\n",
        "\n",
        "        # Ensure save directory exists\n",
        "        os.makedirs(args.save_dir, exist_ok=True)\n",
        "\n",
        "        # Save regular checkpoint\n",
        "        if epoch % args.save_frequency == 0:\n",
        "            checkpoint_path = os.path.join(args.save_dir, f'checkpoint_epoch_{{epoch}}.pth')\n",
        "            try:\n",
        "                torch.save(checkpoint_data, checkpoint_path)\n",
        "                print(f'‚úÖ Saved checkpoint: {{checkpoint_path}}')\n",
        "            except Exception as e:\n",
        "                 print(f\"‚ùå Error saving checkpoint {{checkpoint_path}}: {{e}}\")\n",
        "\n",
        "\n",
        "        # Save best model\n",
        "        if is_best:\n",
        "            best_path = os.path.join(args.save_dir, 'model_best.pth')\n",
        "            try:\n",
        "                torch.save(checkpoint_data, best_path)\n",
        "                print(f'üèÜ New best model saved: {{best_path}}')\n",
        "            except Exception as e:\n",
        "                 print(f\"‚ùå Error saving best model {{best_path}}: {{e}}\")\n",
        "\n",
        "\n",
        "    # W&B logging\n",
        "    if args.use_wandb:\n",
        "        try:\n",
        "            wandb.log({{\n",
        "                'epoch': epoch,\n",
        "                'train_loss': train_loss,\n",
        "                'val_accuracy': val_accuracy,\n",
        "                'best_accuracy': best_accuracy\n",
        "            }})\n",
        "        except Exception as e:\n",
        "             print(f\"‚ö†Ô∏è Error logging to W&B at epoch {{epoch}}: {{e}}\")\n",
        "\n",
        "\n",
        "print(f'\\\\nüéâ Training completed!')\n",
        "print(f'üèÜ Best accuracy: {{best_accuracy:.2f}}%')\n",
        "\n",
        "if args.use_wandb:\n",
        "    try:\n",
        "        wandb.finish()\n",
        "    except Exception as e:\n",
        "         print(f\"‚ö†Ô∏è Error finishing W&B run: {{e}}\")\n",
        "\n",
        "\"\"\"\n",
        "    # Write the script to a temporary file\n",
        "    script_filename = '/content/run_mae_training.py'\n",
        "    with open(script_filename, 'w') as f:\n",
        "        f.write(training_script_content)\n",
        "\n",
        "    # Execute the temporary script, passing config as a JSON string argument\n",
        "    # Use shlex.quote to handle potential special characters in the JSON string\n",
        "    import shlex\n",
        "    quoted_config_json = shlex.quote(training_config_json)\n",
        "\n",
        "    training_cmd = f\"python {script_filename} --config_json {quoted_config_json} --num_classes_arg {num_classes_str} --resume_from_arg {resume_from_str}\"\n",
        "\n",
        "\n",
        "else:\n",
        "    # Build standard training command without MAE\n",
        "    training_cmd = f\"\"\"python train.py \\\\\n",
        "    --mode {TRAINING_CONFIG['mode']} \\\\\n",
        "    --data_dir {TRAINING_CONFIG['data_dir']} \\\\\n",
        "    --epochs {TRAINING_CONFIG['epochs']} \\\\\n",
        "    --batch_size {TRAINING_CONFIG['batch_size']} \\\\\n",
        "    --learning_rate {TRAINING_CONFIG['learning_rate']} \\\\\n",
        "    --weight_decay {TRAINING_CONFIG['weight_decay']} \\\\\n",
        "    --model_name {TRAINING_CONFIG['model_name']} \\\\\n",
        "    --consistency_weight {TRAINING_CONFIG.get('consistency_weight', 0.1)} \\\\\n",
        "    --pseudo_label_threshold {TRAINING_CONFIG.get('pseudo_label_threshold', 0.7)} \\\\\n",
        "    --temperature {TRAINING_CONFIG.get('temperature', 0.7)} \\\\\n",
        "    --warmup_epochs {TRAINING_CONFIG.get('warmup_epochs', 0)} \\\\\n",
        "    --ramp_up_epochs {TRAINING_CONFIG.get('ramp_up_epochs', 0)} \\\\\n",
        "    --save_dir {TRAINING_CONFIG['checkpoint_dir']} \\\\\n",
        "    --save_frequency {TRAINING_CONFIG['save_frequency']}\"\"\"\n",
        "\n",
        "    # Add resume checkpoint if found\n",
        "    if RESUME_FROM:\n",
        "        training_cmd += f\" \\\\\\n    --resume_from {RESUME_FROM}\"\n",
        "\n",
        "    # Add pretrained flag\n",
        "    if TRAINING_CONFIG.get('pretrained', True): # Use .get with default\n",
        "        training_cmd += \" \\\\\\n    --pretrained\"\n",
        "\n",
        "    # Add W&B logging\n",
        "    if TRAINING_CONFIG.get('use_wandb', False): # Use .get with default\n",
        "        training_cmd += \" \\\\\\n    --use_wandb\"\n",
        "\n",
        "print(\"üìã TRAINING CONFIGURATION:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üéØ Training {TRAINING_CONFIG['num_classes']} fish species\")\n",
        "print(f\"üìä Mode: {TRAINING_CONFIG['mode']}\")\n",
        "print(f\"ü§ñ MAE pretrained: {TRAINING_CONFIG.get('mae_pretrained', False)}\")\n",
        "print(f\"üåê ImageNet pretrained: {TRAINING_CONFIG.get('pretrained', True)}\")\n",
        "\n",
        "if RESUME_FROM:\n",
        "    print(f\"üîÑ Resuming from: {os.path.basename(RESUME_FROM)}\")\n",
        "else:\n",
        "    print(f\"üÜï Starting fresh training\")\n",
        "\n",
        "print(f\"‚è±Ô∏è Estimated time: {TRAINING_CONFIG['epochs'] * 3 / 60:.1f} hours\")\n",
        "print(f\"üíæ Checkpoints: {TRAINING_CONFIG['checkpoint_dir']}\")\n",
        "print(f\"üìà W&B logging: {TRAINING_CONFIG['use_wandb']}\")\n",
        "\n",
        "if TRAINING_CONFIG.get('mae_pretrained', False):\n",
        "    print(f\"üéâ Using MAE-learned features from: {os.path.basename(TRAINING_CONFIG.get('mae_model_path', ''))}\")\n",
        "    print(f\"üöÄ This should significantly improve training performance!\")\n",
        "\n",
        "print(f\"\\nüé¨ TRAINING STARTED\")\n",
        "print(\"‚è∞ Started at:\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "# Execute training\n",
        "# Use PYTHONPATH to help the executed script find local modules\n",
        "!PYTHONPATH=/content/ViT-FishID {training_cmd}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ TRAINING COMPLETED!\")\n",
        "print(\"‚è∞ Finished at:\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "# Check for results\n",
        "best_model_path = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'model_best.pth')\n",
        "if os.path.exists(best_model_path):\n",
        "    try:\n",
        "        import torch\n",
        "        checkpoint = torch.load(best_model_path, map_location='cpu')\n",
        "        if 'best_accuracy' in checkpoint:\n",
        "            print(f\"üèÜ Best accuracy achieved: {checkpoint['best_accuracy']:.2f}%\")\n",
        "        if 'epoch' in checkpoint:\n",
        "            print(f\"üìä Best model from epoch: {checkpoint['epoch']}\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"‚úÖ Your MAE-enhanced model is ready for evaluation and deployment!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
