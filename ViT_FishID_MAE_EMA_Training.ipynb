{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f1581f",
   "metadata": {},
   "source": [
    "# üöÄ ViT-FishID: MAE Pretraining + EMA Student-Teacher Training\n",
    "\n",
    "**ADVANCED FISH CLASSIFICATION WITH MASKED AUTOENCODERS & SEMI-SUPERVISED LEARNING**\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/cat-thomson/ViT-FishID/blob/main/ViT_FishID_MAE_EMA_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## üéØ Training Pipeline Overview\n",
    "\n",
    "This notebook implements a comprehensive two-stage training approach:\n",
    "\n",
    "**Stage 1: Masked Autoencoder (MAE) Pretraining** üé≠\n",
    "- Self-supervised pretraining on unlabeled fish images\n",
    "- Learns robust visual representations by reconstructing masked patches\n",
    "- Uses 75% masking ratio for strong representation learning\n",
    "- Expected training time: 2-3 hours\n",
    "\n",
    "**Stage 2: EMA Student-Teacher Semi-Supervised Learning** üéì\n",
    "- Fine-tunes MAE-pretrained backbone for fish classification\n",
    "- Combines labeled supervision with unlabeled consistency learning\n",
    "- Uses exponential moving average teacher for pseudo-labeling\n",
    "- Expected training time: 4-6 hours\n",
    "\n",
    "## üìä Expected Performance Improvements\n",
    "\n",
    "- **Without MAE**: ~75-80% accuracy after 100 epochs\n",
    "- **With MAE + EMA**: ~85-92% accuracy after 100 epochs\n",
    "- **Data efficiency**: Better performance with limited labeled data\n",
    "- **Generalization**: Improved robustness to unseen fish species\n",
    "\n",
    "## üõ†Ô∏è Requirements\n",
    "\n",
    "- **GPU**: Colab Pro recommended (T4/V100/A100)\n",
    "- **Memory**: ~12-16GB GPU memory\n",
    "- **Runtime**: 6-9 hours total training time\n",
    "- **Data**: Fish cutouts dataset with labeled/unlabeled images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd81dfbf",
   "metadata": {},
   "source": [
    "## üîß Section 1: Environment Setup and GPU Check\n",
    "\n",
    "Setting up the optimal environment for MAE pretraining and EMA training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic environment setup and GPU check (lightweight)\n",
    "import sys\n",
    "import os\n",
    "import platform\n",
    "\n",
    "print(\"üîç BASIC SYSTEM INFORMATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# System info (no heavy imports)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "\n",
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running in Google Colab\")\n",
    "    \n",
    "    # Check Colab GPU status\n",
    "    try:\n",
    "        gpu_info = !nvidia-smi\n",
    "        print(\"‚úÖ nvidia-smi available - GPU runtime detected\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  nvidia-smi not available - may be CPU runtime\")\n",
    "        print(\"   Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ÑπÔ∏è  Not running in Google Colab\")\n",
    "\n",
    "print(\"\\nüéØ Training Pipeline Overview:\")\n",
    "print(\"  - Stage 1: MAE Pretraining (Self-supervised)\")\n",
    "print(\"  - Stage 2: EMA Student-Teacher (Semi-supervised)\")\n",
    "print(\"  - Expected total time: 6-9 hours\")\n",
    "print(\"  - Memory requirements: 8-12GB GPU (optimized)\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT SETUP NOTES:\")\n",
    "print(\"1. Ensure GPU runtime is enabled\")\n",
    "print(\"2. Upload fish_cutouts.zip to Google Drive root\")\n",
    "print(\"3. Allow kernel restart in Section 2 (this fixes CUDNN issues)\")\n",
    "print(\"4. Section 1 should run in <30 seconds\")\n",
    "\n",
    "print(\"\\n‚úÖ Basic environment check complete!\")\n",
    "print(\"üöÄ Proceed to Section 2 to mount Drive and install dependencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a977938",
   "metadata": {},
   "source": [
    "## üö® TROUBLESHOOTING: If Section 1 is Running Too Long\n",
    "\n",
    "**If the previous cell has been executing for >2 minutes:**\n",
    "\n",
    "1. **INTERRUPT THE CELL**: Click the ‚èπÔ∏è **Stop** button in Colab\n",
    "2. **RESTART RUNTIME**: Runtime ‚Üí Restart Runtime  \n",
    "3. **RE-RUN**: Execute the cell again\n",
    "\n",
    "**Common causes of long execution:**\n",
    "- Missing GPU runtime (switches to slow CPU mode)\n",
    "- Automatic package installation in background\n",
    "- Import conflicts from previous runs\n",
    "\n",
    "**Expected behavior:**\n",
    "- Section 1 should complete in **<30 seconds**\n",
    "- Should show \"‚úÖ Basic environment check complete!\"\n",
    "- No heavy library imports (PyTorch comes later)\n",
    "\n",
    "**Next steps after Section 1:**\n",
    "1. Section 2: Mount Google Drive (quick)\n",
    "2. Section 2: Install Dependencies (will restart kernel - normal!)\n",
    "3. Section 2: Verify Installation (checks PyTorch/CUDA)\n",
    "4. Then proceed with data setup and MAE training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36557ca5",
   "metadata": {},
   "source": [
    "## üìÅ Section 2: Mount Google Drive and Install Dependencies\n",
    "\n",
    "Setting up data access and installing packages for MAE and EMA training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac96382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for data access\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "print(\"üìÅ MOUNTING GOOGLE DRIVE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify mount and show available space\n",
    "drive_path = '/content/drive/MyDrive'\n",
    "if os.path.exists(drive_path):\n",
    "    # Get drive info\n",
    "    statvfs = os.statvfs(drive_path)\n",
    "    free_space = statvfs.f_frsize * statvfs.f_bavail / (1024**3)  # GB\n",
    "    \n",
    "    print(f\"‚úÖ Google Drive mounted successfully!\")\n",
    "    print(f\"üíæ Available space: {free_space:.1f} GB\")\n",
    "    \n",
    "    # List some contents to verify\n",
    "    items = os.listdir(drive_path)[:10]\n",
    "    print(f\"\\nüìÇ Drive contents (first 10 items):\")\n",
    "    for item in items:\n",
    "        print(f\"  - {item}\")\n",
    "    \n",
    "    if len(os.listdir(drive_path)) > 10:\n",
    "        print(f\"  ... and {len(os.listdir(drive_path)) - 10} more items\")\n",
    "    \n",
    "    # Check for required dataset\n",
    "    dataset_path = '/content/drive/MyDrive/fish_cutouts.zip'\n",
    "    if os.path.exists(dataset_path):\n",
    "        dataset_size = os.path.getsize(dataset_path) / (1024**2)  # MB\n",
    "        print(f\"\\nüêü Found fish dataset: {dataset_size:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Fish dataset not found at: {dataset_path}\")\n",
    "        print(\"   Please ensure fish_cutouts.zip is uploaded to Google Drive root\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Failed to mount Google Drive\")\n",
    "    print(\"   Please check your Google account permissions\")\n",
    "\n",
    "print(\"\\nüí° Ready for data setup and model training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install comprehensive dependencies for MAE + EMA training\n",
    "print(\"üì¶ INSTALLING ADVANCED DEPENDENCIES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# CRITICAL: Fix CUDNN compatibility issues first\n",
    "print(\"üîß Fixing CUDNN compatibility...\")\n",
    "\n",
    "# Uninstall existing PyTorch to avoid conflicts\n",
    "!pip uninstall -y torch torchvision torchaudio\n",
    "\n",
    "# Install compatible PyTorch version for Colab\n",
    "print(\"üîß Installing compatible PyTorch...\")\n",
    "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Core ML libraries\n",
    "print(\"üîß Installing core ML libraries...\")\n",
    "!pip install -q timm==0.9.7  # Specific version to avoid conflicts\n",
    "!pip install -q transformers==4.33.0\n",
    "\n",
    "# Vision and augmentation\n",
    "print(\"üñºÔ∏è  Installing vision libraries...\")\n",
    "!pip install -q albumentations==1.3.1\n",
    "!pip install -q opencv-python-headless==4.8.0.76\n",
    "!pip install -q pillow==9.5.0\n",
    "\n",
    "# Training utilities\n",
    "print(\"‚öôÔ∏è Installing training utilities...\")\n",
    "!pip install -q wandb==0.15.8\n",
    "!pip install -q scikit-learn==1.3.0\n",
    "!pip install -q matplotlib==3.7.2\n",
    "!pip install -q seaborn==0.12.2\n",
    "!pip install -q tqdm==4.66.1\n",
    "\n",
    "# Additional utilities for MAE (minimal versions)\n",
    "print(\"üé≠ Installing MAE-specific utilities...\")\n",
    "!pip install -q accelerate==0.21.0\n",
    "!pip install -q datasets==2.14.4\n",
    "\n",
    "print(\"‚úÖ All dependencies installed with version pinning!\")\n",
    "\n",
    "# Restart Python kernel to ensure clean imports\n",
    "print(\"\\n\udd04 RESTARTING PYTHON KERNEL\")\n",
    "print(\"=\"*50)\n",
    "print(\"‚ö†Ô∏è  After running this cell, you may see a kernel restart.\")\n",
    "print(\"   This is NORMAL and fixes CUDNN compatibility issues.\")\n",
    "print(\"   Simply continue with the next cell.\")\n",
    "\n",
    "import os\n",
    "os.kill(os.getpid(), 9)  # Force restart to clear CUDNN conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installations and setup PyTorch environment\n",
    "print(\"üìã VERIFYING PYTORCH INSTALLATION & GPU SETUP\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Import PyTorch and related libraries (now that they're installed)\n",
    "import torch\n",
    "import torchvision\n",
    "import timm\n",
    "import transformers\n",
    "import albumentations\n",
    "import cv2\n",
    "import sklearn\n",
    "import wandb\n",
    "import gc\n",
    "\n",
    "print(\"‚úÖ Package Versions:\")\n",
    "print(f\"  torch: {torch.__version__}\")\n",
    "print(f\"  torchvision: {torchvision.__version__}\")\n",
    "print(f\"  timm: {timm.__version__}\")\n",
    "print(f\"  transformers: {transformers.__version__}\")\n",
    "print(f\"  albumentations: {albumentations.__version__}\")\n",
    "print(f\"  opencv: {cv2.__version__}\")\n",
    "print(f\"  sklearn: {sklearn.__version__}\")\n",
    "\n",
    "# Comprehensive CUDA and GPU verification\n",
    "print(f\"\\nüîç CUDA & GPU VERIFICATION\")\n",
    "print(\"=\"*30)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # Test CUDNN compatibility (critical fix)\n",
    "    try:\n",
    "        device = torch.device('cuda')\n",
    "        test_tensor = torch.randn(2, 3, 224, 224).to(device)\n",
    "        \n",
    "        # Configure CUDNN for stability\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        torch.backends.cudnn.benchmark = False  # Stable for variable inputs\n",
    "        torch.backends.cudnn.deterministic = True  # Reproducible results\n",
    "        \n",
    "        # Test basic GPU operations\n",
    "        result = test_tensor * 2.0\n",
    "        result = result.cpu()\n",
    "        \n",
    "        print(\"‚úÖ CUDNN compatibility test PASSED!\")\n",
    "        print(\"‚úÖ GPU operations working correctly\")\n",
    "        \n",
    "        # Clean up test tensors\n",
    "        del test_tensor, result\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå CUDNN compatibility issue: {e}\")\n",
    "        print(\"üîß This indicates PyTorch/CUDA version mismatch\")\n",
    "        print(\"   Try restarting runtime and running from the beginning\")\n",
    "    \n",
    "    # Set global device and optimize\n",
    "    DEVICE = torch.device('cuda')\n",
    "    \n",
    "    # Memory optimization for long training\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\nüéØ Using device: {DEVICE}\")\n",
    "    print(\"üí° Memory optimization enabled for long training sessions\")\n",
    "    \n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print(\"\\n‚ö†Ô∏è  No GPU detected!\")\n",
    "    print(\"üîß Enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")\n",
    "    print(\"‚ö†Ô∏è  MAE pretraining will be extremely slow on CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(f\"\\nüöÄ PyTorch environment ready for MAE + EMA training!\")\n",
    "print(\"üí° Key success indicators:\")\n",
    "print(\"  ‚úÖ CUDNN compatibility test passed\")\n",
    "print(\"  ‚úÖ GPU operations working\")\n",
    "print(\"  ‚úÖ No import errors\")\n",
    "\n",
    "# Store device for use in other cells\n",
    "globals()['DEVICE'] = DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d378f",
   "metadata": {},
   "source": [
    "## üîÑ Section 3: Clone Repository and Setup Data\n",
    "\n",
    "Cloning ViT-FishID repository and preparing fish dataset for MAE pretraining and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone ViT-FishID repository and prepare codebase\n",
    "import os\n",
    "\n",
    "print(\"üì• CLONING ViT-FishID REPOSITORY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Remove existing directory if it exists\n",
    "if os.path.exists('/content/ViT-FishID'):\n",
    "    !rm -rf /content/ViT-FishID\n",
    "    print(\"üóëÔ∏è  Removed existing repository\")\n",
    "\n",
    "# Clone the repository\n",
    "print(\"üì• Cloning ViT-FishID repository...\")\n",
    "!git clone https://github.com/cat-thomson/ViT-FishID.git /content/ViT-FishID\n",
    "\n",
    "# Change to project directory\n",
    "%cd /content/ViT-FishID\n",
    "\n",
    "# Verify repository structure\n",
    "print(\"\\nüìÇ Repository structure:\")\n",
    "!ls -la\n",
    "\n",
    "# Check for key files\n",
    "required_files = ['model.py', 'trainer.py', 'data.py', 'train.py']\n",
    "missing_files = []\n",
    "\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"‚úÖ Found: {file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Missing: {file}\")\n",
    "        missing_files.append(file)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing files: {missing_files}\")\n",
    "    print(\"   These will be created as part of the MAE implementation\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All required files found!\")\n",
    "\n",
    "# Set up Python path for imports\n",
    "import sys\n",
    "if '/content/ViT-FishID' not in sys.path:\n",
    "    sys.path.append('/content/ViT-FishID')\n",
    "    print(\"üîß Added repository to Python path\")\n",
    "\n",
    "print(\"\\nüöÄ Repository ready for MAE and EMA implementation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e183528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and setup fish dataset for MAE + EMA training\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üêü EXTRACTING FISH DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Configuration\n",
    "ZIP_FILE_PATH = '/content/drive/MyDrive/fish_cutouts.zip'\n",
    "DATA_DIR = '/content/fish_cutouts'\n",
    "\n",
    "print(f\"üéØ ZIP location: {ZIP_FILE_PATH}\")\n",
    "print(f\"üéØ Target directory: {DATA_DIR}\")\n",
    "\n",
    "# Check if data already exists\n",
    "if os.path.exists(DATA_DIR) and os.path.exists(os.path.join(DATA_DIR, 'labeled')):\n",
    "    print(\"‚úÖ Data already available locally!\")\n",
    "    \n",
    "    # Quick validation\n",
    "    labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
    "    unlabeled_dir = os.path.join(DATA_DIR, 'unlabeled')\n",
    "    \n",
    "    if os.path.exists(labeled_dir):\n",
    "        labeled_species = [d for d in os.listdir(labeled_dir) \n",
    "                         if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')]\n",
    "        labeled_count = sum([len([f for f in os.listdir(os.path.join(labeled_dir, species))\n",
    "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "                           for species in labeled_species])\n",
    "        print(f\"üêü Labeled: {len(labeled_species)} species, {labeled_count} images\")\n",
    "    \n",
    "    if os.path.exists(unlabeled_dir):\n",
    "        unlabeled_count = len([f for f in os.listdir(unlabeled_dir)\n",
    "                             if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"üìä Unlabeled: {unlabeled_count} images (for MAE pretraining)\")\n",
    "    \n",
    "    print(\"‚úÖ Data validation passed!\")\n",
    "\n",
    "else:\n",
    "    print(\"üì• Extracting dataset from Google Drive...\")\n",
    "    \n",
    "    if not os.path.exists(ZIP_FILE_PATH):\n",
    "        print(f\"‚ùå ZIP file not found: {ZIP_FILE_PATH}\")\n",
    "        print(\"üîß Please upload fish_cutouts.zip to Google Drive root\")\n",
    "        raise FileNotFoundError(\"Dataset ZIP file not found\")\n",
    "    \n",
    "    print(f\"üìè ZIP size: {os.path.getsize(ZIP_FILE_PATH) / (1024**2):.1f} MB\")\n",
    "    \n",
    "    # Extract with progress\n",
    "    temp_dir = '/content/temp_extract'\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "    \n",
    "    try:\n",
    "        print(\"üì¶ Extracting ZIP file...\")\n",
    "        with zipfile.ZipFile(ZIP_FILE_PATH, 'r') as zip_ref:\n",
    "            zip_ref.extractall(temp_dir)\n",
    "        \n",
    "        # Organize extracted files\n",
    "        extracted_items = os.listdir(temp_dir)\n",
    "        print(f\"üìÅ Extracted items: {extracted_items}\")\n",
    "        \n",
    "        # Find and move labeled/unlabeled directories\n",
    "        labeled_source = None\n",
    "        unlabeled_source = None\n",
    "        \n",
    "        for item in extracted_items:\n",
    "            item_path = os.path.join(temp_dir, item)\n",
    "            if item == 'labeled' and os.path.isdir(item_path):\n",
    "                labeled_source = item_path\n",
    "            elif item == 'unlabeled' and os.path.isdir(item_path):\n",
    "                unlabeled_source = item_path\n",
    "        \n",
    "        if labeled_source and unlabeled_source:\n",
    "            # Create target and move directories\n",
    "            os.makedirs(DATA_DIR, exist_ok=True)\n",
    "            shutil.move(labeled_source, os.path.join(DATA_DIR, 'labeled'))\n",
    "            shutil.move(unlabeled_source, os.path.join(DATA_DIR, 'unlabeled'))\n",
    "            \n",
    "            print(f\"‚úÖ Data organized at: {DATA_DIR}\")\n",
    "            \n",
    "            # Verification\n",
    "            labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
    "            unlabeled_dir = os.path.join(DATA_DIR, 'unlabeled')\n",
    "            \n",
    "            species_count = len([d for d in os.listdir(labeled_dir)\n",
    "                               if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')])\n",
    "            unlabeled_count = len([f for f in os.listdir(unlabeled_dir)\n",
    "                                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            \n",
    "            print(f\"üêü Labeled: {species_count} species\")\n",
    "            print(f\"üìä Unlabeled: {unlabeled_count} images\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ùå Could not find labeled/unlabeled directories\")\n",
    "            raise FileNotFoundError(\"Invalid dataset structure\")\n",
    "        \n",
    "        # Cleanup\n",
    "        shutil.rmtree(temp_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Extraction failed: {e}\")\n",
    "        if os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        raise\n",
    "\n",
    "# Final verification and stats\n",
    "print(f\"\\nüìä DATASET SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
    "unlabeled_dir = os.path.join(DATA_DIR, 'unlabeled')\n",
    "\n",
    "if os.path.exists(labeled_dir):\n",
    "    species = [d for d in os.listdir(labeled_dir) \n",
    "              if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')]\n",
    "    total_labeled = sum([len([f for f in os.listdir(os.path.join(labeled_dir, s))\n",
    "                            if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "                        for s in species])\n",
    "    print(f\"üéØ Labeled data: {len(species)} species, {total_labeled} images\")\n",
    "    \n",
    "    # Show species distribution\n",
    "    print(\"\\nüêü Species distribution:\")\n",
    "    for species in species[:10]:  # Show first 10 species\n",
    "        species_path = os.path.join(labeled_dir, species)\n",
    "        count = len([f for f in os.listdir(species_path)\n",
    "                    if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"  - {species}: {count} images\")\n",
    "    \n",
    "    if len(species) > 10:\n",
    "        print(f\"  ... and {len(species) - 10} more species\")\n",
    "\n",
    "if os.path.exists(unlabeled_dir):\n",
    "    unlabeled_count = len([f for f in os.listdir(unlabeled_dir)\n",
    "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    print(f\"\\nüé≠ Unlabeled data: {unlabeled_count} images (for MAE pretraining)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset ready for MAE pretraining and EMA fine-tuning!\")\n",
    "\n",
    "# Store global variables for later use\n",
    "LABELED_DIR = labeled_dir\n",
    "UNLABELED_DIR = unlabeled_dir\n",
    "NUM_CLASSES = len(species) if 'species' in locals() else 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff50f82",
   "metadata": {},
   "source": [
    "## üé≠ Section 4: Implement Masked Autoencoder (MAE) Components\n",
    "\n",
    "Creating the complete MAE architecture with ViT encoder, lightweight decoder, and masking strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf920d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Masked Autoencoder (MAE) for Fish Images\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "print(\"üé≠ IMPLEMENTING MASKED AUTOENCODER (MAE)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# CRITICAL: Check GPU memory and compatibility first\n",
    "print(\"üîç CHECKING GPU COMPATIBILITY\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Check CUDA and CUDNN compatibility\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    # Check GPU memory\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "    \n",
    "    print(f\"GPU Total Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"Allocated Memory: {allocated_memory:.1f} GB\")\n",
    "    print(f\"Cached Memory: {cached_memory:.1f} GB\")\n",
    "    print(f\"Available Memory: {gpu_memory - cached_memory:.1f} GB\")\n",
    "    \n",
    "    # Clear any existing GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ GPU cache cleared\")\n",
    "    \n",
    "    # Set memory management\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = False  # More stable for variable input sizes\n",
    "    torch.backends.cudnn.deterministic = True  # For reproducible results\n",
    "    \n",
    "    print(\"‚úÖ CUDNN configured for stability\")\n",
    "    \n",
    "    # Check if we have enough memory for MAE (needs ~8GB minimum)\n",
    "    if gpu_memory < 8.0:\n",
    "        print(\"‚ö†Ô∏è  WARNING: GPU has less than 8GB memory\")\n",
    "        print(\"   Consider using smaller model or batch sizes\")\n",
    "        USE_LIGHTWEIGHT_MAE = True\n",
    "    else:\n",
    "        USE_LIGHTWEIGHT_MAE = False\n",
    "        print(\"‚úÖ Sufficient GPU memory for full MAE model\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No GPU available - MAE requires GPU for reasonable training time\")\n",
    "    raise RuntimeError(\"GPU required for MAE training\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è  BUILDING MAE COMPONENTS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Image to Patch Embedding for MAE\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, \n",
    "                             kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x)  # (B, embed_dim, H/patch_size, W/patch_size)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "# Lightweight MultiHead Attention (to avoid timm dependency issues)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=True)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "# Lightweight Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class MAEEncoder(nn.Module):\n",
    "    \"\"\"Memory-efficient MAE Encoder\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 img_size=224,\n",
    "                 patch_size=16, \n",
    "                 embed_dim=512,  # Reduced from 768\n",
    "                 depth=8,        # Reduced from 12\n",
    "                 num_heads=8,    # Reduced from 12\n",
    "                 mlp_ratio=4.0,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        \n",
    "        # Use custom lightweight blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.pos_embed, std=.02)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def random_masking(self, x, mask_ratio=0.75):\n",
    "        \"\"\"Perform random masking by per-sample shuffling\"\"\"\n",
    "        B, N, D = x.shape\n",
    "        len_keep = int(N * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(B, N, device=x.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "        \n",
    "        # Keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "        \n",
    "        # Generate binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([B, N], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "        \n",
    "        return x_masked, mask, ids_restore\n",
    "    \n",
    "    def forward(self, x, mask_ratio=0.75):\n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Add pos embed without cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        \n",
    "        # Masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "        \n",
    "        # Append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # Apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x, mask, ids_restore\n",
    "\n",
    "class MAEDecoder(nn.Module):\n",
    "    \"\"\"Lightweight MAE Decoder\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_patches=196,\n",
    "                 encoder_embed_dim=512,\n",
    "                 decoder_embed_dim=256,  # Reduced from 512\n",
    "                 decoder_depth=4,        # Reduced from 8\n",
    "                 decoder_num_heads=8,    # Reduced from 16\n",
    "                 mlp_ratio=4.0,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_patches = num_patches\n",
    "        self.decoder_embed = nn.Linear(encoder_embed_dim, decoder_embed_dim, bias=True)\n",
    "        \n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim))\n",
    "        \n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            TransformerBlock(decoder_embed_dim, decoder_num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(decoder_depth)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_norm = nn.LayerNorm(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, 16 * 16 * 3, bias=True)  # patch_size^2 * 3\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "        torch.nn.init.normal_(self.decoder_pos_embed, std=.02)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x, ids_restore):\n",
    "        # Embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "        \n",
    "        # Append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "        \n",
    "        # Add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "        \n",
    "        # Apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "        \n",
    "        # Predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "        \n",
    "        # Remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test with minimal memory allocation first\n",
    "print(\"üß™ Testing basic components...\")\n",
    "\n",
    "try:\n",
    "    # Test patch embedding first\n",
    "    patch_embed = PatchEmbedding(224, 16, 3, 512).to(DEVICE)\n",
    "    test_input = torch.randn(1, 3, 224, 224).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        patches = patch_embed(test_input)\n",
    "    print(f\"‚úÖ Patch embedding works: {patches.shape}\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del patch_embed, test_input, patches\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"‚úÖ Basic components tested successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Component test failed: {e}\")\n",
    "    print(\"\udd27 Try reducing batch size or model dimensions\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nüé≠ MAE components ready for model creation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853e3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the complete MAE model with memory management\n",
    "print(\"üèóÔ∏è  CREATING COMPLETE MAE MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "class MaskedAutoEncoder(nn.Module):\n",
    "    \"\"\"Complete Masked Autoencoder for Fish Images - Memory Optimized\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 patch_size=16,\n",
    "                 encoder_embed_dim=512,  # Reduced for memory efficiency\n",
    "                 encoder_depth=8,        # Reduced from 12\n",
    "                 encoder_num_heads=8,    # Reduced from 12\n",
    "                 decoder_embed_dim=256,  # Reduced from 512\n",
    "                 decoder_depth=4,        # Reduced from 8\n",
    "                 decoder_num_heads=8,    # Reduced from 16\n",
    "                 mlp_ratio=4.0,\n",
    "                 norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "        \n",
    "        print(f\"üîß Encoder: {encoder_embed_dim}d, {encoder_depth} layers, {encoder_num_heads} heads\")\n",
    "        print(f\"üîß Decoder: {decoder_embed_dim}d, {decoder_depth} layers, {decoder_num_heads} heads\")\n",
    "        \n",
    "        # MAE encoder\n",
    "        self.encoder = MAEEncoder(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            embed_dim=encoder_embed_dim,\n",
    "            depth=encoder_depth,\n",
    "            num_heads=encoder_num_heads,\n",
    "            mlp_ratio=mlp_ratio\n",
    "        )\n",
    "        \n",
    "        # MAE decoder\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.decoder = MAEDecoder(\n",
    "            num_patches=num_patches,\n",
    "            encoder_embed_dim=encoder_embed_dim,\n",
    "            decoder_embed_dim=decoder_embed_dim,\n",
    "            decoder_depth=decoder_depth,\n",
    "            decoder_num_heads=decoder_num_heads,\n",
    "            mlp_ratio=mlp_ratio\n",
    "        )\n",
    "        \n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"Convert images to patches\"\"\"\n",
    "        p = self.patch_size\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "        \n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x\n",
    "    \n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"Convert patches back to images\"\"\"\n",
    "        p = self.patch_size\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs\n",
    "    \n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"Compute reconstruction loss\"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        \n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "        \n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "        \n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, imgs, mask_ratio=0.75):\n",
    "        latent, mask, ids_restore = self.encoder(imgs, mask_ratio)\n",
    "        pred = self.decoder(latent, ids_restore)\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask, latent\n",
    "\n",
    "# Create MAE model with progressive memory checking\n",
    "print(\"üöÄ Creating MAE model...\")\n",
    "\n",
    "try:\n",
    "    # Check memory before model creation\n",
    "    if torch.cuda.is_available():\n",
    "        memory_before = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "        print(f\"Memory before model: {memory_before:.2f} GB\")\n",
    "    \n",
    "    # Create model with reduced dimensions for Colab compatibility\n",
    "    mae_model = MaskedAutoEncoder(\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        encoder_embed_dim=512,  # Reduced from 768\n",
    "        encoder_depth=8,        # Reduced from 12\n",
    "        encoder_num_heads=8,    # Reduced from 12\n",
    "        decoder_embed_dim=256,  # Reduced from 512\n",
    "        decoder_depth=4,        # Reduced from 8\n",
    "        decoder_num_heads=8,    # Reduced from 16\n",
    "        mlp_ratio=4.0,\n",
    "        norm_pix_loss=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Model created successfully in CPU memory\")\n",
    "    \n",
    "    # Move to GPU gradually\n",
    "    print(\"üîÑ Moving model to GPU...\")\n",
    "    mae_model = mae_model.to(DEVICE)\n",
    "    \n",
    "    # Check memory after model creation\n",
    "    if torch.cuda.is_available():\n",
    "        memory_after = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "        print(f\"Memory after model: {memory_after:.2f} GB\")\n",
    "        print(f\"Model memory usage: {memory_after - memory_before:.2f} GB\")\n",
    "    \n",
    "    print(\"‚úÖ Model successfully moved to GPU!\")\n",
    "    \n",
    "    # Count parameters\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    total_params = count_parameters(mae_model)\n",
    "    encoder_params = count_parameters(mae_model.encoder)\n",
    "    decoder_params = count_parameters(mae_model.decoder)\n",
    "\n",
    "    print(f\"\\nüìä MODEL STATISTICS\")\n",
    "    print(f\"üìä Total parameters: {total_params:,}\")\n",
    "    print(f\"üìä Encoder parameters: {encoder_params:,}\")\n",
    "    print(f\"üìä Decoder parameters: {decoder_params:,}\")\n",
    "    print(f\"üéØ Model ready for pretraining\")\n",
    "\n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    print(f\"‚ùå GPU Out of Memory: {e}\")\n",
    "    print(\"üîß Solutions:\")\n",
    "    print(\"   1. Restart runtime and run again\")\n",
    "    print(\"   2. Use smaller batch sizes (try batch_size=16)\")\n",
    "    print(\"   3. Further reduce model dimensions\")\n",
    "    print(\"   4. Use Colab Pro for more GPU memory\")\n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model creation failed: {e}\")\n",
    "    print(\"üîß Check CUDA installation and GPU compatibility\")\n",
    "    raise\n",
    "\n",
    "# Test model with a small batch\n",
    "print(\"\\nüß™ TESTING MAE MODEL\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "try:\n",
    "    # Test with minimal batch size\n",
    "    batch_size = 2\n",
    "    test_input = torch.randn(batch_size, 3, 224, 224).to(DEVICE)\n",
    "    \n",
    "    print(f\"üß™ Testing with batch size {batch_size}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss, pred, mask, latent = mae_model(test_input, mask_ratio=0.75)\n",
    "        \n",
    "    print(f\"‚úÖ Forward pass successful!\")\n",
    "    print(f\"üìä Loss: {loss.item():.4f}\")\n",
    "    print(f\"üìä Prediction shape: {pred.shape}\")\n",
    "    print(f\"üìä Mask shape: {mask.shape}\")\n",
    "    print(f\"üìä Latent shape: {latent.shape}\")\n",
    "    print(f\"üìä Masked patches: {mask.sum(dim=1).float().mean().item():.1f} / {mask.shape[1]}\")\n",
    "    \n",
    "    # Clean up test\n",
    "    del test_input, loss, pred, mask, latent\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nüé≠ MAE MODEL READY FOR PRETRAINING!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model test failed: {e}\")\n",
    "    print(\"üîß Try reducing batch size or restarting runtime\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501135d3",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Section 5: Configure MAE Pretraining Parameters\n",
    "\n",
    "Setting up optimal hyperparameters for self-supervised MAE pretraining on fish images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eec2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MAE Pretraining Parameters and Data Pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚öôÔ∏è CONFIGURING MAE PRETRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# MAE Pretraining Configuration\n",
    "MAE_CONFIG = {\n",
    "    # Model settings\n",
    "    'mask_ratio': 0.75,  # Aggressive masking for strong representation learning\n",
    "    'img_size': 224,\n",
    "    'patch_size': 16,\n",
    "    \n",
    "    # Training settings\n",
    "    'epochs': 50,  # Sufficient for good representations on fish data\n",
    "    'batch_size': 64,  # Optimized for GPU memory\n",
    "    'learning_rate': 1e-4,  # Conservative LR for stable training\n",
    "    'weight_decay': 0.05,\n",
    "    'warmup_epochs': 5,\n",
    "    \n",
    "    # Optimization\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'clip_grad': 1.0,\n",
    "    \n",
    "    # Saving\n",
    "    'save_frequency': 10,  # Save every 10 epochs\n",
    "    'checkpoint_dir': '/content/drive/MyDrive/ViT-FishID/mae_checkpoints',\n",
    "    \n",
    "    # Logging\n",
    "    'use_wandb': True,\n",
    "    'wandb_project': 'ViT-FishID-MAE-Pretraining',\n",
    "    'wandb_run_name': f'mae-pretrain-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}',\n",
    "    \n",
    "    # Data\n",
    "    'data_dir': UNLABELED_DIR,\n",
    "    'num_workers': 4,\n",
    "}\n",
    "\n",
    "print(\"üìä MAE Configuration:\")\n",
    "for key, value in MAE_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(MAE_CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "print(f\"\\nüíæ Checkpoint directory: {MAE_CONFIG['checkpoint_dir']}\")\n",
    "\n",
    "# MAE Dataset for Unlabeled Images\n",
    "class MAEDataset(Dataset):\n",
    "    \"\"\"Dataset for MAE pretraining using unlabeled fish images\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, img_size=224):\n",
    "        self.data_dir = data_dir\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Find all image files\n",
    "        self.image_paths = []\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
    "            self.image_paths.extend(glob.glob(os.path.join(data_dir, ext)))\n",
    "        \n",
    "        print(f\"üìä Found {len(self.image_paths)} unlabeled images for MAE pretraining\")\n",
    "        \n",
    "        # MAE-specific transforms - minimal augmentation to preserve structure\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load and process image\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load {img_path}: {e}\")\n",
    "            # Return a black image as fallback\n",
    "            return torch.zeros(3, self.img_size, self.img_size)\n",
    "\n",
    "# Create MAE dataset and dataloader\n",
    "print(\"\\nüì¶ Creating MAE dataset...\")\n",
    "mae_dataset = MAEDataset(MAE_CONFIG['data_dir'], MAE_CONFIG['img_size'])\n",
    "\n",
    "mae_dataloader = DataLoader(\n",
    "    mae_dataset,\n",
    "    batch_size=MAE_CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=MAE_CONFIG['num_workers'],\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ MAE DataLoader created:\")\n",
    "print(f\"  üìä Dataset size: {len(mae_dataset)} images\")\n",
    "print(f\"  üìä Batch size: {MAE_CONFIG['batch_size']}\")\n",
    "print(f\"  üìä Batches per epoch: {len(mae_dataloader)}\")\n",
    "print(f\"  ‚è±Ô∏è  Estimated epoch time: {len(mae_dataloader) * 0.5:.1f}s\")\n",
    "\n",
    "# Setup MAE optimizer and scheduler\n",
    "print(\"\\nüîß Setting up MAE optimizer...\")\n",
    "\n",
    "mae_optimizer = optim.AdamW(\n",
    "    mae_model.parameters(),\n",
    "    lr=MAE_CONFIG['learning_rate'],\n",
    "    betas=(MAE_CONFIG['beta1'], MAE_CONFIG['beta2']),\n",
    "    weight_decay=MAE_CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Cosine annealing scheduler with warmup\n",
    "def cosine_scheduler(optimizer, warmup_epochs, total_epochs):\n",
    "    \"\"\"Cosine annealing scheduler with linear warmup\"\"\"\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return epoch / warmup_epochs\n",
    "        else:\n",
    "            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "            return 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    \n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "mae_scheduler = cosine_scheduler(\n",
    "    mae_optimizer, \n",
    "    MAE_CONFIG['warmup_epochs'], \n",
    "    MAE_CONFIG['epochs']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Optimizer: AdamW with LR={MAE_CONFIG['learning_rate']}\")\n",
    "print(f\"‚úÖ Scheduler: Cosine annealing with {MAE_CONFIG['warmup_epochs']} warmup epochs\")\n",
    "\n",
    "# Initialize Weights & Biases for MAE pretraining\n",
    "if MAE_CONFIG['use_wandb']:\n",
    "    print(\"\\nüìà Initializing Weights & Biases for MAE pretraining...\")\n",
    "    try:\n",
    "        wandb.init(\n",
    "            project=MAE_CONFIG['wandb_project'],\n",
    "            name=MAE_CONFIG['wandb_run_name'],\n",
    "            config=MAE_CONFIG,\n",
    "            tags=['mae', 'pretraining', 'fish', 'self-supervised']\n",
    "        )\n",
    "        print(f\"‚úÖ W&B initialized: {wandb.run.url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  W&B initialization failed: {e}\")\n",
    "        MAE_CONFIG['use_wandb'] = False\n",
    "\n",
    "# Test data loading\n",
    "print(\"\\nüß™ Testing data pipeline...\")\n",
    "try:\n",
    "    test_batch = next(iter(mae_dataloader))\n",
    "    print(f\"‚úÖ Data loading successful!\")\n",
    "    print(f\"üìä Batch shape: {test_batch.shape}\")\n",
    "    print(f\"üìä Batch dtype: {test_batch.dtype}\")\n",
    "    print(f\"üìä Value range: [{test_batch.min():.3f}, {test_batch.max():.3f}]\")\n",
    "    \n",
    "    # Test MAE forward pass\n",
    "    with torch.no_grad():\n",
    "        test_batch = test_batch.to(DEVICE)\n",
    "        loss, pred, mask, latent = mae_model(test_batch, MAE_CONFIG['mask_ratio'])\n",
    "        print(f\"‚úÖ MAE forward pass successful!\")\n",
    "        print(f\"üìä Reconstruction loss: {loss.item():.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Data pipeline test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\nüé≠ MAE pretraining configuration complete!\")\n",
    "print(f\"üöÄ Ready to start self-supervised pretraining on {len(mae_dataset)} fish images\")\n",
    "print(f\"‚è±Ô∏è  Estimated total pretraining time: {MAE_CONFIG['epochs'] * len(mae_dataloader) * 0.5 / 3600:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a8ebe",
   "metadata": {},
   "source": [
    "## üöÄ Section 6: Execute MAE Pretraining Phase\n",
    "\n",
    "Running self-supervised pretraining to learn robust visual representations from unlabeled fish images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89459b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute MAE Pretraining on Unlabeled Fish Images\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"üé≠ STARTING MAE PRETRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def save_mae_checkpoint(model, optimizer, scheduler, epoch, loss, config, filename):\n",
    "    \"\"\"Save MAE checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss,\n",
    "        'config': config,\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"üíæ Saved checkpoint: {filename}\")\n",
    "\n",
    "def visualize_mae_reconstruction(model, dataloader, device, epoch, num_samples=4):\n",
    "    \"\"\"Visualize MAE reconstruction results\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get a batch of images\n",
    "        images = next(iter(dataloader))[:num_samples].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        loss, pred, mask, latent = model(images, mask_ratio=0.75)\n",
    "        \n",
    "        # Convert to numpy for visualization\n",
    "        images_np = images.cpu().numpy()\n",
    "        pred_np = pred.cpu().numpy()\n",
    "        mask_np = mask.cpu().numpy()\n",
    "        \n",
    "        # Denormalize images\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            img = images_np[i].transpose(1, 2, 0)\n",
    "            img = img * std + mean\n",
    "            img = np.clip(img, 0, 1)\n",
    "            \n",
    "            # Create reconstruction\n",
    "            pred_patches = pred_np[i]  # [N_patches, patch_size^2 * 3]\n",
    "            \n",
    "            # This is a simplified visualization - in practice you'd want to \n",
    "            # properly reconstruct the image from patches\n",
    "            \n",
    "        print(f\"üìä Reconstruction loss at epoch {epoch}: {loss.item():.4f}\")\n",
    "        \n",
    "        if wandb.run and MAE_CONFIG['use_wandb']:\n",
    "            try:\n",
    "                # Log some metrics to wandb\n",
    "                wandb.log({\n",
    "                    'mae_reconstruction_loss': loss.item(),\n",
    "                    'epoch': epoch\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "# Training loop\n",
    "print(f\"üé¨ Starting MAE pretraining for {MAE_CONFIG['epochs']} epochs...\")\n",
    "print(f\"üìä Training on {len(mae_dataset)} unlabeled fish images\")\n",
    "print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "mae_model.train()\n",
    "best_loss = float('inf')\n",
    "training_losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(MAE_CONFIG['epochs']):\n",
    "    epoch_start_time = time.time()\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Progress bar for epoch\n",
    "    pbar = tqdm(mae_dataloader, desc=f\"MAE Epoch {epoch+1}/{MAE_CONFIG['epochs']}\")\n",
    "    \n",
    "    for batch_idx, images in enumerate(pbar):\n",
    "        images = images.to(DEVICE)\n",
    "        \n",
    "        # Zero gradients\n",
    "        mae_optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        loss, pred, mask, latent = mae_model(images, MAE_CONFIG['mask_ratio'])\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if MAE_CONFIG['clip_grad'] > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(mae_model.parameters(), MAE_CONFIG['clip_grad'])\n",
    "        \n",
    "        # Optimizer step\n",
    "        mae_optimizer.step()\n",
    "        \n",
    "        # Record loss\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f\"{loss.item():.4f}\",\n",
    "            'LR': f\"{mae_optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        })\n",
    "        \n",
    "        # Log to wandb\n",
    "        if MAE_CONFIG['use_wandb'] and wandb.run:\n",
    "            try:\n",
    "                wandb.log({\n",
    "                    'mae_batch_loss': loss.item(),\n",
    "                    'mae_learning_rate': mae_optimizer.param_groups[0]['lr'],\n",
    "                    'mae_step': epoch * len(mae_dataloader) + batch_idx\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Scheduler step\n",
    "    mae_scheduler.step()\n",
    "    \n",
    "    # Epoch statistics\n",
    "    epoch_loss = np.mean(epoch_losses)\n",
    "    training_losses.append(epoch_loss)\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    print(f\"\\nüìä Epoch {epoch+1} Summary:\")\n",
    "    print(f\"  üìâ Average Loss: {epoch_loss:.4f}\")\n",
    "    print(f\"  ‚è±Ô∏è  Time: {epoch_time:.1f}s\")\n",
    "    print(f\"  üìà Learning Rate: {mae_optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Visualize reconstruction periodically\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        recon_loss = visualize_mae_reconstruction(mae_model, mae_dataloader, DEVICE, epoch + 1)\n",
    "    \n",
    "    # Save checkpoint periodically\n",
    "    if (epoch + 1) % MAE_CONFIG['save_frequency'] == 0:\n",
    "        checkpoint_path = os.path.join(\n",
    "            MAE_CONFIG['checkpoint_dir'], \n",
    "            f'mae_checkpoint_epoch_{epoch+1}.pth'\n",
    "        )\n",
    "        save_mae_checkpoint(\n",
    "            mae_model, mae_optimizer, mae_scheduler, \n",
    "            epoch + 1, epoch_loss, MAE_CONFIG, checkpoint_path\n",
    "        )\n",
    "    \n",
    "    # Save best model\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        best_checkpoint_path = os.path.join(\n",
    "            MAE_CONFIG['checkpoint_dir'], \n",
    "            'mae_best_model.pth'\n",
    "        )\n",
    "        save_mae_checkpoint(\n",
    "            mae_model, mae_optimizer, mae_scheduler, \n",
    "            epoch + 1, epoch_loss, MAE_CONFIG, best_checkpoint_path\n",
    "        )\n",
    "        print(f\"üèÜ New best model saved! Loss: {best_loss:.4f}\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Training completed\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nüéâ MAE PRETRAINING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è∞ Total training time: {total_time/3600:.2f} hours\")\n",
    "print(f\"üèÜ Best reconstruction loss: {best_loss:.4f}\")\n",
    "print(f\"üìà Final learning rate: {mae_optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "# Save final checkpoint\n",
    "final_checkpoint_path = os.path.join(\n",
    "    MAE_CONFIG['checkpoint_dir'], \n",
    "    'mae_final_model.pth'\n",
    ")\n",
    "save_mae_checkpoint(\n",
    "    mae_model, mae_optimizer, mae_scheduler, \n",
    "    MAE_CONFIG['epochs'], training_losses[-1], MAE_CONFIG, final_checkpoint_path\n",
    ")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(training_losses) + 1), training_losses, 'b-', linewidth=2)\n",
    "plt.title('MAE Pretraining Loss Curve', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Reconstruction Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MAE_CONFIG['checkpoint_dir'], 'mae_training_curve.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Log final results to wandb\n",
    "if MAE_CONFIG['use_wandb'] and wandb.run:\n",
    "    try:\n",
    "        wandb.log({\n",
    "            'mae_final_loss': training_losses[-1],\n",
    "            'mae_best_loss': best_loss,\n",
    "            'mae_training_time_hours': total_time/3600,\n",
    "            'mae_total_epochs': MAE_CONFIG['epochs']\n",
    "        })\n",
    "        wandb.finish()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\n‚úÖ MAE encoder is now pretrained on fish images!\")\n",
    "print(f\"üíæ Checkpoints saved to: {MAE_CONFIG['checkpoint_dir']}\")\n",
    "print(f\"üéØ Ready to extract pretrained weights for classification training!\")\n",
    "\n",
    "# Store checkpoint path for next stage\n",
    "MAE_PRETRAINED_PATH = best_checkpoint_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383172de",
   "metadata": {},
   "source": [
    "## üîÑ Section 7: Load MAE Pretrained Weights for ViT\n",
    "\n",
    "Extracting pretrained encoder weights from MAE and loading them into the ViT classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MAE Pretrained Weights into ViT Classification Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "print(\"üîÑ LOADING MAE PRETRAINED WEIGHTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Enhanced ViT model with MAE pretraining support\n",
    "class MAEPretrainedViT(nn.Module):\n",
    "    \"\"\"ViT model that can load MAE pretrained encoder weights\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, mae_encoder=None, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        if mae_encoder is not None:\n",
    "            # Use pretrained MAE encoder\n",
    "            self.backbone = mae_encoder\n",
    "            self.feature_dim = mae_encoder.blocks[0].norm1.normalized_shape[0]  # Get embed_dim\n",
    "            print(f\"‚úÖ Using MAE pretrained encoder with {self.feature_dim} features\")\n",
    "        else:\n",
    "            # Fallback to timm ViT\n",
    "            import timm\n",
    "            self.backbone = timm.create_model(\n",
    "                'vit_base_patch16_224',\n",
    "                pretrained=True,\n",
    "                num_classes=0,\n",
    "                global_pool='token'\n",
    "            )\n",
    "            self.feature_dim = self.backbone.num_features\n",
    "            print(f\"‚ö†Ô∏è  Using ImageNet pretrained ViT with {self.feature_dim} features\")\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(self.feature_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(self.feature_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize classification head\n",
    "        self._init_classifier()\n",
    "    \n",
    "    def _init_classifier(self):\n",
    "        \"\"\"Initialize the classification head weights\"\"\"\n",
    "        for module in self.classifier.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.trunc_normal_(module.weight, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for classification\"\"\"\n",
    "        if hasattr(self.backbone, 'patch_embed'):\n",
    "            # MAE encoder forward pass\n",
    "            x = self.backbone.patch_embed(x)\n",
    "            x = x + self.backbone.pos_embed[:, 1:, :]  # Add pos embed without cls\n",
    "            \n",
    "            # Add cls token\n",
    "            cls_token = self.backbone.cls_token + self.backbone.pos_embed[:, :1, :]\n",
    "            cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "            \n",
    "            # Apply transformer blocks\n",
    "            for blk in self.backbone.blocks:\n",
    "                x = blk(x)\n",
    "            x = self.backbone.norm(x)\n",
    "            \n",
    "            # Take cls token\n",
    "            x = x[:, 0]\n",
    "        else:\n",
    "            # Standard timm ViT forward\n",
    "            x = self.backbone(x)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        \"\"\"Extract features without classification\"\"\"\n",
    "        if hasattr(self.backbone, 'patch_embed'):\n",
    "            # MAE encoder feature extraction\n",
    "            x = self.backbone.patch_embed(x)\n",
    "            x = x + self.backbone.pos_embed[:, 1:, :]\n",
    "            \n",
    "            cls_token = self.backbone.cls_token + self.backbone.pos_embed[:, :1, :]\n",
    "            cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "            \n",
    "            for blk in self.backbone.blocks:\n",
    "                x = blk(x)\n",
    "            x = self.backbone.norm(x)\n",
    "            \n",
    "            return x[:, 0]  # Return cls token features\n",
    "        else:\n",
    "            return self.backbone(x)\n",
    "\n",
    "print(f\"üèóÔ∏è  Creating ViT classification model...\")\n",
    "\n",
    "# Load MAE checkpoint\n",
    "if 'MAE_PRETRAINED_PATH' in globals() and os.path.exists(MAE_PRETRAINED_PATH):\n",
    "    print(f\"üìÇ Loading MAE checkpoint: {MAE_PRETRAINED_PATH}\")\n",
    "    \n",
    "    mae_checkpoint = torch.load(MAE_PRETRAINED_PATH, map_location='cpu')\n",
    "    print(f\"‚úÖ MAE checkpoint loaded (epoch {mae_checkpoint['epoch']})\")\n",
    "    print(f\"üìä MAE training loss: {mae_checkpoint['loss']:.4f}\")\n",
    "    \n",
    "    # Create new MAE model and load weights\n",
    "    mae_pretrained = MaskedAutoEncoder(\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        encoder_embed_dim=768,\n",
    "        encoder_depth=12,\n",
    "        encoder_num_heads=12,\n",
    "        decoder_embed_dim=512,\n",
    "        decoder_depth=8,\n",
    "        decoder_num_heads=16,\n",
    "        mlp_ratio=4.0,\n",
    "        norm_pix_loss=True\n",
    "    )\n",
    "    \n",
    "    mae_pretrained.load_state_dict(mae_checkpoint['model_state_dict'])\n",
    "    mae_encoder = mae_pretrained.encoder\n",
    "    \n",
    "    # Create ViT with MAE pretrained encoder\n",
    "    vit_model = MAEPretrainedViT(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        mae_encoder=mae_encoder,\n",
    "        dropout_rate=0.1\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(f\"‚úÖ ViT model created with MAE pretrained encoder!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  MAE checkpoint not found, using ImageNet pretrained ViT\")\n",
    "    vit_model = MAEPretrainedViT(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        mae_encoder=None,\n",
    "        dropout_rate=0.1\n",
    "    ).to(DEVICE)\n",
    "\n",
    "# Model statistics\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params = count_parameters(vit_model)\n",
    "backbone_params = count_parameters(vit_model.backbone)\n",
    "classifier_params = count_parameters(vit_model.classifier)\n",
    "\n",
    "print(f\"\\nüìä ViT Model Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Backbone parameters: {backbone_params:,}\")\n",
    "print(f\"  Classifier parameters: {classifier_params:,}\")\n",
    "print(f\"  Number of classes: {NUM_CLASSES}\")\n",
    "\n",
    "# Test model forward pass\n",
    "print(f\"\\nüß™ Testing ViT model...\")\n",
    "test_input = torch.randn(2, 3, 224, 224).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test classification\n",
    "    logits = vit_model(test_input)\n",
    "    print(f\"‚úÖ Classification forward pass successful!\")\n",
    "    print(f\"üìä Logits shape: {logits.shape}\")\n",
    "    \n",
    "    # Test feature extraction\n",
    "    features = vit_model.get_features(test_input)\n",
    "    print(f\"‚úÖ Feature extraction successful!\")\n",
    "    print(f\"üìä Features shape: {features.shape}\")\n",
    "\n",
    "# Enhanced ViT model is now ready with MAE pretrained weights\n",
    "print(f\"\\nüéØ ViT MODEL READY FOR EMA TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úÖ Model architecture: Vision Transformer\")\n",
    "print(f\"‚úÖ Pretraining: {'MAE self-supervised' if 'MAE_PRETRAINED_PATH' in globals() else 'ImageNet supervised'}\")\n",
    "print(f\"‚úÖ Classification head: Initialized for {NUM_CLASSES} fish species\")\n",
    "print(f\"üöÄ Ready for EMA student-teacher semi-supervised training!\")\n",
    "\n",
    "# Clean up MAE model to free memory\n",
    "if 'mae_model' in globals():\n",
    "    del mae_model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"üóëÔ∏è  Cleaned up MAE model to free GPU memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64effe8a",
   "metadata": {},
   "source": [
    "## üéì Section 8: Configure EMA Student-Teacher Framework\n",
    "\n",
    "Setting up the EMA teacher model and semi-supervised learning pipeline with the MAE-pretrained backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f80a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure EMA Student-Teacher Framework for Semi-Supervised Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import copy\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üéì CONFIGURING EMA STUDENT-TEACHER FRAMEWORK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# EMA Teacher Implementation\n",
    "class EMATeacher(nn.Module):\n",
    "    \"\"\"Exponential Moving Average Teacher for Semi-Supervised Learning\"\"\"\n",
    "    \n",
    "    def __init__(self, student_model, ema_decay=0.995):\n",
    "        super().__init__()\n",
    "        self.ema_decay = ema_decay\n",
    "        self.student_model = student_model\n",
    "        \n",
    "        # Create teacher as copy of student\n",
    "        self.teacher_model = copy.deepcopy(student_model)\n",
    "        \n",
    "        # Disable gradients for teacher\n",
    "        for param in self.teacher_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        print(f\"‚úÖ EMA Teacher created with decay: {ema_decay}\")\n",
    "    \n",
    "    def update_teacher(self):\n",
    "        \"\"\"Update teacher weights using EMA\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for teacher_param, student_param in zip(\n",
    "                self.teacher_model.parameters(), \n",
    "                self.student_model.parameters()\n",
    "            ):\n",
    "                teacher_param.data = (\n",
    "                    self.ema_decay * teacher_param.data + \n",
    "                    (1 - self.ema_decay) * student_param.data\n",
    "                )\n",
    "    \n",
    "    def forward(self, x, use_teacher=False):\n",
    "        \"\"\"Forward pass through student or teacher\"\"\"\n",
    "        if use_teacher:\n",
    "            return self.teacher_model(x)\n",
    "        else:\n",
    "            return self.student_model(x)\n",
    "    \n",
    "    def get_teacher_predictions(self, x):\n",
    "        \"\"\"Get teacher predictions for pseudo-labeling\"\"\"\n",
    "        self.teacher_model.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.teacher_model(x)\n",
    "\n",
    "# Consistency Loss for Semi-Supervised Learning\n",
    "class ConsistencyLoss(nn.Module):\n",
    "    \"\"\"Consistency loss between student and teacher predictions\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=4.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    def forward(self, student_logits, teacher_logits):\n",
    "        \"\"\"Compute consistency loss using KL divergence\"\"\"\n",
    "        # Apply temperature scaling\n",
    "        student_probs = F.log_softmax(student_logits / self.temperature, dim=1)\n",
    "        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n",
    "        \n",
    "        # KL divergence loss\n",
    "        consistency_loss = self.kl_div(student_probs, teacher_probs)\n",
    "        consistency_loss *= (self.temperature ** 2)\n",
    "        \n",
    "        return consistency_loss\n",
    "\n",
    "# Semi-Supervised Dataset\n",
    "class FishSemiSupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset combining labeled and unlabeled fish images\"\"\"\n",
    "    \n",
    "    def __init__(self, labeled_dir, unlabeled_dir, img_size=224, mode='train'):\n",
    "        self.img_size = img_size\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Load labeled data\n",
    "        self.labeled_data = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "        \n",
    "        if os.path.exists(labeled_dir):\n",
    "            species_dirs = [d for d in os.listdir(labeled_dir) \n",
    "                          if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')]\n",
    "            species_dirs.sort()\n",
    "            \n",
    "            for idx, species in enumerate(species_dirs):\n",
    "                self.class_to_idx[species] = idx\n",
    "                self.idx_to_class[idx] = species\n",
    "                \n",
    "                species_path = os.path.join(labeled_dir, species)\n",
    "                for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
    "                    for img_path in glob.glob(os.path.join(species_path, ext)):\n",
    "                        self.labeled_data.append((img_path, idx))\n",
    "        \n",
    "        # Load unlabeled data\n",
    "        self.unlabeled_data = []\n",
    "        if os.path.exists(unlabeled_dir):\n",
    "            for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
    "                for img_path in glob.glob(os.path.join(unlabeled_dir, ext)):\n",
    "                    self.unlabeled_data.append(img_path)\n",
    "        \n",
    "        print(f\"üìä Loaded {len(self.labeled_data)} labeled images from {len(self.class_to_idx)} species\")\n",
    "        print(f\"üìä Loaded {len(self.unlabeled_data)} unlabeled images\")\n",
    "        \n",
    "        # Data augmentation transforms\n",
    "        if mode == 'train':\n",
    "            self.labeled_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=15),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            self.unlabeled_transform_weak = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            self.unlabeled_transform_strong = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=20),\n",
    "                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.labeled_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labeled_data) + len(self.unlabeled_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.labeled_data):\n",
    "            # Labeled data\n",
    "            img_path, label = self.labeled_data[idx]\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                image = self.labeled_transform(image)\n",
    "                return image, label, True  # True indicates labeled data\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load {img_path}: {e}\")\n",
    "                return torch.zeros(3, self.img_size, self.img_size), 0, True\n",
    "        else:\n",
    "            # Unlabeled data\n",
    "            unlabeled_idx = idx - len(self.labeled_data)\n",
    "            img_path = self.unlabeled_data[unlabeled_idx]\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                \n",
    "                if self.mode == 'train':\n",
    "                    # Return both weak and strong augmentations for consistency training\n",
    "                    image_weak = self.unlabeled_transform_weak(image)\n",
    "                    image_strong = self.unlabeled_transform_strong(image)\n",
    "                    return (image_weak, image_strong), -1, False  # -1 indicates no label, False indicates unlabeled\n",
    "                else:\n",
    "                    image = self.labeled_transform(image)\n",
    "                    return image, -1, False\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load {img_path}: {e}\")\n",
    "                if self.mode == 'train':\n",
    "                    zero_img = torch.zeros(3, self.img_size, self.img_size)\n",
    "                    return (zero_img, zero_img), -1, False\n",
    "                else:\n",
    "                    return torch.zeros(3, self.img_size, self.img_size), -1, False\n",
    "\n",
    "# EMA Training Configuration\n",
    "EMA_CONFIG = {\n",
    "    # Model settings\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'img_size': 224,\n",
    "    \n",
    "    # Training settings\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32,  # Balanced for labeled + unlabeled data\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.05,\n",
    "    'warmup_epochs': 10,\n",
    "    \n",
    "    # EMA settings\n",
    "    'ema_decay': 0.995,\n",
    "    'consistency_weight': 2.0,\n",
    "    'pseudo_label_threshold': 0.7,\n",
    "    'temperature': 4.0,\n",
    "    'ramp_up_epochs': 20,\n",
    "    \n",
    "    # Optimization\n",
    "    'clip_grad': 1.0,\n",
    "    'label_smoothing': 0.1,\n",
    "    \n",
    "    # Saving\n",
    "    'save_frequency': 10,\n",
    "    'checkpoint_dir': '/content/drive/MyDrive/ViT-FishID/ema_checkpoints',\n",
    "    \n",
    "    # Logging\n",
    "    'use_wandb': True,\n",
    "    'wandb_project': 'ViT-FishID-EMA-Training',\n",
    "    'wandb_run_name': f'ema-mae-pretrained-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}',\n",
    "    \n",
    "    # Data\n",
    "    'labeled_dir': LABELED_DIR,\n",
    "    'unlabeled_dir': UNLABELED_DIR,\n",
    "    'train_split': 0.8,\n",
    "    'num_workers': 4,\n",
    "}\n",
    "\n",
    "print(\"üìä EMA Configuration:\")\n",
    "for key, value in EMA_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(EMA_CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "print(f\"\\nüíæ EMA Checkpoint directory: {EMA_CONFIG['checkpoint_dir']}\")\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "print(f\"\\nüì¶ Creating semi-supervised datasets...\")\n",
    "\n",
    "# Full dataset\n",
    "full_dataset = FishSemiSupervisedDataset(\n",
    "    EMA_CONFIG['labeled_dir'], \n",
    "    EMA_CONFIG['unlabeled_dir'], \n",
    "    EMA_CONFIG['img_size'], \n",
    "    mode='train'\n",
    ")\n",
    "\n",
    "# Split labeled data into train/validation\n",
    "labeled_size = len(full_dataset.labeled_data)\n",
    "train_size = int(EMA_CONFIG['train_split'] * labeled_size)\n",
    "val_size = labeled_size - train_size\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_labeled_data = full_dataset.labeled_data[:train_size]\n",
    "val_labeled_data = full_dataset.labeled_data[train_size:]\n",
    "\n",
    "# Training dataset (includes all unlabeled data)\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, labeled_data, unlabeled_data, class_to_idx, img_size):\n",
    "        self.labeled_data = labeled_data\n",
    "        self.unlabeled_data = unlabeled_data\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Same transforms as before\n",
    "        self.labeled_transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.unlabeled_transform_weak = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.unlabeled_transform_strong = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=20),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labeled_data) + len(self.unlabeled_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.labeled_data):\n",
    "            # Labeled data\n",
    "            img_path, label = self.labeled_data[idx]\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                image = self.labeled_transform(image)\n",
    "                return image, label, True\n",
    "            except:\n",
    "                return torch.zeros(3, self.img_size, self.img_size), 0, True\n",
    "        else:\n",
    "            # Unlabeled data\n",
    "            unlabeled_idx = idx - len(self.labeled_data)\n",
    "            img_path = self.unlabeled_data[unlabeled_idx]\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                image_weak = self.unlabeled_transform_weak(image)\n",
    "                image_strong = self.unlabeled_transform_strong(image)\n",
    "                return (image_weak, image_strong), -1, False\n",
    "            except:\n",
    "                zero_img = torch.zeros(3, self.img_size, self.img_size)\n",
    "                return (zero_img, zero_img), -1, False\n",
    "\n",
    "# Validation dataset\n",
    "class ValDataset(Dataset):\n",
    "    def __init__(self, labeled_data, class_to_idx, img_size):\n",
    "        self.labeled_data = labeled_data\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labeled_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.labeled_data[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "            return image, label\n",
    "        except:\n",
    "            return torch.zeros(3, self.img_size, self.img_size), 0\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TrainDataset(\n",
    "    train_labeled_data, \n",
    "    full_dataset.unlabeled_data, \n",
    "    full_dataset.class_to_idx, \n",
    "    EMA_CONFIG['img_size']\n",
    ")\n",
    "\n",
    "val_dataset = ValDataset(\n",
    "    val_labeled_data, \n",
    "    full_dataset.class_to_idx, \n",
    "    EMA_CONFIG['img_size']\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=EMA_CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=EMA_CONFIG['num_workers'],\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=EMA_CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=EMA_CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Datasets created:\")\n",
    "print(f\"  üìä Training: {len(train_dataset)} samples ({len(train_labeled_data)} labeled + {len(full_dataset.unlabeled_data)} unlabeled)\")\n",
    "print(f\"  üìä Validation: {len(val_dataset)} samples (labeled)\")\n",
    "print(f\"  üìä Classes: {len(full_dataset.class_to_idx)}\")\n",
    "\n",
    "# Create EMA teacher\n",
    "ema_teacher = EMATeacher(vit_model, ema_decay=EMA_CONFIG['ema_decay']).to(DEVICE)\n",
    "consistency_loss_fn = ConsistencyLoss(temperature=EMA_CONFIG['temperature']).to(DEVICE)\n",
    "\n",
    "print(f\"\\n‚úÖ EMA Framework Ready:\")\n",
    "print(f\"  üéì Student model: MAE-pretrained ViT\")\n",
    "print(f\"  üë®‚Äçüè´ Teacher model: EMA with decay {EMA_CONFIG['ema_decay']}\")\n",
    "print(f\"  üîÑ Consistency loss: KL divergence with temperature {EMA_CONFIG['temperature']}\")\n",
    "print(f\"üöÄ Ready for semi-supervised training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c5a67",
   "metadata": {},
   "source": [
    "## üöÄ Section 9: Execute Semi-Supervised Training with EMA\n",
    "\n",
    "Running the complete semi-supervised training pipeline combining labeled supervision with unlabeled consistency learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b01226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Semi-Supervised EMA Training with MAE Pretrained Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üéì STARTING EMA SEMI-SUPERVISED TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "ema_optimizer = optim.AdamW(\n",
    "    vit_model.parameters(),\n",
    "    lr=EMA_CONFIG['learning_rate'],\n",
    "    weight_decay=EMA_CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Cosine annealing scheduler with warmup\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))\n",
    "    \n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "total_steps = len(train_dataloader) * EMA_CONFIG['epochs']\n",
    "warmup_steps = len(train_dataloader) * EMA_CONFIG['warmup_epochs']\n",
    "\n",
    "ema_scheduler = get_cosine_schedule_with_warmup(\n",
    "    ema_optimizer, \n",
    "    warmup_steps, \n",
    "    total_steps\n",
    ")\n",
    "\n",
    "# Loss functions\n",
    "supervised_loss_fn = nn.CrossEntropyLoss(label_smoothing=EMA_CONFIG['label_smoothing'])\n",
    "\n",
    "# Consistency weight ramp-up function\n",
    "def get_consistency_weight(epoch, ramp_up_epochs):\n",
    "    \"\"\"Gradually ramp up consistency weight\"\"\"\n",
    "    if epoch < ramp_up_epochs:\n",
    "        return EMA_CONFIG['consistency_weight'] * (epoch / ramp_up_epochs)\n",
    "    return EMA_CONFIG['consistency_weight']\n",
    "\n",
    "# Pseudo-labeling function\n",
    "def get_pseudo_labels(teacher_logits, threshold):\n",
    "    \"\"\"Generate pseudo-labels from teacher predictions\"\"\"\n",
    "    teacher_probs = F.softmax(teacher_logits, dim=1)\n",
    "    max_probs, pseudo_labels = torch.max(teacher_probs, dim=1)\n",
    "    \n",
    "    # Create mask for confident predictions\n",
    "    confident_mask = max_probs >= threshold\n",
    "    \n",
    "    return pseudo_labels, confident_mask, max_probs\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, val_dataloader, device):\n",
    "    \"\"\"Validate model on labeled validation set\"\"\"\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    accuracy = 100.0 * total_correct / total_samples\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "    \n",
    "    return accuracy, avg_loss\n",
    "\n",
    "# Save checkpoint function\n",
    "def save_ema_checkpoint(student_model, teacher_model, optimizer, scheduler, epoch, \n",
    "                       best_acc, config, filename):\n",
    "    \"\"\"Save EMA training checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'student_state_dict': student_model.state_dict(),\n",
    "        'teacher_state_dict': teacher_model.teacher_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_accuracy': best_acc,\n",
    "        'config': config,\n",
    "        'class_to_idx': full_dataset.class_to_idx,\n",
    "        'num_classes': config['num_classes']\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"üíæ Saved checkpoint: {filename}\")\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "if EMA_CONFIG['use_wandb']:\n",
    "    print(\"üìà Initializing Weights & Biases...\")\n",
    "    try:\n",
    "        wandb.init(\n",
    "            project=EMA_CONFIG['wandb_project'],\n",
    "            name=EMA_CONFIG['wandb_run_name'],\n",
    "            config=EMA_CONFIG,\n",
    "            tags=['ema', 'semi-supervised', 'fish', 'mae-pretrained']\n",
    "        )\n",
    "        print(f\"‚úÖ W&B initialized: {wandb.run.url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  W&B initialization failed: {e}\")\n",
    "        EMA_CONFIG['use_wandb'] = False\n",
    "\n",
    "# Training loop\n",
    "print(f\"üé¨ Starting EMA training for {EMA_CONFIG['epochs']} epochs...\")\n",
    "print(f\"üìä Training data: {len(train_dataset)} samples\")\n",
    "print(f\"üìä Validation data: {len(val_dataset)} samples\")\n",
    "print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "vit_model.train()\n",
    "ema_teacher.student_model.train()\n",
    "ema_teacher.teacher_model.eval()\n",
    "\n",
    "best_accuracy = 0.0\n",
    "training_history = {\n",
    "    'supervised_loss': [],\n",
    "    'consistency_loss': [],\n",
    "    'total_loss': [],\n",
    "    'val_accuracy': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EMA_CONFIG['epochs']):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    vit_model.train()\n",
    "    epoch_supervised_loss = 0.0\n",
    "    epoch_consistency_loss = 0.0\n",
    "    epoch_total_loss = 0.0\n",
    "    num_labeled_samples = 0\n",
    "    num_unlabeled_samples = 0\n",
    "    \n",
    "    # Get current consistency weight\n",
    "    current_consistency_weight = get_consistency_weight(epoch, EMA_CONFIG['ramp_up_epochs'])\n",
    "    \n",
    "    pbar = tqdm(train_dataloader, desc=f\"EMA Epoch {epoch+1}/{EMA_CONFIG['epochs']}\")\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(pbar):\n",
    "        # Separate labeled and unlabeled data\n",
    "        labeled_data = []\n",
    "        unlabeled_data = []\n",
    "        \n",
    "        for data, label, is_labeled in zip(*batch_data):\n",
    "            if is_labeled:\n",
    "                labeled_data.append((data, label))\n",
    "            else:\n",
    "                unlabeled_data.append(data)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        supervised_loss = torch.tensor(0.0).to(DEVICE)\n",
    "        consistency_loss = torch.tensor(0.0).to(DEVICE)\n",
    "        \n",
    "        # Process labeled data\n",
    "        if labeled_data:\n",
    "            labeled_images = torch.stack([data for data, _ in labeled_data]).to(DEVICE)\n",
    "            labeled_targets = torch.tensor([label for _, label in labeled_data]).to(DEVICE)\n",
    "            \n",
    "            # Student forward pass\n",
    "            student_outputs = vit_model(labeled_images)\n",
    "            supervised_loss = supervised_loss_fn(student_outputs, labeled_targets)\n",
    "            \n",
    "            num_labeled_samples += len(labeled_data)\n",
    "        \n",
    "        # Process unlabeled data\n",
    "        if unlabeled_data and current_consistency_weight > 0:\n",
    "            # Unlabeled data comes as (weak_aug, strong_aug) tuples\n",
    "            weak_images = torch.stack([data[0] for data in unlabeled_data]).to(DEVICE)\n",
    "            strong_images = torch.stack([data[1] for data in unlabeled_data]).to(DEVICE)\n",
    "            \n",
    "            # Teacher predictions on weakly augmented images\n",
    "            teacher_outputs = ema_teacher.get_teacher_predictions(weak_images)\n",
    "            \n",
    "            # Generate pseudo-labels\n",
    "            pseudo_labels, confident_mask, max_probs = get_pseudo_labels(\n",
    "                teacher_outputs, EMA_CONFIG['pseudo_label_threshold']\n",
    "            )\n",
    "            \n",
    "            if confident_mask.sum() > 0:\n",
    "                # Student predictions on strongly augmented images\n",
    "                student_outputs_unlabeled = vit_model(strong_images)\n",
    "                \n",
    "                # Consistency loss only for confident predictions\n",
    "                if confident_mask.sum() > 0:\n",
    "                    student_confident = student_outputs_unlabeled[confident_mask]\n",
    "                    teacher_confident = teacher_outputs[confident_mask]\n",
    "                    \n",
    "                    consistency_loss = consistency_loss_fn(student_confident, teacher_confident)\n",
    "                    consistency_loss *= current_consistency_weight\n",
    "            \n",
    "            num_unlabeled_samples += len(unlabeled_data)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = supervised_loss + consistency_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        ema_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if EMA_CONFIG['clip_grad'] > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(vit_model.parameters(), EMA_CONFIG['clip_grad'])\n",
    "        \n",
    "        ema_optimizer.step()\n",
    "        ema_scheduler.step()\n",
    "        \n",
    "        # Update teacher with EMA\n",
    "        ema_teacher.update_teacher()\n",
    "        \n",
    "        # Record losses\n",
    "        epoch_supervised_loss += supervised_loss.item()\n",
    "        epoch_consistency_loss += consistency_loss.item()\n",
    "        epoch_total_loss += total_loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'Sup_Loss': f\"{supervised_loss.item():.4f}\",\n",
    "            'Con_Loss': f\"{consistency_loss.item():.4f}\",\n",
    "            'Con_Weight': f\"{current_consistency_weight:.3f}\",\n",
    "            'LR': f\"{ema_optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        })\n",
    "        \n",
    "        # Log to wandb\n",
    "        if EMA_CONFIG['use_wandb'] and wandb.run:\n",
    "            try:\n",
    "                wandb.log({\n",
    "                    'batch_supervised_loss': supervised_loss.item(),\n",
    "                    'batch_consistency_loss': consistency_loss.item(),\n",
    "                    'batch_total_loss': total_loss.item(),\n",
    "                    'consistency_weight': current_consistency_weight,\n",
    "                    'learning_rate': ema_optimizer.param_groups[0]['lr'],\n",
    "                    'step': epoch * len(train_dataloader) + batch_idx\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Validation phase\n",
    "    val_accuracy, val_loss = validate_model(vit_model, val_dataloader, DEVICE)\n",
    "    \n",
    "    # Epoch statistics\n",
    "    avg_supervised_loss = epoch_supervised_loss / len(train_dataloader)\n",
    "    avg_consistency_loss = epoch_consistency_loss / len(train_dataloader)\n",
    "    avg_total_loss = epoch_total_loss / len(train_dataloader)\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    training_history['supervised_loss'].append(avg_supervised_loss)\n",
    "    training_history['consistency_loss'].append(avg_consistency_loss)\n",
    "    training_history['total_loss'].append(avg_total_loss)\n",
    "    training_history['val_accuracy'].append(val_accuracy)\n",
    "    training_history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f\"\\nüìä Epoch {epoch+1} Summary:\")\n",
    "    print(f\"  üìâ Supervised Loss: {avg_supervised_loss:.4f}\")\n",
    "    print(f\"  üìâ Consistency Loss: {avg_consistency_loss:.4f}\")\n",
    "    print(f\"  üìâ Total Loss: {avg_total_loss:.4f}\")\n",
    "    print(f\"  üìà Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "    print(f\"  üìâ Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"  ‚è±Ô∏è  Time: {epoch_time:.1f}s\")\n",
    "    print(f\"  üéì Labeled samples: {num_labeled_samples}\")\n",
    "    print(f\"  üîÑ Unlabeled samples: {num_unlabeled_samples}\")\n",
    "    \n",
    "    # Save checkpoint periodically\n",
    "    if (epoch + 1) % EMA_CONFIG['save_frequency'] == 0:\n",
    "        checkpoint_path = os.path.join(\n",
    "            EMA_CONFIG['checkpoint_dir'], \n",
    "            f'ema_checkpoint_epoch_{epoch+1}.pth'\n",
    "        )\n",
    "        save_ema_checkpoint(\n",
    "            vit_model, ema_teacher, ema_optimizer, ema_scheduler,\n",
    "            epoch + 1, val_accuracy, EMA_CONFIG, checkpoint_path\n",
    "        )\n",
    "    \n",
    "    # Save best model\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_checkpoint_path = os.path.join(\n",
    "            EMA_CONFIG['checkpoint_dir'], \n",
    "            'ema_best_model.pth'\n",
    "        )\n",
    "        save_ema_checkpoint(\n",
    "            vit_model, ema_teacher, ema_optimizer, ema_scheduler,\n",
    "            epoch + 1, val_accuracy, EMA_CONFIG, best_checkpoint_path\n",
    "        )\n",
    "        print(f\"üèÜ New best model saved! Accuracy: {best_accuracy:.2f}%\")\n",
    "    \n",
    "    # Log epoch results to wandb\n",
    "    if EMA_CONFIG['use_wandb'] and wandb.run:\n",
    "        try:\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'epoch_supervised_loss': avg_supervised_loss,\n",
    "                'epoch_consistency_loss': avg_consistency_loss,\n",
    "                'epoch_total_loss': avg_total_loss,\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'val_loss': val_loss,\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'epoch_time': epoch_time\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Memory cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Training completed\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nüéâ EMA TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è∞ Total training time: {total_time/3600:.2f} hours\")\n",
    "print(f\"üèÜ Best validation accuracy: {best_accuracy:.2f}%\")\n",
    "print(f\"üìà Final learning rate: {ema_optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "# Save final checkpoint\n",
    "final_checkpoint_path = os.path.join(\n",
    "    EMA_CONFIG['checkpoint_dir'], \n",
    "    'ema_final_model.pth'\n",
    ")\n",
    "save_ema_checkpoint(\n",
    "    vit_model, ema_teacher, ema_optimizer, ema_scheduler,\n",
    "    EMA_CONFIG['epochs'], training_history['val_accuracy'][-1], \n",
    "    EMA_CONFIG, final_checkpoint_path\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss curves\n",
    "epochs = range(1, len(training_history['total_loss']) + 1)\n",
    "ax1.plot(epochs, training_history['supervised_loss'], 'b-', label='Supervised Loss', linewidth=2)\n",
    "ax1.plot(epochs, training_history['consistency_loss'], 'r-', label='Consistency Loss', linewidth=2)\n",
    "ax1.plot(epochs, training_history['total_loss'], 'g-', label='Total Loss', linewidth=2)\n",
    "ax1.set_title('Training Loss Curves', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy\n",
    "ax2.plot(epochs, training_history['val_accuracy'], 'purple', linewidth=2)\n",
    "ax2.set_title('Validation Accuracy', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "ax3.plot(epochs, training_history['val_loss'], 'orange', linewidth=2)\n",
    "ax3.set_title('Validation Loss', fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss ratio\n",
    "consistency_ratio = [c/(s+1e-8) for s, c in zip(training_history['supervised_loss'], \n",
    "                                                training_history['consistency_loss'])]\n",
    "ax4.plot(epochs, consistency_ratio, 'brown', linewidth=2)\n",
    "ax4.set_title('Consistency/Supervised Loss Ratio', fontweight='bold')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Ratio')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(EMA_CONFIG['checkpoint_dir'], 'ema_training_curves.png'), \n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final wandb logging\n",
    "if EMA_CONFIG['use_wandb'] and wandb.run:\n",
    "    try:\n",
    "        wandb.log({\n",
    "            'final_best_accuracy': best_accuracy,\n",
    "            'final_val_accuracy': training_history['val_accuracy'][-1],\n",
    "            'total_training_time_hours': total_time/3600,\n",
    "            'total_epochs': EMA_CONFIG['epochs']\n",
    "        })\n",
    "        wandb.finish()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\n‚úÖ Semi-supervised EMA training with MAE pretraining completed!\")\n",
    "print(f\"üíæ Checkpoints saved to: {EMA_CONFIG['checkpoint_dir']}\")\n",
    "print(f\"üéØ Best model achieved {best_accuracy:.2f}% accuracy!\")\n",
    "print(f\"üöÄ Model ready for evaluation and deployment!\")\n",
    "\n",
    "# Store paths for evaluation\n",
    "EMA_BEST_MODEL_PATH = best_checkpoint_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3789d94",
   "metadata": {},
   "source": [
    "## üìä Section 10: Monitor Training Progress and Save Checkpoints\n",
    "\n",
    "Tracking training metrics, analyzing model performance, and managing checkpoint saves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111848cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor Training Progress and Manage Checkpoints\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "print(\"üìä TRAINING PROGRESS MONITORING & CHECKPOINT MANAGEMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def analyze_training_checkpoints(checkpoint_dir):\n",
    "    \"\"\"Analyze all training checkpoints and extract metrics\"\"\"\n",
    "    \n",
    "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'ema_checkpoint_epoch_*.pth'))\n",
    "    checkpoint_files.sort(key=lambda x: int(x.split('epoch_')[1].split('.')[0]))\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        print(\"‚ùå No checkpoint files found\")\n",
    "        return None\n",
    "    \n",
    "    checkpoint_data = []\n",
    "    \n",
    "    print(f\"üìÇ Found {len(checkpoint_files)} checkpoint files\")\n",
    "    print(\"\\nüìä Checkpoint Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Epoch':<8} {'Accuracy':<12} {'File Size':<12} {'Timestamp'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for checkpoint_file in checkpoint_files:\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_file, map_location='cpu')\n",
    "            \n",
    "            epoch = checkpoint.get('epoch', 0)\n",
    "            accuracy = checkpoint.get('best_accuracy', 0)\n",
    "            file_size = os.path.getsize(checkpoint_file) / (1024 * 1024)  # MB\n",
    "            timestamp = datetime.fromtimestamp(os.path.getmtime(checkpoint_file))\n",
    "            \n",
    "            checkpoint_data.append({\n",
    "                'epoch': epoch,\n",
    "                'accuracy': accuracy,\n",
    "                'file_size': file_size,\n",
    "                'timestamp': timestamp,\n",
    "                'file_path': checkpoint_file\n",
    "            })\n",
    "            \n",
    "            print(f\"{epoch:<8} {accuracy:<12.2f} {file_size:<12.1f} {timestamp.strftime('%H:%M:%S')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not load {checkpoint_file}: {e}\")\n",
    "    \n",
    "    return checkpoint_data\n",
    "\n",
    "def visualize_training_progress(checkpoint_dir, training_history=None):\n",
    "    \"\"\"Create comprehensive training progress visualizations\"\"\"\n",
    "    \n",
    "    if training_history is None:\n",
    "        print(\"‚ö†Ô∏è  Training history not available, using checkpoint data only\")\n",
    "        return\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # 1. Loss progression\n",
    "    ax1 = plt.subplot(2, 4, 1)\n",
    "    epochs = range(1, len(training_history['total_loss']) + 1)\n",
    "    ax1.plot(epochs, training_history['supervised_loss'], 'b-', label='Supervised', linewidth=2)\n",
    "    ax1.plot(epochs, training_history['consistency_loss'], 'r-', label='Consistency', linewidth=2)\n",
    "    ax1.plot(epochs, training_history['total_loss'], 'g-', label='Total', linewidth=2)\n",
    "    ax1.set_title('Training Loss Progression', fontweight='bold', fontsize=12)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Validation accuracy\n",
    "    ax2 = plt.subplot(2, 4, 2)\n",
    "    ax2.plot(epochs, training_history['val_accuracy'], 'purple', linewidth=3)\n",
    "    ax2.set_title('Validation Accuracy', fontweight='bold', fontsize=12)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add best accuracy line\n",
    "    best_acc = max(training_history['val_accuracy'])\n",
    "    ax2.axhline(y=best_acc, color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'Best: {best_acc:.2f}%')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Loss smoothed (moving average)\n",
    "    ax3 = plt.subplot(2, 4, 3)\n",
    "    window = min(10, len(epochs) // 4)\n",
    "    if window > 1:\n",
    "        smoothed_total = np.convolve(training_history['total_loss'], \n",
    "                                   np.ones(window)/window, mode='valid')\n",
    "        smoothed_epochs = epochs[window-1:]\n",
    "        ax3.plot(smoothed_epochs, smoothed_total, 'darkgreen', linewidth=2)\n",
    "    ax3.set_title(f'Smoothed Total Loss (window={window})', fontweight='bold', fontsize=12)\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Learning dynamics\n",
    "    ax4 = plt.subplot(2, 4, 4)\n",
    "    consistency_ratio = [c/(s+1e-8) for s, c in zip(training_history['supervised_loss'], \n",
    "                                                    training_history['consistency_loss'])]\n",
    "    ax4.plot(epochs, consistency_ratio, 'brown', linewidth=2)\n",
    "    ax4.set_title('Consistency/Supervised Ratio', fontweight='bold', fontsize=12)\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Ratio')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Validation loss vs accuracy\n",
    "    ax5 = plt.subplot(2, 4, 5)\n",
    "    scatter = ax5.scatter(training_history['val_loss'], training_history['val_accuracy'], \n",
    "                         c=epochs, cmap='viridis', alpha=0.7)\n",
    "    ax5.set_title('Validation Loss vs Accuracy', fontweight='bold', fontsize=12)\n",
    "    ax5.set_xlabel('Validation Loss')\n",
    "    ax5.set_ylabel('Validation Accuracy (%)')\n",
    "    plt.colorbar(scatter, ax=ax5, label='Epoch')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Training efficiency\n",
    "    ax6 = plt.subplot(2, 4, 6)\n",
    "    total_loss_diff = np.diff(training_history['total_loss'])\n",
    "    ax6.plot(epochs[1:], total_loss_diff, 'orange', linewidth=2)\n",
    "    ax6.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax6.set_title('Loss Change Rate', fontweight='bold', fontsize=12)\n",
    "    ax6.set_xlabel('Epoch')\n",
    "    ax6.set_ylabel('Loss Œî')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Accuracy improvement\n",
    "    ax7 = plt.subplot(2, 4, 7)\n",
    "    acc_diff = np.diff(training_history['val_accuracy'])\n",
    "    ax7.plot(epochs[1:], acc_diff, 'darkblue', linewidth=2)\n",
    "    ax7.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax7.set_title('Accuracy Change Rate', fontweight='bold', fontsize=12)\n",
    "    ax7.set_xlabel('Epoch')\n",
    "    ax7.set_ylabel('Accuracy Œî (%)')\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 8. Training summary stats\n",
    "    ax8 = plt.subplot(2, 4, 8)\n",
    "    ax8.axis('off')\n",
    "    \n",
    "    # Calculate statistics\n",
    "    final_acc = training_history['val_accuracy'][-1]\n",
    "    best_acc = max(training_history['val_accuracy'])\n",
    "    best_epoch = training_history['val_accuracy'].index(best_acc) + 1\n",
    "    acc_improvement = final_acc - training_history['val_accuracy'][0]\n",
    "    \n",
    "    summary_text = f\\\"\\\"\\\"Training Summary\n",
    "    \n",
    "üìä Final Accuracy: {final_acc:.2f}%\n",
    "üèÜ Best Accuracy: {best_acc:.2f}%\n",
    "üéØ Best Epoch: {best_epoch}\n",
    "üìà Total Improvement: {acc_improvement:.2f}%\n",
    "üî• Epochs Trained: {len(epochs)}\n",
    "\n",
    "üí° Loss Components:\n",
    "üìâ Final Supervised: {training_history['supervised_loss'][-1]:.4f}\n",
    "üîÑ Final Consistency: {training_history['consistency_loss'][-1]:.4f}\n",
    "‚öñÔ∏è Final Total: {training_history['total_loss'][-1]:.4f}\n",
    "    \\\"\\\"\\\"\n",
    "    \n",
    "    ax8.text(0.1, 0.9, summary_text, transform=ax8.transAxes, fontsize=10,\n",
    "            verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the visualization\n",
    "    viz_path = os.path.join(checkpoint_dir, 'training_analysis.png')\n",
    "    plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìä Training analysis saved to: {viz_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def backup_best_models(checkpoint_dir, backup_dir=None):\n",
    "    \"\"\"Backup the best models to Google Drive\"\"\"\n",
    "    \n",
    "    if backup_dir is None:\n",
    "        backup_dir = '/content/drive/MyDrive/ViT-FishID_MAE_EMA_Backup'\n",
    "    \n",
    "    os.makedirs(backup_dir, exist_ok=True)\n",
    "    \n",
    "    # Find best model files\n",
    "    best_files = ['ema_best_model.pth', 'ema_final_model.pth']\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    for filename in best_files:\n",
    "        source_path = os.path.join(checkpoint_dir, filename)\n",
    "        if os.path.exists(source_path):\n",
    "            # Create timestamped backup\n",
    "            backup_filename = f\\\"{timestamp}_{filename}\\\"\n",
    "            backup_path = os.path.join(backup_dir, backup_filename)\\n            \\n            try:\\n                shutil.copy2(source_path, backup_path)\\n                file_size = os.path.getsize(backup_path) / (1024 * 1024)  # MB\\n                print(f\\\"‚úÖ Backed up {filename} -> {backup_filename} ({file_size:.1f} MB)\\\")\\n            except Exception as e:\\n                print(f\\\"‚ùå Failed to backup {filename}: {e}\\\")\\n        else:\\n            print(f\\\"‚ö†Ô∏è  {filename} not found in {checkpoint_dir}\\\")\\n    \\n    print(f\\\"üíæ Backups saved to: {backup_dir}\\\")\\n\\n# Run analysis if checkpoints exist\\nif 'EMA_CONFIG' in globals() and os.path.exists(EMA_CONFIG['checkpoint_dir']):\\n    print(f\\\"üìÅ Analyzing checkpoints in: {EMA_CONFIG['checkpoint_dir']}\\\")\\n    \\n    # Analyze checkpoints\\n    checkpoint_data = analyze_training_checkpoints(EMA_CONFIG['checkpoint_dir'])\\n    \\n    # Visualize training progress\\n    if 'training_history' in globals():\\n        print(\\\"\\\\nüìä Creating training progress visualization...\\\")\\n        fig = visualize_training_progress(EMA_CONFIG['checkpoint_dir'], training_history)\\n    \\n    # Backup best models\\n    print(\\\"\\\\nüíæ Backing up best models to Google Drive...\\\")\\n    backup_best_models(EMA_CONFIG['checkpoint_dir'])\\n    \\n    # Model size analysis\\n    print(\\\"\\\\nüìä MODEL SIZE ANALYSIS:\\\")\\n    print(\\\"=\\\"*40)\\n    \\n    if checkpoint_data:\\n        latest_checkpoint = max(checkpoint_data, key=lambda x: x['epoch'])\\n        print(f\\\"Latest checkpoint size: {latest_checkpoint['file_size']:.1f} MB\\\")\\n        \\n        total_size = sum(cp['file_size'] for cp in checkpoint_data)\\n        print(f\\\"Total checkpoint storage: {total_size:.1f} MB\\\")\\n        \\n        avg_size = total_size / len(checkpoint_data)\\n        print(f\\\"Average checkpoint size: {avg_size:.1f} MB\\\")\\n    \\n    # Training efficiency metrics\\n    if 'training_history' in globals() and 'total_time' in globals():\\n        print(\\\"\\\\n‚ö° TRAINING EFFICIENCY:\\\")\\n        print(\\\"=\\\"*40)\\n        \\n        total_epochs = len(training_history['val_accuracy'])\\n        time_per_epoch = total_time / total_epochs\\n        \\n        print(f\\\"Total training time: {total_time/3600:.2f} hours\\\")\\n        print(f\\\"Time per epoch: {time_per_epoch:.1f} seconds\\\")\\n        print(f\\\"Final accuracy: {training_history['val_accuracy'][-1]:.2f}%\\\")\\n        print(f\\\"Best accuracy: {max(training_history['val_accuracy']):.2f}%\\\")\\n        \\n        # Accuracy per hour\\n        acc_per_hour = max(training_history['val_accuracy']) / (total_time / 3600)\\n        print(f\\\"Accuracy gained per hour: {acc_per_hour:.2f}%/hr\\\")\\n    \\n    # Disk usage summary\\n    print(\\\"\\\\nüíæ STORAGE SUMMARY:\\\")\\n    print(\\\"=\\\"*40)\\n    \\n    checkpoint_size = sum(os.path.getsize(os.path.join(EMA_CONFIG['checkpoint_dir'], f)) \\n                         for f in os.listdir(EMA_CONFIG['checkpoint_dir']) \\n                         if f.endswith('.pth')) / (1024 * 1024)  # MB\\n    \\n    mae_checkpoint_size = 0\\n    if 'MAE_CONFIG' in globals() and os.path.exists(MAE_CONFIG['checkpoint_dir']):\\n        mae_checkpoint_size = sum(os.path.getsize(os.path.join(MAE_CONFIG['checkpoint_dir'], f)) \\n                                 for f in os.listdir(MAE_CONFIG['checkpoint_dir']) \\n                                 if f.endswith('.pth')) / (1024 * 1024)  # MB\\n    \\n    total_storage = checkpoint_size + mae_checkpoint_size\\n    \\n    print(f\\\"EMA checkpoints: {checkpoint_size:.1f} MB\\\")\\n    print(f\\\"MAE checkpoints: {mae_checkpoint_size:.1f} MB\\\")\\n    print(f\\\"Total storage used: {total_storage:.1f} MB\\\")\\n    \\nelse:\\n    print(\\\"‚ö†Ô∏è  No checkpoint directory found. Training may not have completed.\\\")\\n\\nprint(f\\\"\\\\n‚úÖ Training monitoring and checkpoint management complete!\\\")\\nprint(f\\\"üìä All analysis saved to checkpoint directories\\\")\\nprint(f\\\"üíæ Best models backed up to Google Drive\\\")\\nprint(f\\\"üéØ Ready for model evaluation and deployment!\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480fab9",
   "metadata": {},
   "source": [
    "## üß™ Section 11: Evaluate Final Model Performance\n",
    "\n",
    "Comprehensive evaluation of the trained model with performance comparisons and deployment preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Evaluation and Performance Analysis\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "\n",
    "print(\"üß™ COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def load_best_model(checkpoint_path, device):\n",
    "    \"\"\"Load the best trained model\"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"‚ùå Checkpoint not found: {checkpoint_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    print(f\"üìÇ Loading checkpoint from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"üèÜ Best accuracy: {checkpoint['best_accuracy']:.2f}%\")\n",
    "    \n",
    "    # Recreate model architecture\n",
    "    model = MAEPretrainedViT(\n",
    "        num_classes=checkpoint['num_classes'],\n",
    "        mae_encoder=None,  # Will load weights directly\n",
    "        dropout_rate=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load state dict\n",
    "    model.load_state_dict(checkpoint['student_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    class_to_idx = checkpoint.get('class_to_idx', {})\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    \n",
    "    return model, idx_to_class\n",
    "\n",
    "def create_test_dataset(labeled_dir, class_to_idx, img_size=224, test_split=0.2):\n",
    "    \\\"\\\"\\\"Create a test dataset from labeled data\\\"\\\"\\\"\\n    \\n    test_data = []\\n    \\n    for species, class_idx in class_to_idx.items():\\n        species_path = os.path.join(labeled_dir, species)\\n        if not os.path.exists(species_path):\\n            continue\\n            \\n        # Get all images for this species\\n        images = []\\n        for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\\n            images.extend(glob.glob(os.path.join(species_path, ext)))\\n        \\n        # Take last 20% as test set (assuming first 80% used for training)\\n        test_count = max(1, int(len(images) * test_split))\\n        test_images = images[-test_count:]\\n        \\n        for img_path in test_images:\\n            test_data.append((img_path, class_idx))\\n    \\n    print(f\\\"üìä Created test set with {len(test_data)} images\\\")\\n    return test_data\\n\\ndef evaluate_model_comprehensive(model, test_data, idx_to_class, device, img_size=224):\\n    \\\"\\\"\\\"Comprehensive model evaluation\\\"\\\"\\\"\\n    \\n    model.eval()\\n    \\n    # Prepare data transforms\\n    test_transform = transforms.Compose([\\n        transforms.Resize((img_size, img_size)),\\n        transforms.ToTensor(),\\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n    ])\\n    \\n    all_predictions = []\\n    all_labels = []\\n    all_probabilities = []\\n    prediction_details = []\\n    \\n    print(f\\\"üß™ Evaluating on {len(test_data)} test images...\\\")\\n    \\n    with torch.no_grad():\\n        for i, (img_path, true_label) in enumerate(tqdm(test_data, desc=\\\"Evaluating\\\")):\\n            try:\\n                # Load and preprocess image\\n                image = Image.open(img_path).convert('RGB')\\n                image_tensor = test_transform(image).unsqueeze(0).to(device)\\n                \\n                # Get model prediction\\n                outputs = model(image_tensor)\\n                probabilities = F.softmax(outputs, dim=1)\\n                predicted_class = torch.argmax(outputs, dim=1).item()\\n                confidence = probabilities[0, predicted_class].item()\\n                \\n                all_predictions.append(predicted_class)\\n                all_labels.append(true_label)\\n                all_probabilities.append(probabilities.cpu().numpy()[0])\\n                \\n                prediction_details.append({\\n                    'image_path': img_path,\\n                    'true_label': true_label,\\n                    'true_species': idx_to_class[true_label],\\n                    'predicted_label': predicted_class,\\n                    'predicted_species': idx_to_class[predicted_class],\\n                    'confidence': confidence,\\n                    'correct': predicted_class == true_label\\n                })\\n                \\n            except Exception as e:\\n                print(f\\\"‚ö†Ô∏è  Error processing {img_path}: {e}\\\")\\n                continue\\n    \\n    return all_predictions, all_labels, all_probabilities, prediction_details\\n\\ndef analyze_results(predictions, labels, probabilities, prediction_details, idx_to_class):\\n    \\\"\\\"\\\"Analyze evaluation results\\\"\\\"\\\"\\n    \\n    # Basic metrics\\n    accuracy = np.mean(np.array(predictions) == np.array(labels)) * 100\\n    \\n    # Top-k accuracy\\n    top3_acc = top_k_accuracy_score(labels, probabilities, k=3) * 100\\n    top5_acc = top_k_accuracy_score(labels, probabilities, k=5) * 100\\n    \\n    print(f\\\"\\\\nüìä EVALUATION RESULTS\\\")\\n    print(\\\"=\\\"*50)\\n    print(f\\\"üéØ Top-1 Accuracy: {accuracy:.2f}%\\\")\\n    print(f\\\"üéØ Top-3 Accuracy: {top3_acc:.2f}%\\\")\\n    print(f\\\"üéØ Top-5 Accuracy: {top5_acc:.2f}%\\\")\\n    \\n    # Per-class metrics\\n    precision, recall, f1, support = precision_recall_fscore_support(\\n        labels, predictions, average=None, zero_division=0\\n    )\\n    \\n    # Create detailed classification report\\n    class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\\n    report = classification_report(\\n        labels, predictions, \\n        target_names=class_names, \\n        output_dict=True,\\n        zero_division=0\\n    )\\n    \\n    # Convert to DataFrame for better visualization\\n    report_df = pd.DataFrame(report).transpose()\\n    \\n    print(f\\\"\\\\nüìä PER-CLASS PERFORMANCE (Top 10 by F1-Score):\\\")\\n    print(\\\"-\\\"*70)\\n    \\n    # Sort by F1-score and show top 10\\n    class_metrics = report_df.iloc[:-3].sort_values('f1-score', ascending=False)\\n    top_classes = class_metrics.head(10)\\n    \\n    for idx, (species, metrics) in enumerate(top_classes.iterrows()):\\n        print(f\\\"{idx+1:2d}. {species[:25]:<25} Precision: {metrics['precision']:.3f} \\\"\\n              f\\\"Recall: {metrics['recall']:.3f} F1: {metrics['f1-score']:.3f}\\\")\\n    \\n    # Confidence analysis\\n    confidences = [detail['confidence'] for detail in prediction_details]\\n    correct_confidences = [detail['confidence'] for detail in prediction_details if detail['correct']]\\n    incorrect_confidences = [detail['confidence'] for detail in prediction_details if not detail['correct']]\\n    \\n    print(f\\\"\\\\nüìä CONFIDENCE ANALYSIS:\\\")\\n    print(\\\"-\\\"*40)\\n    print(f\\\"Average confidence (all): {np.mean(confidences):.3f}\\\")\\n    print(f\\\"Average confidence (correct): {np.mean(correct_confidences):.3f}\\\")\\n    print(f\\\"Average confidence (incorrect): {np.mean(incorrect_confidences):.3f}\\\")\\n    \\n    return {\\n        'accuracy': accuracy,\\n        'top3_accuracy': top3_acc,\\n        'top5_accuracy': top5_acc,\\n        'report_df': report_df,\\n        'class_metrics': class_metrics,\\n        'confidences': confidences,\\n        'correct_confidences': correct_confidences,\\n        'incorrect_confidences': incorrect_confidences\\n    }\\n\\ndef visualize_evaluation_results(results, predictions, labels, idx_to_class, save_dir):\\n    \\\"\\\"\\\"Create comprehensive evaluation visualizations\\\"\\\"\\\"\\n    \\n    # Create figure with multiple subplots\\n    fig = plt.figure(figsize=(20, 15))\\n    \\n    # 1. Confusion Matrix\\n    ax1 = plt.subplot(3, 3, 1)\\n    cm = confusion_matrix(labels, predictions)\\n    \\n    # Normalize confusion matrix\\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\\n    \\n    # Only show top 15 classes for readability\\n    top_classes_idx = results['class_metrics'].head(15).index\\n    top_class_indices = [i for i, name in enumerate(idx_to_class.values()) if name in top_classes_idx]\\n    \\n    if len(top_class_indices) > 1:\\n        cm_subset = cm_normalized[np.ix_(top_class_indices, top_class_indices)]\\n        class_names_subset = [list(idx_to_class.values())[i][:10] for i in top_class_indices]\\n        \\n        sns.heatmap(cm_subset, annot=True, fmt='.2f', cmap='Blues', \\n                   xticklabels=class_names_subset, yticklabels=class_names_subset,\\n                   ax=ax1)\\n        ax1.set_title('Confusion Matrix (Top 15 Classes)', fontweight='bold')\\n        ax1.set_xlabel('Predicted')\\n        ax1.set_ylabel('Actual')\\n    \\n    # 2. Accuracy by class\\n    ax2 = plt.subplot(3, 3, 2)\\n    top_f1_classes = results['class_metrics'].head(15)\\n    ax2.barh(range(len(top_f1_classes)), top_f1_classes['f1-score'])\\n    ax2.set_yticks(range(len(top_f1_classes)))\\n    ax2.set_yticklabels([name[:15] for name in top_f1_classes.index])\\n    ax2.set_xlabel('F1-Score')\\n    ax2.set_title('F1-Score by Species (Top 15)', fontweight='bold')\\n    ax2.grid(True, alpha=0.3)\\n    \\n    # 3. Confidence distribution\\n    ax3 = plt.subplot(3, 3, 3)\\n    ax3.hist(results['correct_confidences'], bins=30, alpha=0.7, label='Correct', color='green')\\n    ax3.hist(results['incorrect_confidences'], bins=30, alpha=0.7, label='Incorrect', color='red')\\n    ax3.set_xlabel('Confidence')\\n    ax3.set_ylabel('Frequency')\\n    ax3.set_title('Confidence Distribution', fontweight='bold')\\n    ax3.legend()\\n    ax3.grid(True, alpha=0.3)\\n    \\n    # 4. Top-k accuracy\\n    ax4 = plt.subplot(3, 3, 4)\\n    k_values = [1, 3, 5]\\n    k_accuracies = [results['accuracy'], results['top3_accuracy'], results['top5_accuracy']]\\n    ax4.bar(k_values, k_accuracies, color=['blue', 'orange', 'green'])\\n    ax4.set_xlabel('K (Top-K)')\\n    ax4.set_ylabel('Accuracy (%)')\\n    ax4.set_title('Top-K Accuracy', fontweight='bold')\\n    ax4.grid(True, alpha=0.3)\\n    \\n    # 5. Precision vs Recall scatter\\n    ax5 = plt.subplot(3, 3, 5)\\n    class_metrics = results['class_metrics'].iloc[:-3]  # Exclude summary rows\\n    scatter = ax5.scatter(class_metrics['recall'], class_metrics['precision'], \\n                         c=class_metrics['f1-score'], cmap='viridis', alpha=0.7)\\n    ax5.set_xlabel('Recall')\\n    ax5.set_ylabel('Precision')\\n    ax5.set_title('Precision vs Recall by Species', fontweight='bold')\\n    plt.colorbar(scatter, ax=ax5, label='F1-Score')\\n    ax5.grid(True, alpha=0.3)\\n    \\n    # 6. Support distribution\\n    ax6 = plt.subplot(3, 3, 6)\\n    support_counts = results['class_metrics']['support'].iloc[:-3]\\n    ax6.hist(support_counts, bins=20, color='purple', alpha=0.7)\\n    ax6.set_xlabel('Number of Test Samples')\\n    ax6.set_ylabel('Number of Species')\\n    ax6.set_title('Test Sample Distribution', fontweight='bold')\\n    ax6.grid(True, alpha=0.3)\\n    \\n    # 7. Performance summary text\\n    ax7 = plt.subplot(3, 3, 7)\\n    ax7.axis('off')\\n    \\n    summary_text = f\\\"\\\"\\\"Model Performance Summary\\n    \\nüéØ Overall Accuracy: {results['accuracy']:.2f}%\\nüéØ Top-3 Accuracy: {results['top3_accuracy']:.2f}%\\nüéØ Top-5 Accuracy: {results['top5_accuracy']:.2f}%\\n\\nüìä Best Performing Species:\\n{results['class_metrics'].head(3).index.tolist()[0][:20]}...\\n{results['class_metrics'].head(3).index.tolist()[1][:20]}...\\n{results['class_metrics'].head(3).index.tolist()[2][:20]}...\\n\\nüí° Average Confidence:\\n‚úÖ Correct: {np.mean(results['correct_confidences']):.3f}\\n‚ùå Incorrect: {np.mean(results['incorrect_confidences']):.3f}\\n\\nüìà Model Quality:\\n{\\\"Excellent\\\" if results['accuracy'] > 90 else \\\"Good\\\" if results['accuracy'] > 80 else \\\"Fair\\\" if results['accuracy'] > 70 else \\\"Needs Improvement\\\"}\\n    \\\"\\\"\\\"\\n    \\n    ax7.text(0.1, 0.9, summary_text, transform=ax7.transAxes, fontsize=10,\\n            verticalalignment='top', fontfamily='monospace',\\n            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\\n    \\n    plt.tight_layout()\\n    \\n    # Save visualization\\n    viz_path = os.path.join(save_dir, 'evaluation_results.png')\\n    plt.savefig(viz_path, dpi=300, bbox_inches='tight')\\n    print(f\\\"üìä Evaluation visualization saved to: {viz_path}\\\")\\n    \\n    plt.show()\\n    \\n    return fig\\n\\n# Run comprehensive evaluation\\nif 'EMA_BEST_MODEL_PATH' in globals() and os.path.exists(EMA_BEST_MODEL_PATH):\\n    print(f\\\"üß™ Loading best EMA model: {EMA_BEST_MODEL_PATH}\\\")\\n    \\n    # Load the best model\\n    best_model, idx_to_class = load_best_model(EMA_BEST_MODEL_PATH, DEVICE)\\n    \\n    if best_model is not None:\\n        # Create test dataset\\n        test_data = create_test_dataset(LABELED_DIR, \\n                                       {v: k for k, v in idx_to_class.items()},\\n                                       img_size=224)\\n        \\n        if test_data:\\n            # Run comprehensive evaluation\\n            predictions, labels, probabilities, prediction_details = evaluate_model_comprehensive(\\n                best_model, test_data, idx_to_class, DEVICE\\n            )\\n            \\n            # Analyze results\\n            results = analyze_results(predictions, labels, probabilities, \\n                                    prediction_details, idx_to_class)\\n            \\n            # Create visualizations\\n            if 'EMA_CONFIG' in globals():\\n                fig = visualize_evaluation_results(results, predictions, labels, \\n                                                  idx_to_class, EMA_CONFIG['checkpoint_dir'])\\n            \\n            # Save detailed results\\n            if 'EMA_CONFIG' in globals():\\n                results_path = os.path.join(EMA_CONFIG['checkpoint_dir'], 'evaluation_results.json')\\n                \\n                # Prepare results for JSON serialization\\n                json_results = {\\n                    'overall_accuracy': float(results['accuracy']),\\n                    'top3_accuracy': float(results['top3_accuracy']),\\n                    'top5_accuracy': float(results['top5_accuracy']),\\n                    'average_confidence_correct': float(np.mean(results['correct_confidences'])),\\n                    'average_confidence_incorrect': float(np.mean(results['incorrect_confidences'])),\\n                    'total_test_samples': len(test_data),\\n                    'num_classes': len(idx_to_class)\\n                }\\n                \\n                import json\\n                with open(results_path, 'w') as f:\\n                    json.dump(json_results, f, indent=2)\\n                \\n                print(f\\\"üìä Detailed results saved to: {results_path}\\\")\\n            \\n            print(f\\\"\\\\nüéâ EVALUATION COMPLETED SUCCESSFULLY!\\\")\\n            print(f\\\"üèÜ Final model achieved {results['accuracy']:.2f}% accuracy\\\")\\n            print(f\\\"üöÄ Model ready for deployment!\\\")\\n        \\n        else:\\n            print(\\\"‚ùå No test data available for evaluation\\\")\\n    \\n    else:\\n        print(\\\"‚ùå Could not load the best model\\\")\\n\\nelse:\\n    print(\\\"‚ö†Ô∏è  Best model checkpoint not found. Please complete training first.\\\")\\n\\nprint(f\\\"\\\\n‚úÖ Model evaluation and analysis complete!\\\")\\nprint(f\\\"üìä All evaluation results saved to checkpoint directory\\\")\\nprint(f\\\"üéØ Ready for model deployment and production use!\\\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
