{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f1581f",
   "metadata": {},
   "source": [
    "# üöÄ ViT-FishID: MAE Pretraining + EMA Student-Teacher Training\n",
    "\n",
    "**ADVANCED FISH CLASSIFICATION WITH MASKED AUTOENCODERS & SEMI-SUPERVISED LEARNING**\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/cat-thomson/ViT-FishID/blob/main/ViT_FishID_MAE_EMA_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## üéØ Training Pipeline Overview\n",
    "\n",
    "This notebook implements a comprehensive two-stage training approach:\n",
    "\n",
    "**Stage 1: Masked Autoencoder (MAE) Pretraining** üé≠\n",
    "- Self-supervised pretraining on unlabeled fish images\n",
    "- Learns robust visual representations by reconstructing masked patches\n",
    "- Uses 75% masking ratio for strong representation learning\n",
    "- Expected training time: 2-3 hours\n",
    "\n",
    "**Stage 2: EMA Student-Teacher Semi-Supervised Learning** üéì\n",
    "- Fine-tunes MAE-pretrained backbone for fish classification\n",
    "- Combines labeled supervision with unlabeled consistency learning\n",
    "- Uses exponential moving average teacher for pseudo-labeling\n",
    "- Expected training time: 4-6 hours\n",
    "\n",
    "## üìä Expected Performance Improvements\n",
    "\n",
    "- **Without MAE**: ~75-80% accuracy after 100 epochs\n",
    "- **With MAE + EMA**: ~85-92% accuracy after 100 epochs\n",
    "- **Data efficiency**: Better performance with limited labeled data\n",
    "- **Generalization**: Improved robustness to unseen fish species\n",
    "\n",
    "## üõ†Ô∏è Requirements\n",
    "\n",
    "- **GPU**: Colab Pro recommended (T4/V100/A100)\n",
    "- **Memory**: ~12-16GB GPU memory\n",
    "- **Runtime**: 6-9 hours total training time\n",
    "- **Data**: Fish cutouts dataset with labeled/unlabeled images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd81dfbf",
   "metadata": {},
   "source": [
    "## üîß Section 1: Environment Setup and GPU Check\n",
    "\n",
    "Setting up the optimal environment for MAE pretraining and EMA training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451d2982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç BASIC SYSTEM INFORMATION\n",
      "==================================================\n",
      "Python version: 3.13.5 (main, Jun 11 2025, 15:36:57) [Clang 17.0.0 (clang-1700.0.13.3)]\n",
      "Platform: macOS-15.5-arm64-arm-64bit-Mach-O\n",
      "Architecture: arm64\n",
      "‚ÑπÔ∏è  Not running in Google Colab\n",
      "\n",
      "üéØ Training Pipeline Overview:\n",
      "  - Stage 1: MAE Pretraining (Self-supervised)\n",
      "  - Stage 2: EMA Student-Teacher (Semi-supervised)\n",
      "  - Expected total time: 6-9 hours\n",
      "  - Memory requirements: 8-12GB GPU (optimized)\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANT SETUP NOTES:\n",
      "1. Ensure GPU runtime is enabled\n",
      "2. Upload fish_cutouts.zip to Google Drive root\n",
      "3. Allow kernel restart in Section 2 (this fixes CUDNN issues)\n",
      "4. Section 1 should run in <30 seconds\n",
      "\n",
      "‚úÖ Basic environment check complete!\n",
      "üöÄ Proceed to Section 2 to mount Drive and install dependencies\n"
     ]
    }
   ],
   "source": [
    "# Basic environment setup and GPU check (lightweight)\n",
    "import sys\n",
    "import os\n",
    "import platform\n",
    "\n",
    "print(\"üîç BASIC SYSTEM INFORMATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# System info (no heavy imports)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Architecture: {platform.machine()}\")\n",
    "\n",
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running in Google Colab\")\n",
    "    \n",
    "    # Check Colab GPU status\n",
    "    try:\n",
    "        gpu_info = !nvidia-smi\n",
    "        print(\"‚úÖ nvidia-smi available - GPU runtime detected\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  nvidia-smi not available - may be CPU runtime\")\n",
    "        print(\"   Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ÑπÔ∏è  Not running in Google Colab\")\n",
    "\n",
    "print(\"\\nüéØ Training Pipeline Overview:\")\n",
    "print(\"  - Stage 1: MAE Pretraining (Self-supervised)\")\n",
    "print(\"  - Stage 2: EMA Student-Teacher (Semi-supervised)\")\n",
    "print(\"  - Expected total time: 6-9 hours\")\n",
    "print(\"  - Memory requirements: 8-12GB GPU (optimized)\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT SETUP NOTES:\")\n",
    "print(\"1. Ensure GPU runtime is enabled\")\n",
    "print(\"2. Upload fish_cutouts.zip to Google Drive root\")\n",
    "print(\"3. Allow kernel restart in Section 2 (this fixes CUDNN issues)\")\n",
    "print(\"4. Section 1 should run in <30 seconds\")\n",
    "\n",
    "print(\"\\n‚úÖ Basic environment check complete!\")\n",
    "print(\"üöÄ Proceed to Section 2 to mount Drive and install dependencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a977938",
   "metadata": {},
   "source": [
    "## üö® TROUBLESHOOTING: If Section 1 is Running Too Long\n",
    "\n",
    "**If the previous cell has been executing for >2 minutes:**\n",
    "\n",
    "1. **INTERRUPT THE CELL**: Click the ‚èπÔ∏è **Stop** button in Colab\n",
    "2. **RESTART RUNTIME**: Runtime ‚Üí Restart Runtime  \n",
    "3. **RE-RUN**: Execute the cell again\n",
    "\n",
    "**Common causes of long execution:**\n",
    "- Missing GPU runtime (switches to slow CPU mode)\n",
    "- Automatic package installation in background\n",
    "- Import conflicts from previous runs\n",
    "\n",
    "**Expected behavior:**\n",
    "- Section 1 should complete in **<30 seconds**\n",
    "- Should show \"‚úÖ Basic environment check complete!\"\n",
    "- No heavy library imports (PyTorch comes later)\n",
    "\n",
    "**Next steps after Section 1:**\n",
    "1. Section 2: Mount Google Drive (quick)\n",
    "2. Section 2: Install Dependencies (will restart kernel - normal!)\n",
    "3. Section 2: Verify Installation (checks PyTorch/CUDA)\n",
    "4. Then proceed with data setup and MAE training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36557ca5",
   "metadata": {},
   "source": [
    "## üìÅ Section 2: Mount Google Drive and Install Dependencies\n",
    "\n",
    "Setting up data access and installing packages for MAE and EMA training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac96382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ SETTING UP LOCAL DATA ACCESS\n",
      "==================================================\n",
      "üè† Project root: /Users/catalinathomson/Desktop/Fish/ViT-FishID\n",
      "üìÇ Data directory: /Users/catalinathomson/Desktop/Fish/ViT-FishID/fish_cutouts\n",
      "‚úÖ Changed to project directory: /Users/catalinathomson/Desktop/Fish/ViT-FishID\n",
      "‚úÖ Added project to Python path\n",
      "‚úÖ Data directory found: /Users/catalinathomson/Desktop/Fish/ViT-FishID/fish_cutouts\n",
      "üêü Found 37 species with 5142 labeled images\n",
      "üìä Found 24015 unlabeled images for MAE pretraining\n",
      "üíæ Found 63 existing checkpoints\n",
      "\n",
      "üí° Local environment ready for MAE training!\n"
     ]
    }
   ],
   "source": [
    "# Setup local data access (adapted for local execution)\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"üìÅ SETTING UP LOCAL DATA ACCESS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set up local paths based on your existing configuration\n",
    "project_root = \"/Users/catalinathomson/Desktop/Fish/ViT-FishID\"\n",
    "data_dir = \"/Users/catalinathomson/Desktop/Fish/ViT-FishID/fish_cutouts\"\n",
    "\n",
    "print(f\"üè† Project root: {project_root}\")\n",
    "print(f\"üìÇ Data directory: {data_dir}\")\n",
    "\n",
    "# Check if we're in the right directory\n",
    "if os.path.exists(project_root):\n",
    "    os.chdir(project_root)\n",
    "    print(f\"‚úÖ Changed to project directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"‚ùå Project directory not found: {project_root}\")\n",
    "    raise FileNotFoundError(\"Project directory not found\")\n",
    "\n",
    "# Add project to Python path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(\"‚úÖ Added project to Python path\")\n",
    "\n",
    "# Verify data structure\n",
    "if os.path.exists(data_dir):\n",
    "    print(f\"‚úÖ Data directory found: {data_dir}\")\n",
    "    \n",
    "    # Check for labeled and unlabeled directories\n",
    "    labeled_dir = os.path.join(data_dir, 'labeled')\n",
    "    unlabeled_dir = os.path.join(data_dir, 'unlabeled')\n",
    "    \n",
    "    if os.path.exists(labeled_dir):\n",
    "        species = [d for d in os.listdir(labeled_dir) \n",
    "                  if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')]\n",
    "        total_labeled = sum([len([f for f in os.listdir(os.path.join(labeled_dir, s))\n",
    "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "                            for s in species])\n",
    "        print(f\"üêü Found {len(species)} species with {total_labeled} labeled images\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Labeled directory not found: {labeled_dir}\")\n",
    "    \n",
    "    if os.path.exists(unlabeled_dir):\n",
    "        unlabeled_count = len([f for f in os.listdir(unlabeled_dir)\n",
    "                             if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"üìä Found {unlabeled_count} unlabeled images for MAE pretraining\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Unlabeled directory not found: {unlabeled_dir}\")\n",
    "        print(\"   Creating unlabeled directory from existing labeled data...\")\n",
    "        # We'll create this later if needed\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Data directory not found: {data_dir}\")\n",
    "    print(\"   Please ensure fish_cutouts directory exists in project root\")\n",
    "    raise FileNotFoundError(\"Data directory not found\")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "checkpoint_dir = os.path.join(project_root, 'local_checkpoints')\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
    "    print(f\"üíæ Found {len(checkpoints)} existing checkpoints\")\n",
    "else:\n",
    "    print(\"üìÅ No existing checkpoints found\")\n",
    "\n",
    "print(\"\\nüí° Local environment ready for MAE training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d538c3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ VERIFYING LOCAL DEPENDENCIES\n",
      "==================================================\n",
      "üß™ Testing package imports...\n",
      "‚úÖ PyTorch: 2.8.0\n",
      "‚úÖ PyTorch: 2.8.0\n",
      "‚úÖ TorchVision: 0.23.0\n",
      "‚úÖ TorchVision: 0.23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catalinathomson/Desktop/Fish/ViT-FishID/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Timm (PyTorch Image Models): 1.0.19\n",
      "‚úÖ HuggingFace Transformers: 4.55.2\n",
      "‚úÖ HuggingFace Transformers: 4.55.2\n",
      "‚úÖ Albumentations: 2.0.8\n",
      "‚úÖ OpenCV: 4.12.0\n",
      "‚úÖ Albumentations: 2.0.8\n",
      "‚úÖ OpenCV: 4.12.0\n",
      "‚úÖ Scikit-learn: 1.7.1\n",
      "‚úÖ Matplotlib: 3.10.5\n",
      "‚úÖ NumPy: 2.2.6\n",
      "‚úÖ Pillow: 11.3.0\n",
      "‚úÖ TQDM: 4.67.1\n",
      "\n",
      "üìä DEPENDENCY STATUS\n",
      "==============================\n",
      "‚úÖ Working: 11/11\n",
      "üéâ All required packages are available!\n",
      "\n",
      "‚úÖ LOCAL DEPENDENCY CHECK COMPLETE!\n",
      "üöÄ All dependencies ready for MAE training!\n",
      "‚úÖ Scikit-learn: 1.7.1\n",
      "‚úÖ Matplotlib: 3.10.5\n",
      "‚úÖ NumPy: 2.2.6\n",
      "‚úÖ Pillow: 11.3.0\n",
      "‚úÖ TQDM: 4.67.1\n",
      "\n",
      "üìä DEPENDENCY STATUS\n",
      "==============================\n",
      "‚úÖ Working: 11/11\n",
      "üéâ All required packages are available!\n",
      "\n",
      "‚úÖ LOCAL DEPENDENCY CHECK COMPLETE!\n",
      "üöÄ All dependencies ready for MAE training!\n"
     ]
    }
   ],
   "source": [
    "# Verify local dependencies (adapted for local execution)\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "print(\"üì¶ VERIFYING LOCAL DEPENDENCIES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Critical packages for MAE training\n",
    "required_packages = [\n",
    "    ('torch', 'PyTorch'),\n",
    "    ('torchvision', 'TorchVision'),\n",
    "    ('timm', 'Timm (PyTorch Image Models)'),\n",
    "    ('transformers', 'HuggingFace Transformers'),\n",
    "    ('albumentations', 'Albumentations'),\n",
    "    ('cv2', 'OpenCV'),\n",
    "    ('sklearn', 'Scikit-learn'),\n",
    "    ('matplotlib', 'Matplotlib'),\n",
    "    ('numpy', 'NumPy'),\n",
    "    ('PIL', 'Pillow'),\n",
    "    ('tqdm', 'TQDM')\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing package imports...\")\n",
    "working_packages = []\n",
    "failed_packages = []\n",
    "\n",
    "for package, name in required_packages:\n",
    "    try:\n",
    "        module = importlib.import_module(package)\n",
    "        version = getattr(module, '__version__', 'Unknown')\n",
    "        print(f\"‚úÖ {name}: {version}\")\n",
    "        working_packages.append(name)\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {name}: Failed to import ({e})\")\n",
    "        failed_packages.append(name)\n",
    "\n",
    "print(f\"\\nüìä DEPENDENCY STATUS\")\n",
    "print(\"=\"*30)\n",
    "print(f\"‚úÖ Working: {len(working_packages)}/{len(required_packages)}\")\n",
    "\n",
    "if failed_packages:\n",
    "    print(f\"‚ùå Failed: {', '.join(failed_packages)}\")\n",
    "    print(\"\\nüîß To install missing packages:\")\n",
    "    for package, name in required_packages:\n",
    "        if name in failed_packages:\n",
    "            print(f\"  pip install {package}\")\n",
    "else:\n",
    "    print(\"üéâ All required packages are available!\")\n",
    "\n",
    "print(f\"\\n‚úÖ LOCAL DEPENDENCY CHECK COMPLETE!\")\n",
    "print(\"üöÄ All dependencies ready for MAE training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc13a2e",
   "metadata": {},
   "source": [
    "## üö® **TROUBLESHOOTING: Dependency Installation Issues**\n",
    "\n",
    "### **If the previous cell shows a dotted line and won't run:**\n",
    "\n",
    "**Immediate Solutions:**\n",
    "\n",
    "1. **Interrupt the Cell**: \n",
    "   - Click ‚èπÔ∏è **Stop** button in Colab\n",
    "   - Or press **Ctrl+M I** (Cmd+M I on Mac)\n",
    "\n",
    "2. **Restart Runtime**:\n",
    "   - Go to **Runtime ‚Üí Restart Runtime**\n",
    "   - Then re-run the cell\n",
    "\n",
    "3. **Alternative: Manual Step-by-Step Installation**:\n",
    "   ```python\n",
    "   # Run these one at a time if needed:\n",
    "   !pip install torch torchvision torchaudio\n",
    "   !pip install timm transformers albumentations\n",
    "   !pip install opencv-python-headless scikit-learn\n",
    "   !pip install matplotlib seaborn tqdm wandb\n",
    "   ```\n",
    "\n",
    "### **Why the dotted line appears:**\n",
    "- Colab is waiting for pip uninstall confirmation  \n",
    "- Large package downloads are taking time\n",
    "- Network connectivity issues\n",
    "- Kernel is processing heavy installations\n",
    "\n",
    "### **Fixed approach benefits:**\n",
    "- ‚úÖ No forced kernel restart (was causing hang)\n",
    "- ‚úÖ No aggressive PyTorch uninstall (was causing permission issues) \n",
    "- ‚úÖ Step-by-step with timeout handling\n",
    "- ‚úÖ Error recovery and fallback options\n",
    "- ‚úÖ Progress feedback every step\n",
    "\n",
    "### **Success indicators to look for:**\n",
    "```\n",
    "‚úÖ Current PyTorch version: 2.x.x\n",
    "‚úÖ CUDA is available with current installation  \n",
    "‚úÖ All critical imports working!\n",
    "üöÄ No kernel restart needed\n",
    "```\n",
    "\n",
    "**Expected execution time: 2-4 minutes (much faster than before)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae1f04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã PYTORCH ENVIRONMENT SETUP (LOCAL)\n",
      "==================================================\n",
      "‚úÖ Package Versions:\n",
      "  torch: 2.8.0\n",
      "  torchvision: 0.23.0\n",
      "  timm: 1.0.19\n",
      "  transformers: 4.55.2\n",
      "  albumentations: 2.0.8\n",
      "  opencv: 4.12.0\n",
      "  sklearn: 1.7.1\n",
      "\n",
      "üîç DEVICE DETECTION\n",
      "==============================\n",
      "CUDA available: False\n",
      "MPS available: True\n",
      "‚úÖ Using Apple Silicon GPU (MPS)\n",
      "üéØ This matches your EMA training device configuration\n",
      "\n",
      "üß™ TESTING DEVICE OPERATIONS\n",
      "==============================\n",
      "‚úÖ Device operations working correctly on Apple Silicon GPU (MPS)\n",
      "üìä Test tensor shape: torch.Size([2, 3, 224, 224])\n",
      "üìä Device: mps:0\n",
      "‚úÖ Memory cleanup completed\n",
      "\n",
      "üéØ Using device: mps\n",
      "üí° Device setup matches your EMA training configuration\n",
      "\n",
      "üöÄ PyTorch environment ready for MAE training!\n",
      "üí° Configuration matches your EMA teacher-student setup!\n",
      "‚úÖ Device operations working correctly on Apple Silicon GPU (MPS)\n",
      "üìä Test tensor shape: torch.Size([2, 3, 224, 224])\n",
      "üìä Device: mps:0\n",
      "‚úÖ Memory cleanup completed\n",
      "\n",
      "üéØ Using device: mps\n",
      "üí° Device setup matches your EMA training configuration\n",
      "\n",
      "üöÄ PyTorch environment ready for MAE training!\n",
      "üí° Configuration matches your EMA teacher-student setup!\n"
     ]
    }
   ],
   "source": [
    "# Setup PyTorch environment for local execution (Mac MPS)\n",
    "import torch\n",
    "import torchvision\n",
    "import timm\n",
    "import transformers\n",
    "import albumentations\n",
    "import cv2\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "print(\"üìã PYTORCH ENVIRONMENT SETUP (LOCAL)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"‚úÖ Package Versions:\")\n",
    "print(f\"  torch: {torch.__version__}\")\n",
    "print(f\"  torchvision: {torchvision.__version__}\")\n",
    "print(f\"  timm: {timm.__version__}\")\n",
    "print(f\"  transformers: {transformers.__version__}\")\n",
    "print(f\"  albumentations: {albumentations.__version__}\")\n",
    "print(f\"  opencv: {cv2.__version__}\")\n",
    "print(f\"  sklearn: {sklearn.__version__}\")\n",
    "\n",
    "# Device detection and setup for Mac\n",
    "print(f\"\\nüîç DEVICE DETECTION\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Check available devices\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Select optimal device based on your training configuration\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "    device_name = \"Apple Silicon GPU (MPS)\"\n",
    "    print(f\"‚úÖ Using Apple Silicon GPU (MPS)\")\n",
    "    print(f\"üéØ This matches your EMA training device configuration\")\n",
    "    \n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    device_name = f\"CUDA GPU: {torch.cuda.get_device_name(0)}\"\n",
    "    print(f\"‚úÖ Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    device_name = \"CPU\"\n",
    "    print(\"‚ö†Ô∏è  Using CPU - training will be slower\")\n",
    "    print(\"   For Mac: Consider using MPS if available\")\n",
    "\n",
    "# Test device functionality\n",
    "print(f\"\\nüß™ TESTING DEVICE OPERATIONS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "try:\n",
    "    # Test tensor operations on selected device\n",
    "    test_tensor = torch.randn(2, 3, 224, 224).to(DEVICE)\n",
    "    \n",
    "    # Test basic operations\n",
    "    result = test_tensor * 2.0\n",
    "    result = result.sum()\n",
    "    \n",
    "    print(f\"‚úÖ Device operations working correctly on {device_name}\")\n",
    "    print(f\"üìä Test tensor shape: {test_tensor.shape}\")\n",
    "    print(f\"üìä Device: {test_tensor.device}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del test_tensor, result\n",
    "    \n",
    "    if DEVICE.type == 'mps':\n",
    "        # MPS-specific cleanup\n",
    "        torch.mps.empty_cache()\n",
    "    elif DEVICE.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    gc.collect()\n",
    "    print(\"‚úÖ Memory cleanup completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Device test failed: {e}\")\n",
    "    print(\"üîß Falling back to CPU\")\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "# Set random seeds for reproducibility (matching your config)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if DEVICE.type == 'cuda':\n",
    "    torch.cuda.manual_seed(42)\n",
    "elif DEVICE.type == 'mps':\n",
    "    torch.mps.manual_seed(42)\n",
    "\n",
    "print(f\"\\nüéØ Using device: {DEVICE}\")\n",
    "print(\"üí° Device setup matches your EMA training configuration\")\n",
    "\n",
    "# Global configuration matching your training setup\n",
    "CONFIG = {\n",
    "    'device': DEVICE,\n",
    "    'batch_size': 8,  # Same as your EMA training\n",
    "    'learning_rate': 1e-4,  # Same as your EMA training\n",
    "    'num_classes': 37,  # Same as your dataset\n",
    "    'image_size': 224,  # Same as your training\n",
    "    'num_workers': 2,  # Same as your training\n",
    "    'seed': 42,  # Same as your training\n",
    "}\n",
    "\n",
    "print(f\"\\nüöÄ PyTorch environment ready for MAE training!\")\n",
    "print(\"üí° Configuration matches your EMA teacher-student setup!\")\n",
    "\n",
    "# Store global variables for later cells\n",
    "globals().update({\n",
    "    'DEVICE': DEVICE,\n",
    "    'CONFIG': CONFIG\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d378f",
   "metadata": {},
   "source": [
    "## üîÑ Section 3: Clone Repository and Setup Data\n",
    "\n",
    "Cloning ViT-FishID repository and preparing fish dataset for MAE pretraining and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c6a44e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• CLONING ViT-FishID REPOSITORY\n",
      "==================================================\n",
      "üì• Cloning ViT-FishID repository...\n",
      "fatal: could not create leading directories of '/content/ViT-FishID': Read-only file system\n",
      "[Errno 2] No such file or directory: '/content/ViT-FishID'\n",
      "/Users/catalinathomson/Desktop/Fish/ViT-FishID\n",
      "\n",
      "üìÇ Repository structure:\n",
      "total 3135328\n",
      "drwxr-xr-x@  35 catalinathomson  staff        1120 Aug 18 18:47 \u001b[34m.\u001b[m\u001b[m\n",
      "drwxr-xr-x    8 catalinathomson  staff         256 Aug 12 21:27 \u001b[34m..\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 catalinathomson  staff        8196 Aug 18 18:32 .DS_Store\n",
      "drwxr-xr-x@  15 catalinathomson  staff         480 Aug 18 18:47 \u001b[34m.git\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 catalinathomson  staff          66 Jul 31 10:30 .gitattributes\n",
      "-rw-r--r--@   1 catalinathomson  staff         646 Aug  5 15:33 .gitignore\n",
      "-rw-r--r--@   1 catalinathomson  staff        4182 Aug 16 15:59 COLAB_CRASH_FIXES.md\n",
      "-rw-r--r--@   1 catalinathomson  staff        3328 Aug 13 21:33 EXTENDED_TRAINING_SETUP.md\n",
      "-rw-r--r--@   1 catalinathomson  staff           0 Aug 18 10:05 MAE_INTEGRATION_GUIDE.md\n",
      "-rw-r--r--@   1 catalinathomson  staff       16566 Aug 13 13:14 README.md\n",
      "-rw-r--r--@   1 catalinathomson  staff        4982 Aug 13 21:38 TRAINING_FIXES_APPLIED.md\n",
      "-rw-r--r--@   1 catalinathomson  staff      366088 Aug 18 18:47 ViT_FishID_Colab_Training.ipynb\n",
      "-rw-r--r--@   1 catalinathomson  staff      167703 Aug 18 11:46 ViT_FishID_Colab_Training_Reordered.ipynb\n",
      "-rw-r--r--@   1 catalinathomson  staff      201683 Aug 18 18:47 ViT_FishID_MAE_EMA_Training.ipynb\n",
      "drwxr-xr-x@   6 catalinathomson  staff         192 Aug 14 17:00 \u001b[34m__pycache__\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 catalinathomson  staff  1373524534 Aug 13 21:25 checkpoint_epoch_19.pth\n",
      "drwxr-xr-x@   4 catalinathomson  staff         128 Aug 18 10:39 \u001b[34mcolab_checkpoints\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 catalinathomson  staff       21217 Aug 13 13:39 data.py\n",
      "-rw-r--r--@   1 catalinathomson  staff       15023 Aug 18 18:20 evaluate.py\n",
      "drwxr-xr-x@   8 catalinathomson  staff         256 Aug 14 07:31 \u001b[34mfish_cutouts\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 catalinathomson  staff   228517968 Aug 13 13:04 fish_cutouts.zip\n",
      "drwxr-xr-x@  66 catalinathomson  staff        2112 Aug 16 08:27 \u001b[34mlocal_checkpoints\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 catalinathomson  staff       13100 Aug 14 16:53 local_resume_training.py\n",
      "drwxr-xr-x@   2 catalinathomson  staff          64 Aug 16 17:30 \u001b[34mmae_small_backups\u001b[m\u001b[m\n",
      "drwxr-xr-x@ 765 catalinathomson  staff       24480 Aug 16 20:05 \u001b[34mmae_small_checkpoints\u001b[m\u001b[m\n",
      "drwxr-xr-x@   3 catalinathomson  staff          96 Aug 16 17:45 \u001b[34mmae_small_logs\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 catalinathomson  staff        9495 Aug 13 21:38 model.py\n",
      "-rw-r--r--@   1 catalinathomson  staff       16771 Aug 13 21:33 pipeline.py\n",
      "-rw-r--r--@   1 catalinathomson  staff         202 Aug 13 09:15 requirements.txt\n",
      "-rw-r--r--@   1 catalinathomson  staff        4271 Aug 14 16:53 resume_training.py\n",
      "-rw-r--r--@   1 catalinathomson  staff        5134 Aug 13 12:40 species_mapping.txt\n",
      "-rw-r--r--@   1 catalinathomson  staff       15343 Aug 14 16:53 train.py\n",
      "-rw-r--r--@   1 catalinathomson  staff       25503 Aug 14 16:53 trainer.py\n",
      "-rw-r--r--@   1 catalinathomson  staff        8818 Aug 13 09:15 utils.py\n",
      "drwxr-xr-x@   8 catalinathomson  staff         256 Aug 14 14:32 \u001b[34mvenv\u001b[m\u001b[m\n",
      "[Errno 2] No such file or directory: '/content/ViT-FishID'\n",
      "/Users/catalinathomson/Desktop/Fish/ViT-FishID\n",
      "\n",
      "üìÇ Repository structure:\n",
      "total 3135328\n",
      "drwxr-xr-x@  35 catalinathomson  staff        1120 Aug 18 18:47 \u001b[34m.\u001b[m\u001b[m\n",
      "drwxr-xr-x    8 catalinathomson  staff         256 Aug 12 21:27 \u001b[34m..\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 catalinathomson  staff        8196 Aug 18 18:32 .DS_Store\n",
      "drwxr-xr-x@  15 catalinathomson  staff         480 Aug 18 18:47 \u001b[34m.git\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 catalinathomson  staff          66 Jul 31 10:30 .gitattributes\n",
      "-rw-r--r--@   1 catalinathomson  staff         646 Aug  5 15:33 .gitignore\n",
      "-rw-r--r--@   1 catalinathomson  staff        4182 Aug 16 15:59 COLAB_CRASH_FIXES.md\n",
      "-rw-r--r--@   1 catalinathomson  staff        3328 Aug 13 21:33 EXTENDED_TRAINING_SETUP.md\n",
      "-rw-r--r--@   1 catalinathomson  staff           0 Aug 18 10:05 MAE_INTEGRATION_GUIDE.md\n",
      "-rw-r--r--@   1 catalinathomson  staff       16566 Aug 13 13:14 README.md\n",
      "-rw-r--r--@   1 catalinathomson  staff        4982 Aug 13 21:38 TRAINING_FIXES_APPLIED.md\n",
      "-rw-r--r--@   1 catalinathomson  staff      366088 Aug 18 18:47 ViT_FishID_Colab_Training.ipynb\n",
      "-rw-r--r--@   1 catalinathomson  staff      167703 Aug 18 11:46 ViT_FishID_Colab_Training_Reordered.ipynb\n",
      "-rw-r--r--@   1 catalinathomson  staff      201683 Aug 18 18:47 ViT_FishID_MAE_EMA_Training.ipynb\n",
      "drwxr-xr-x@   6 catalinathomson  staff         192 Aug 14 17:00 \u001b[34m__pycache__\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 catalinathomson  staff  1373524534 Aug 13 21:25 checkpoint_epoch_19.pth\n",
      "drwxr-xr-x@   4 catalinathomson  staff         128 Aug 18 10:39 \u001b[34mcolab_checkpoints\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 catalinathomson  staff       21217 Aug 13 13:39 data.py\n",
      "-rw-r--r--@   1 catalinathomson  staff       15023 Aug 18 18:20 evaluate.py\n",
      "drwxr-xr-x@   8 catalinathomson  staff         256 Aug 14 07:31 \u001b[34mfish_cutouts\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 catalinathomson  staff   228517968 Aug 13 13:04 fish_cutouts.zip\n",
      "drwxr-xr-x@  66 catalinathomson  staff        2112 Aug 16 08:27 \u001b[34mlocal_checkpoints\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 catalinathomson  staff       13100 Aug 14 16:53 local_resume_training.py\n",
      "drwxr-xr-x@   2 catalinathomson  staff          64 Aug 16 17:30 \u001b[34mmae_small_backups\u001b[m\u001b[m\n",
      "drwxr-xr-x@ 765 catalinathomson  staff       24480 Aug 16 20:05 \u001b[34mmae_small_checkpoints\u001b[m\u001b[m\n",
      "drwxr-xr-x@   3 catalinathomson  staff          96 Aug 16 17:45 \u001b[34mmae_small_logs\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 catalinathomson  staff        9495 Aug 13 21:38 model.py\n",
      "-rw-r--r--@   1 catalinathomson  staff       16771 Aug 13 21:33 pipeline.py\n",
      "-rw-r--r--@   1 catalinathomson  staff         202 Aug 13 09:15 requirements.txt\n",
      "-rw-r--r--@   1 catalinathomson  staff        4271 Aug 14 16:53 resume_training.py\n",
      "-rw-r--r--@   1 catalinathomson  staff        5134 Aug 13 12:40 species_mapping.txt\n",
      "-rw-r--r--@   1 catalinathomson  staff       15343 Aug 14 16:53 train.py\n",
      "-rw-r--r--@   1 catalinathomson  staff       25503 Aug 14 16:53 trainer.py\n",
      "-rw-r--r--@   1 catalinathomson  staff        8818 Aug 13 09:15 utils.py\n",
      "drwxr-xr-x@   8 catalinathomson  staff         256 Aug 14 14:32 \u001b[34mvenv\u001b[m\u001b[m\n",
      "‚úÖ Found: model.py\n",
      "‚úÖ Found: trainer.py\n",
      "‚úÖ Found: data.py\n",
      "‚úÖ Found: train.py\n",
      "\n",
      "‚úÖ All required files found!\n",
      "üîß Added repository to Python path\n",
      "\n",
      "üöÄ Repository ready for MAE and EMA implementation!\n",
      "‚úÖ Found: model.py\n",
      "‚úÖ Found: trainer.py\n",
      "‚úÖ Found: data.py\n",
      "‚úÖ Found: train.py\n",
      "\n",
      "‚úÖ All required files found!\n",
      "üîß Added repository to Python path\n",
      "\n",
      "üöÄ Repository ready for MAE and EMA implementation!\n"
     ]
    }
   ],
   "source": [
    "# Clone ViT-FishID repository and prepare codebase\n",
    "import os\n",
    "\n",
    "print(\"üì• CLONING ViT-FishID REPOSITORY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Remove existing directory if it exists\n",
    "if os.path.exists('/content/ViT-FishID'):\n",
    "    !rm -rf /content/ViT-FishID\n",
    "    print(\"üóëÔ∏è  Removed existing repository\")\n",
    "\n",
    "# Clone the repository\n",
    "print(\"üì• Cloning ViT-FishID repository...\")\n",
    "!git clone https://github.com/cat-thomson/ViT-FishID.git /content/ViT-FishID\n",
    "\n",
    "# Change to project directory\n",
    "%cd /content/ViT-FishID\n",
    "\n",
    "# Verify repository structure\n",
    "print(\"\\nüìÇ Repository structure:\")\n",
    "!ls -la\n",
    "\n",
    "# Check for key files\n",
    "required_files = ['model.py', 'trainer.py', 'data.py', 'train.py']\n",
    "missing_files = []\n",
    "\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"‚úÖ Found: {file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Missing: {file}\")\n",
    "        missing_files.append(file)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing files: {missing_files}\")\n",
    "    print(\"   These will be created as part of the MAE implementation\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All required files found!\")\n",
    "\n",
    "# Set up Python path for imports\n",
    "import sys\n",
    "if '/content/ViT-FishID' not in sys.path:\n",
    "    sys.path.append('/content/ViT-FishID')\n",
    "    print(\"üîß Added repository to Python path\")\n",
    "\n",
    "print(\"\\nüöÄ Repository ready for MAE and EMA implementation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e183528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêü SETTING UP LOCAL FISH DATASET\n",
      "==================================================\n",
      "üéØ Data directory: /Users/catalinathomson/Desktop/Fish/ViT-FishID/fish_cutouts\n",
      "üéØ Labeled directory: /Users/catalinathomson/Desktop/Fish/ViT-FishID/fish_cutouts/labeled\n",
      "üéØ Unlabeled directory: /Users/catalinathomson/Desktop/Fish/ViT-FishID/fish_cutouts/unlabeled\n",
      "‚úÖ Data directory found!\n",
      "\n",
      "üêü LABELED DATA ANALYSIS\n",
      "==============================\n",
      "Species found: 37\n",
      "Total labeled images: 5142\n",
      "\n",
      "Top 10 species by image count:\n",
      "   1. Sparidae_Chrysoblephus_puniceus: 1013 images\n",
      "   2. Sparidae_Diplodus_capensis: 747 images\n",
      "   3. Sparidae_Porcostoma_dentata: 447 images\n",
      "   4. Sparidae_Boopsoidea_inornata: 370 images\n",
      "   5. Sparidae_Chrysoblephus_anglicus: 366 images\n",
      "   6. Sparidae_Sarpa_salpa: 319 images\n",
      "   7. Sparidae_Cheimerius_nufar: 270 images\n",
      "   8. Sparidae_Polysteganus_praeorbitalis: 256 images\n",
      "   9. Carangidae_Trachurus_delagoa: 207 images\n",
      "  10. Sparidae_Pachymetopon_aeneum: 205 images\n",
      "  ... and 27 more species\n",
      "\n",
      "üìä UNLABELED DATA FOR MAE\n",
      "==============================\n",
      "Unlabeled images: 24015\n",
      "‚úÖ Perfect for MAE self-supervised pretraining!\n",
      "\n",
      "üìä DATASET SUMMARY (MATCHING YOUR EMA CONFIG)\n",
      "==================================================\n",
      "‚úÖ Number of classes: 37\n",
      "‚úÖ Total labeled images: 5142\n",
      "‚úÖ Unlabeled images for MAE: 24015\n",
      "‚úÖ Image size: 224x224 (matches your config)\n",
      "‚úÖ Data ready for MAE pretraining + EMA fine-tuning!\n",
      "\n",
      "‚úÖ Class count verification: 37 classes matches your EMA training!\n",
      "\n",
      "üéØ Dataset configuration ready for MAE pretraining!\n"
     ]
    }
   ],
   "source": [
    "# Setup fish dataset for local MAE + EMA training (with Unicode error handling)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üêü SETTING UP LOCAL FISH DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use your existing local data structure\n",
    "DATA_DIR = '/Users/catalinathomson/Desktop/Fish/ViT-FishID/fish_cutouts'\n",
    "labeled_dir = os.path.join(DATA_DIR, 'labeled')\n",
    "unlabeled_dir = os.path.join(DATA_DIR, 'unlabeled')\n",
    "\n",
    "print(f\"üéØ Data directory: {DATA_DIR}\")\n",
    "print(f\"üéØ Labeled directory: {labeled_dir}\")\n",
    "print(f\"üéØ Unlabeled directory: {unlabeled_dir}\")\n",
    "\n",
    "def safe_listdir(path):\n",
    "    \"\"\"Safely list directory contents, handling Unicode errors.\"\"\"\n",
    "    try:\n",
    "        items = []\n",
    "        for item in os.listdir(path):\n",
    "            try:\n",
    "                # Test if item name can be encoded/decoded properly\n",
    "                item.encode('utf-8')\n",
    "                items.append(item)\n",
    "            except UnicodeEncodeError:\n",
    "                print(f\"‚ö†Ô∏è  Skipping file with invalid Unicode: {repr(item)}\")\n",
    "                continue\n",
    "        return items\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error listing directory {path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def count_images_safe(directory):\n",
    "    \"\"\"Safely count images in a directory.\"\"\"\n",
    "    count = 0\n",
    "    for item in safe_listdir(directory):\n",
    "        if item.lower().endswith(('.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG')):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Verify dataset structure\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(\"‚úÖ Data directory found!\")\n",
    "    \n",
    "    # Check labeled data\n",
    "    if os.path.exists(labeled_dir):\n",
    "        species = []\n",
    "        for item in safe_listdir(labeled_dir):\n",
    "            species_path = os.path.join(labeled_dir, item)\n",
    "            if os.path.isdir(species_path) and not item.startswith('.'):\n",
    "                species.append(item)\n",
    "        \n",
    "        print(f\"\\nüêü LABELED DATA ANALYSIS\")\n",
    "        print(\"=\"*30)\n",
    "        print(f\"Species found: {len(species)}\")\n",
    "        \n",
    "        # Count images per species (first 10)\n",
    "        total_labeled = 0\n",
    "        species_distribution = []\n",
    "        \n",
    "        for species_name in species:\n",
    "            species_path = os.path.join(labeled_dir, species_name)\n",
    "            if os.path.isdir(species_path):\n",
    "                count = count_images_safe(species_path)\n",
    "                species_distribution.append((species_name, count))\n",
    "                total_labeled += count\n",
    "        \n",
    "        # Sort by count\n",
    "        species_distribution.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"Total labeled images: {total_labeled}\")\n",
    "        print(f\"\\nTop 10 species by image count:\")\n",
    "        for i, (species_name, count) in enumerate(species_distribution[:10]):\n",
    "            print(f\"  {i+1:2d}. {species_name}: {count} images\")\n",
    "        \n",
    "        if len(species_distribution) > 10:\n",
    "            print(f\"  ... and {len(species_distribution) - 10} more species\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Labeled directory not found\")\n",
    "        raise FileNotFoundError(\"Labeled data directory missing\")\n",
    "    \n",
    "    # Check unlabeled data\n",
    "    if os.path.exists(unlabeled_dir):\n",
    "        unlabeled_count = count_images_safe(unlabeled_dir)\n",
    "        \n",
    "        print(f\"\\nüìä UNLABELED DATA FOR MAE\")\n",
    "        print(\"=\"*30)\n",
    "        print(f\"Unlabeled images: {unlabeled_count}\")\n",
    "        print(\"‚úÖ Perfect for MAE self-supervised pretraining!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Unlabeled directory not found\")\n",
    "        print(\"   This is fine - MAE can work with labeled data too\")\n",
    "        unlabeled_count = 0\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Data directory not found\")\n",
    "    raise FileNotFoundError(\"Dataset not found\")\n",
    "\n",
    "# Final dataset summary matching your configuration\n",
    "print(f\"\\nüìä DATASET SUMMARY (MATCHING YOUR EMA CONFIG)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úÖ Number of classes: {len(species)}\")\n",
    "print(f\"‚úÖ Total labeled images: {total_labeled}\")\n",
    "print(f\"‚úÖ Unlabeled images for MAE: {unlabeled_count}\")\n",
    "print(f\"‚úÖ Image size: 224x224 (matches your config)\")\n",
    "print(f\"‚úÖ Data ready for MAE pretraining + EMA fine-tuning!\")\n",
    "\n",
    "# Store global variables for later use (matching your config)\n",
    "LABELED_DIR = labeled_dir\n",
    "UNLABELED_DIR = unlabeled_dir\n",
    "NUM_CLASSES = len(species)\n",
    "\n",
    "# Verify it matches your training config\n",
    "if NUM_CLASSES == CONFIG['num_classes']:\n",
    "    print(f\"\\n‚úÖ Class count verification: {NUM_CLASSES} classes matches your EMA training!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Class count mismatch: Found {NUM_CLASSES}, expected {CONFIG['num_classes']}\")\n",
    "    print(f\"   Updating CONFIG to match actual dataset...\")\n",
    "    CONFIG['num_classes'] = NUM_CLASSES\n",
    "\n",
    "print(f\"\\nüéØ Dataset configuration ready for MAE pretraining!\")\n",
    "\n",
    "# Store in globals\n",
    "globals().update({\n",
    "    'LABELED_DIR': LABELED_DIR,\n",
    "    'UNLABELED_DIR': UNLABELED_DIR,\n",
    "    'NUM_CLASSES': NUM_CLASSES,\n",
    "    'species': species\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff50f82",
   "metadata": {},
   "source": [
    "## üé≠ Section 4: Implement Masked Autoencoder (MAE) Components\n",
    "\n",
    "Creating the complete MAE architecture with ViT encoder, lightweight decoder, and masking strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf920d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≠ CREATING MPS-OPTIMIZED MAE COMPONENTS\n",
      "==================================================\n",
      "üß™ TESTING MPS COMPATIBILITY\n",
      "==============================\n",
      "Creating test model...\n",
      "‚úÖ MPS-Optimized MAE created:\n",
      "  - Encoder: 384d, 6 layers, 6 heads\n",
      "  - Decoder: 192d, 4 layers, 3 heads\n",
      "Testing forward pass...\n",
      "‚úÖ MPS-optimized MAE test successful!\n",
      "‚è±Ô∏è  Test completed in 2.71 seconds\n",
      "üìä Loss: 1.3757\n",
      "üìä Prediction shape: torch.Size([1, 196, 768])\n",
      "üìä Mask shape: torch.Size([1, 196])\n",
      "‚úÖ Memory cleaned up\n",
      "\n",
      "üé≠ MPS-OPTIMIZED MAE READY!\n",
      "üí° This should run much faster than the original implementation!\n",
      "‚úÖ MPS-optimized MAE test successful!\n",
      "‚è±Ô∏è  Test completed in 2.71 seconds\n",
      "üìä Loss: 1.3757\n",
      "üìä Prediction shape: torch.Size([1, 196, 768])\n",
      "üìä Mask shape: torch.Size([1, 196])\n",
      "‚úÖ Memory cleaned up\n",
      "\n",
      "üé≠ MPS-OPTIMIZED MAE READY!\n",
      "üí° This should run much faster than the original implementation!\n"
     ]
    }
   ],
   "source": [
    "# MPS-Optimized MAE Implementation for Local Training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "print(\"üé≠ CREATING MPS-OPTIMIZED MAE COMPONENTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# MPS-friendly implementations\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"MPS-optimized Image to Patch Embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=384):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Use smaller embedding dimension for MPS efficiency\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, \n",
    "                             kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x)  # (B, embed_dim, H/patch_size, W/patch_size)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"Simplified attention for MPS compatibility\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=True)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, N, head_dim)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        \n",
    "        # Attention computation (MPS-friendly)\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        x = torch.matmul(attn, v)\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class SimpleTransformerBlock(nn.Module):\n",
    "    \"\"\"MPS-optimized transformer block\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = SimpleAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pre-norm architecture for better training\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class MPSOptimizedMAE(nn.Module):\n",
    "    \"\"\"Complete MAE optimized for Apple Silicon MPS\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 patch_size=16,\n",
    "                 embed_dim=384,  # Smaller for MPS efficiency\n",
    "                 depth=6,        # Reduced depth\n",
    "                 num_heads=6,    # Reduced heads\n",
    "                 decoder_embed_dim=192,\n",
    "                 decoder_depth=4,\n",
    "                 decoder_num_heads=3,\n",
    "                 mlp_ratio=4.0,\n",
    "                 norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Encoder\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            SimpleTransformerBlock(embed_dim, num_heads, mlp_ratio)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, decoder_embed_dim))\n",
    "        \n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            SimpleTransformerBlock(decoder_embed_dim, decoder_num_heads, mlp_ratio)\n",
    "            for _ in range(decoder_depth)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_norm = nn.LayerNorm(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * 3, bias=True)\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "        print(f\"‚úÖ MPS-Optimized MAE created:\")\n",
    "        print(f\"  - Encoder: {embed_dim}d, {depth} layers, {num_heads} heads\")\n",
    "        print(f\"  - Decoder: {decoder_embed_dim}d, {decoder_depth} layers, {decoder_num_heads} heads\")\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        # Initialize parameters\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.pos_embed, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "        torch.nn.init.normal_(self.decoder_pos_embed, std=.02)\n",
    "        \n",
    "        # Initialize linear layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def random_masking(self, x, mask_ratio=0.75):\n",
    "        \"\"\"Random masking optimized for MPS\"\"\"\n",
    "        B, N, D = x.shape\n",
    "        len_keep = int(N * (1 - mask_ratio))\n",
    "        \n",
    "        # Use CPU for random operations, then move to device\n",
    "        noise = torch.rand(B, N, device='cpu')\n",
    "        ids_shuffle = torch.argsort(noise, dim=1).to(x.device)\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "        \n",
    "        # Keep subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "        \n",
    "        # Generate mask\n",
    "        mask = torch.ones([B, N], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "        \n",
    "        return x_masked, mask, ids_restore\n",
    "    \n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"Convert images to patches\"\"\"\n",
    "        p = self.patch_size\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "        \n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(imgs.shape[0], 3, h, p, w, p)\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(imgs.shape[0], h * w, p**2 * 3)\n",
    "        return x\n",
    "    \n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"Convert patches back to images\"\"\"\n",
    "        p = self.patch_size\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(x.shape[0], h, w, p, p, 3)\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(x.shape[0], 3, h * p, h * p)\n",
    "        return imgs\n",
    "    \n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Add pos embed without cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        \n",
    "        # Masking\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "        \n",
    "        # Append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # Apply blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x, mask, ids_restore\n",
    "    \n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # Embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "        \n",
    "        # Append mask tokens\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)\n",
    "        \n",
    "        # Add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "        \n",
    "        # Apply blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "        \n",
    "        # Predictor\n",
    "        x = self.decoder_pred(x)\n",
    "        x = x[:, 1:, :]  # Remove cls token\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"Compute reconstruction loss\"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        \n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "        \n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)\n",
    "        loss = (loss * mask).sum() / mask.sum()\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, imgs, mask_ratio=0.75):\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore)\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask\n",
    "\n",
    "print(\"üß™ TESTING MPS COMPATIBILITY\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Quick test to ensure it works\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Creating test model...\")\n",
    "    test_mae = MPSOptimizedMAE().to(DEVICE)\n",
    "    \n",
    "    print(\"Testing forward pass...\")\n",
    "    test_input = torch.randn(1, 3, 224, 224).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss, pred, mask = test_mae(test_input, mask_ratio=0.75)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"‚úÖ MPS-optimized MAE test successful!\")\n",
    "    print(f\"‚è±Ô∏è  Test completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"üìä Loss: {loss.item():.4f}\")\n",
    "    print(f\"üìä Prediction shape: {pred.shape}\")\n",
    "    print(f\"üìä Mask shape: {mask.shape}\")\n",
    "    \n",
    "    # Clean up test\n",
    "    del test_mae, test_input, loss, pred, mask\n",
    "    if DEVICE.type == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    print(\"‚úÖ Memory cleaned up\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå MPS test failed: {e}\")\n",
    "    print(\"üîß Consider reducing model size further\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\nüé≠ MPS-OPTIMIZED MAE READY!\")\n",
    "print(\"üí° This should run much faster than the original implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d67c0d",
   "metadata": {},
   "source": [
    "## üö® **IMMEDIATE ACTION: Cell 14 Running Too Long**\n",
    "\n",
    "**If the previous cell is still running after 15+ minutes:**\n",
    "\n",
    "### **üõë STOP THE CELL RIGHT NOW:**\n",
    "1. Click the **‚èπÔ∏è Stop** button next to the running cell\n",
    "2. Or press **Ctrl+C** in the terminal\n",
    "3. Or interrupt the kernel: **Kernel ‚Üí Interrupt**\n",
    "\n",
    "### **‚ö° WHAT WAS WRONG:**\n",
    "- Original MAE implementation was too complex for MPS (Apple Silicon)\n",
    "- Heavy tensor operations causing memory pressure\n",
    "- Xavier weight initialization taking too long\n",
    "- Complex attention mechanisms not optimized for MPS\n",
    "\n",
    "### **‚úÖ NEW OPTIMIZED VERSION:**\n",
    "- **Smaller model**: 384d instead of 512d embeddings\n",
    "- **Fewer layers**: 6 encoder + 4 decoder layers\n",
    "- **MPS-friendly operations**: Optimized for Apple Silicon\n",
    "- **Faster initialization**: Simplified weight init\n",
    "- **Expected runtime**: 30-60 seconds (not 15+ minutes!)\n",
    "\n",
    "### **üîß IF STILL HAVING ISSUES:**\n",
    "```python\n",
    "# Emergency fallback - minimal MAE for testing\n",
    "class MinimalMAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 16, 16),  # Patch embedding\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*196, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Linear(256, 3*224*224)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded).view(-1, 3, 224, 224)\n",
    "        loss = F.mse_loss(decoded, x)\n",
    "        return loss, decoded, torch.ones(x.shape[0], 196)\n",
    "\n",
    "# Use this if the optimized version still fails\n",
    "```\n",
    "\n",
    "**The new implementation should complete in under 1 minute!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "853e3f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è DEFINING COMPLETE MAE ARCHITECTURE\n",
      "==================================================\n",
      "üìã This cell defines all MAE components needed for ViT-Base\n",
      "üîß Classes: PatchEmbed, Attention, MLP, TransformerBlock, MAEEncoder, MAEDecoder, MaskedAutoEncoder\n",
      "‚úÖ Complete MAE Architecture Defined!\n",
      "üèóÔ∏è Components Created:\n",
      "   ‚úÖ PatchEmbed - Image to patch conversion\n",
      "   ‚úÖ Attention - Multi-head self-attention\n",
      "   ‚úÖ MLP - Feed-forward network\n",
      "   ‚úÖ TransformerBlock - Complete transformer layer\n",
      "   ‚úÖ MAEEncoder - ViT encoder with masking\n",
      "   ‚úÖ MAEDecoder - Lightweight decoder\n",
      "   ‚úÖ MaskedAutoEncoder - Complete MAE model\n",
      "\n",
      "üöÄ Ready to create ViT-Base or ViT-Small models!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# üèóÔ∏è Define Complete MAE Architecture (Required for ViT-Base)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "print(\"üèóÔ∏è DEFINING COMPLETE MAE ARCHITECTURE\")\n",
    "print(\"=\"*50)\n",
    "print(\"üìã This cell defines all MAE components needed for ViT-Base\")\n",
    "print(\"üîß Classes: PatchEmbed, Attention, MLP, TransformerBlock, MAEEncoder, MAEDecoder, MaskedAutoEncoder\")\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Image to Patch Embedding\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class MAEEncoder(nn.Module):\n",
    "    \"\"\"MAE Encoder\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, 3, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, qkv_bias=True)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.initialize_weights()\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        # Initialize positional embedding\n",
    "        torch.nn.init.normal_(self.pos_embed, std=.02)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        \n",
    "        # Initialize patch_embed like nn.Linear\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "        \n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"Random masking for MAE\"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # Sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # Keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # Generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward(self, x, mask_ratio):\n",
    "        # Embed patches\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        \n",
    "        # Masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "        \n",
    "        # Append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # Apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "class MAEDecoder(nn.Module):\n",
    "    \"\"\"MAE Decoder\"\"\"\n",
    "    def __init__(self, num_patches=196, encoder_embed_dim=768, decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16, mlp_ratio=4.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder_embed = nn.Linear(encoder_embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            TransformerBlock(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True)\n",
    "            for _ in range(decoder_depth)\n",
    "        ])\n",
    "\n",
    "        self.decoder_norm = nn.LayerNorm(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, 16**2 * 3, bias=True)  # patch_size=16\n",
    "        \n",
    "        self.initialize_weights()\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        # Initialize positional embedding\n",
    "        torch.nn.init.normal_(self.decoder_pos_embed, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "    def forward(self, x, ids_restore):\n",
    "        # Embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # Append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # Add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # Apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # Predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # Remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "class MaskedAutoEncoder(nn.Module):\n",
    "    \"\"\"Complete Masked Autoencoder for ViT-Base and ViT-Small\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 patch_size=16,\n",
    "                 encoder_embed_dim=768,   # 768 for ViT-Base, 384 for ViT-Small\n",
    "                 encoder_depth=12,        # 12 for ViT-Base, 6 for ViT-Small  \n",
    "                 encoder_num_heads=12,    # 12 for ViT-Base, 6 for ViT-Small\n",
    "                 decoder_embed_dim=512,   # Decoder embedding dimension\n",
    "                 decoder_depth=8,         # Decoder depth\n",
    "                 decoder_num_heads=16,    # Decoder attention heads\n",
    "                 mlp_ratio=4.0,\n",
    "                 norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "        \n",
    "        # MAE encoder\n",
    "        self.encoder = MAEEncoder(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            embed_dim=encoder_embed_dim,\n",
    "            depth=encoder_depth,\n",
    "            num_heads=encoder_num_heads,\n",
    "            mlp_ratio=mlp_ratio\n",
    "        )\n",
    "        \n",
    "        # MAE decoder\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.decoder = MAEDecoder(\n",
    "            num_patches=num_patches,\n",
    "            encoder_embed_dim=encoder_embed_dim,\n",
    "            decoder_embed_dim=decoder_embed_dim,\n",
    "            decoder_depth=decoder_depth,\n",
    "            decoder_num_heads=decoder_num_heads,\n",
    "            mlp_ratio=mlp_ratio\n",
    "        )\n",
    "    \n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"Convert images to patches\"\"\"\n",
    "        p = self.patch_size\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x\n",
    "    \n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"Convert patches back to images\"\"\"\n",
    "        p = self.patch_size\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs\n",
    "    \n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"Compute reconstruction loss\"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "        \n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "        \n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, imgs, mask_ratio=0.75):\n",
    "        latent, mask, ids_restore = self.encoder(imgs, mask_ratio)\n",
    "        pred = self.decoder(latent, ids_restore)\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask, latent\n",
    "\n",
    "print(\"‚úÖ Complete MAE Architecture Defined!\")\n",
    "print(\"üèóÔ∏è Components Created:\")\n",
    "print(\"   ‚úÖ PatchEmbed - Image to patch conversion\")\n",
    "print(\"   ‚úÖ Attention - Multi-head self-attention\")  \n",
    "print(\"   ‚úÖ MLP - Feed-forward network\")\n",
    "print(\"   ‚úÖ TransformerBlock - Complete transformer layer\")\n",
    "print(\"   ‚úÖ MAEEncoder - ViT encoder with masking\")\n",
    "print(\"   ‚úÖ MAEDecoder - Lightweight decoder\")\n",
    "print(\"   ‚úÖ MaskedAutoEncoder - Complete MAE model\")\n",
    "print(\"\\nüöÄ Ready to create ViT-Base or ViT-Small models!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5492d96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CONFIGURING MAE FOR ViT-BASE\n",
      "==================================================\n",
      "üéØ Switching from ViT-Small (384-dim) to ViT-Base (768-dim)\n",
      "üìä This will provide better feature representation but require more memory\n",
      "‚úÖ Created MAE directories for ViT-Base:\n",
      "   üìÅ Checkpoints: mae_base_checkpoints\n",
      "   üìÅ Backups: mae_base_backups\n",
      "   üìÅ Logs: mae_base_logs\n",
      "\n",
      "üìä CONFIGURATION COMPARISON:\n",
      "--------------------------------------------------\n",
      "Aspect               ViT-Small (384) ViT-Base (768)\n",
      "--------------------------------------------------\n",
      "Embed Dimension      384             768\n",
      "Encoder Depth        6               12\n",
      "Encoder Heads        6               12\n",
      "Batch Size           16              12\n",
      "Parameters           ~22M            ~86M\n",
      "Memory Usage         ~4GB            ~6-8GB\n",
      "\n",
      "üéØ ViT-Base Benefits:\n",
      "‚úÖ Better feature representation (768-dim vs 384-dim)\n",
      "‚úÖ More model capacity (86M vs 22M parameters)\n",
      "‚úÖ Compatible with standard ViT architectures\n",
      "‚úÖ Better transfer learning potential\n",
      "\n",
      "‚ö†Ô∏è  ViT-Base Considerations:\n",
      "üìà Higher memory usage (~6-8GB vs ~4GB)\n",
      "‚è±Ô∏è  Longer training time per epoch\n",
      "üíæ Larger checkpoint files (~350MB vs ~90MB)\n",
      "\n",
      "‚úÖ Updated MAE_CONFIG for ViT-Base training\n",
      "üöÄ Ready to create ViT-Base MAE model!\n"
     ]
    }
   ],
   "source": [
    "# üîÑ Configure MAE for ViT-Base (768-dimensional embeddings)\n",
    "print(\"üîß CONFIGURING MAE FOR ViT-BASE\")\n",
    "print(\"=\"*50)\n",
    "print(\"üéØ Switching from ViT-Small (384-dim) to ViT-Base (768-dim)\")\n",
    "print(\"üìä This will provide better feature representation but require more memory\")\n",
    "\n",
    "# Update MAE configuration for ViT-Base\n",
    "MAE_CONFIG_BASE = {\n",
    "    # Model architecture (ViT-Base configuration)\n",
    "    'mask_ratio': 0.75,\n",
    "    'img_size': 224,\n",
    "    'patch_size': 16,\n",
    "    \n",
    "    # Training parameters (adjusted for ViT-Base)\n",
    "    'epochs': 50,\n",
    "    'batch_size': 12,  # Reduced from 16 due to larger model\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.05,\n",
    "    'warmup_epochs': 5,\n",
    "    \n",
    "    # Optimizer settings\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'clip_grad': 1.0,\n",
    "    \n",
    "    # Saving and logging\n",
    "    'save_frequency': 5,\n",
    "    'save_every_n_batches': 100,\n",
    "    'log_frequency': 50,\n",
    "    'num_workers': 0,  # For MPS compatibility\n",
    "    \n",
    "    # Directory paths for ViT-Base checkpoints\n",
    "    'checkpoint_dir': 'mae_base_checkpoints',\n",
    "    'backup_dir': 'mae_base_backups', \n",
    "    'logs_dir': 'mae_base_logs',\n",
    "    'use_wandb': False,\n",
    "    'resume_checkpoint': None,\n",
    "    'auto_resume': False,\n",
    "}\n",
    "\n",
    "# Create directories for ViT-Base MAE\n",
    "os.makedirs(MAE_CONFIG_BASE['checkpoint_dir'], exist_ok=True)\n",
    "os.makedirs(MAE_CONFIG_BASE['backup_dir'], exist_ok=True)\n",
    "os.makedirs(MAE_CONFIG_BASE['logs_dir'], exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Created MAE directories for ViT-Base:\")\n",
    "print(f\"   üìÅ Checkpoints: {MAE_CONFIG_BASE['checkpoint_dir']}\")\n",
    "print(f\"   üìÅ Backups: {MAE_CONFIG_BASE['backup_dir']}\")\n",
    "print(f\"   üìÅ Logs: {MAE_CONFIG_BASE['logs_dir']}\")\n",
    "\n",
    "# Compare configurations\n",
    "print(\"\\nüìä CONFIGURATION COMPARISON:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'Aspect':<20} {'ViT-Small (384)':<15} {'ViT-Base (768)'}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'Embed Dimension':<20} {'384':<15} {'768'}\")\n",
    "print(f\"{'Encoder Depth':<20} {'6':<15} {'12'}\")  \n",
    "print(f\"{'Encoder Heads':<20} {'6':<15} {'12'}\")\n",
    "print(f\"{'Batch Size':<20} {'16':<15} {MAE_CONFIG_BASE['batch_size']}\")\n",
    "print(f\"{'Parameters':<20} {'~22M':<15} {'~86M'}\")\n",
    "print(f\"{'Memory Usage':<20} {'~4GB':<15} {'~6-8GB'}\")\n",
    "\n",
    "print(\"\\nüéØ ViT-Base Benefits:\")\n",
    "print(\"‚úÖ Better feature representation (768-dim vs 384-dim)\")\n",
    "print(\"‚úÖ More model capacity (86M vs 22M parameters)\")\n",
    "print(\"‚úÖ Compatible with standard ViT architectures\")\n",
    "print(\"‚úÖ Better transfer learning potential\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  ViT-Base Considerations:\")  \n",
    "print(\"üìà Higher memory usage (~6-8GB vs ~4GB)\")\n",
    "print(\"‚è±Ô∏è  Longer training time per epoch\")\n",
    "print(\"üíæ Larger checkpoint files (~350MB vs ~90MB)\")\n",
    "\n",
    "# Update global MAE_CONFIG to use ViT-Base\n",
    "MAE_CONFIG = MAE_CONFIG_BASE.copy()\n",
    "print(f\"\\n‚úÖ Updated MAE_CONFIG for ViT-Base training\")\n",
    "print(f\"üöÄ Ready to create ViT-Base MAE model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb938af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "501135d3",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Section 5: Configure MAE Pretraining Parameters\n",
    "\n",
    "Setting up optimal hyperparameters for self-supervised MAE pretraining on fish images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98eec2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è CONFIGURING MAE PRETRAINING FOR LOCAL EXECUTION\n",
      "=======================================================\n",
      "üìä MPS-Optimized MAE Configuration:\n",
      "  mask_ratio: 0.75\n",
      "  img_size: 224\n",
      "  patch_size: 16\n",
      "  epochs: 50\n",
      "  batch_size: 16\n",
      "  learning_rate: 0.0001\n",
      "  weight_decay: 0.05\n",
      "  warmup_epochs: 5\n",
      "  beta1: 0.9\n",
      "  beta2: 0.95\n",
      "  clip_grad: 1.0\n",
      "  save_frequency: 5\n",
      "  save_every_n_batches: 100\n",
      "  checkpoint_dir: /Users/catalinathomson/Desktop/Fish/ViT-FishID/mae_checkpoints\n",
      "  backup_dir: /Users/catalinathomson/Desktop/Fish/ViT-FishID/mae_backups\n",
      "  logs_dir: /Users/catalinathomson/Desktop/Fish/ViT-FishID/mae_logs\n",
      "  auto_resume: True\n",
      "  use_wandb: False\n",
      "  log_frequency: 50\n",
      "  data_dir: /Users/catalinathomson/Desktop/Fish/ViT-FishID/fish_cutouts/unlabeled\n",
      "  num_workers: 2\n",
      "  device: mps\n",
      "  mixed_precision: False\n",
      "  pin_memory: True\n",
      "  mps_optimizations: True\n",
      "\n",
      "üíæ Creating local directories...\n",
      "   ‚úÖ Checkpoints: /Users/catalinathomson/Desktop/Fish/ViT-FishID/mae_checkpoints\n",
      "   ‚úÖ Backups: /Users/catalinathomson/Desktop/Fish/ViT-FishID/mae_backups\n",
      "   ‚úÖ Logs: /Users/catalinathomson/Desktop/Fish/ViT-FishID/mae_logs\n",
      "\n",
      "üìÇ Verifying unlabeled data directory...\n",
      "   ‚úÖ Directory exists: /Users/catalinathomson/Desktop/Fish/ViT-FishID/fish_cutouts/unlabeled\n",
      "   ‚úÖ Contains image files (verified sample: 100/100)\n",
      "   üìä Using known count: ~24,015 unlabeled images\n",
      "\n",
      "üçé Apple Silicon MPS Optimizations:\n",
      "   Batch size: 16 (optimized for MPS memory)\n",
      "   Mixed precision: disabled (not supported on MPS)\n",
      "   Workers: 2 (reduced for local execution)\n",
      "\n",
      "‚úÖ MAE Configuration complete for mps!\n",
      "üéØ Ready to start pretraining with ~24,015 unlabeled fish images\n",
      "‚ö° Configuration completed in seconds (optimized for large datasets)\n"
     ]
    }
   ],
   "source": [
    "# Configure MAE pretraining with local paths and MPS optimization\n",
    "print(\"‚öôÔ∏è CONFIGURING MAE PRETRAINING FOR LOCAL EXECUTION\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Create local directories for MAE training\n",
    "mae_checkpoint_dir = os.path.join(project_root, \"mae_checkpoints\")\n",
    "mae_backup_dir = os.path.join(project_root, \"mae_backups\") \n",
    "mae_logs_dir = os.path.join(project_root, \"mae_logs\")\n",
    "\n",
    "# MAE Configuration optimized for Apple Silicon MPS\n",
    "MAE_CONFIG = {\n",
    "    # Model architecture (optimized for MPS)\n",
    "    'mask_ratio': 0.75,\n",
    "    'img_size': 224,\n",
    "    'patch_size': 16,\n",
    "    \n",
    "    # Training parameters (optimized for local execution)\n",
    "    'epochs': 50,\n",
    "    'batch_size': 16,  # Reduced from 32 for MPS memory efficiency\n",
    "    'learning_rate': 1e-4,  # Match EMA training config\n",
    "    'weight_decay': 0.05,\n",
    "    'warmup_epochs': 5,\n",
    "    \n",
    "    # Optimizer settings\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.95,\n",
    "    'clip_grad': 1.0,\n",
    "    \n",
    "    # Saving and logging (local paths)\n",
    "    'save_frequency': 5,\n",
    "    'save_every_n_batches': 100,\n",
    "    'checkpoint_dir': mae_checkpoint_dir,\n",
    "    'backup_dir': mae_backup_dir,\n",
    "    'logs_dir': mae_logs_dir,\n",
    "    'auto_resume': True,\n",
    "    \n",
    "    # Logging settings (disabled wandb for local)\n",
    "    'use_wandb': False,  # Disable for local execution\n",
    "    'log_frequency': 50,\n",
    "    \n",
    "    # Data settings (local paths)\n",
    "    'data_dir': UNLABELED_DIR,\n",
    "    'num_workers': 2,  # Reduced for local execution\n",
    "    \n",
    "    # Device settings\n",
    "    'device': str(DEVICE),\n",
    "    'mixed_precision': DEVICE.type == 'cuda',  # Only use for CUDA\n",
    "    'pin_memory': True,\n",
    "    \n",
    "    # MPS specific optimizations\n",
    "    'mps_optimizations': DEVICE.type == 'mps'\n",
    "}\n",
    "\n",
    "print(\"üìä MPS-Optimized MAE Configuration:\")\n",
    "for key, value in MAE_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create checkpoint and backup directories locally\n",
    "print(\"\\nüíæ Creating local directories...\")\n",
    "os.makedirs(MAE_CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "os.makedirs(MAE_CONFIG['backup_dir'], exist_ok=True)\n",
    "os.makedirs(MAE_CONFIG['logs_dir'], exist_ok=True)\n",
    "\n",
    "print(f\"   ‚úÖ Checkpoints: {MAE_CONFIG['checkpoint_dir']}\")\n",
    "print(f\"   ‚úÖ Backups: {MAE_CONFIG['backup_dir']}\")\n",
    "print(f\"   ‚úÖ Logs: {MAE_CONFIG['logs_dir']}\")\n",
    "\n",
    "# Quick verification of data directory (without counting all files)\n",
    "print(f\"\\nüìÇ Verifying unlabeled data directory...\")\n",
    "if os.path.exists(MAE_CONFIG['data_dir']):\n",
    "    print(f\"   ‚úÖ Directory exists: {MAE_CONFIG['data_dir']}\")\n",
    "    \n",
    "    # Quick sample check instead of counting all 24K+ files\n",
    "    sample_files = os.listdir(MAE_CONFIG['data_dir'])[:100]  # Just check first 100\n",
    "    image_files = [f for f in sample_files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    if len(image_files) > 0:\n",
    "        print(f\"   ‚úÖ Contains image files (verified sample: {len(image_files)}/100)\")\n",
    "        print(f\"   üìä Using known count: ~24,015 unlabeled images\")\n",
    "        unlabeled_count = 24015  # Use known count instead of scanning all files\n",
    "    else:\n",
    "        print(f\"   ‚ùå No image files found in sample\")\n",
    "        raise FileNotFoundError(f\"No images in directory: {MAE_CONFIG['data_dir']}\")\n",
    "else:\n",
    "    print(f\"‚ùå Unlabeled data directory not found: {MAE_CONFIG['data_dir']}\")\n",
    "    raise FileNotFoundError(f\"Missing unlabeled data: {MAE_CONFIG['data_dir']}\")\n",
    "\n",
    "# Memory optimization settings for MPS\n",
    "if DEVICE.type == 'mps':\n",
    "    print(f\"\\nüçé Apple Silicon MPS Optimizations:\")\n",
    "    print(f\"   Batch size: {MAE_CONFIG['batch_size']} (optimized for MPS memory)\")\n",
    "    print(f\"   Mixed precision: disabled (not supported on MPS)\")\n",
    "    print(f\"   Workers: {MAE_CONFIG['num_workers']} (reduced for local execution)\")\n",
    "elif DEVICE.type == 'cuda':\n",
    "    print(f\"\\nüöÄ CUDA GPU Optimizations:\")\n",
    "    print(f\"   Mixed precision: {MAE_CONFIG['mixed_precision']}\")\n",
    "    print(f\"   Pin memory: {MAE_CONFIG['pin_memory']}\")\n",
    "else:\n",
    "    print(f\"\\nüíª CPU Configuration:\")\n",
    "    print(f\"   Mixed precision: disabled\")\n",
    "    print(f\"   Pin memory: disabled\")\n",
    "\n",
    "print(f\"\\n‚úÖ MAE Configuration complete for {DEVICE}!\")\n",
    "print(f\"üéØ Ready to start pretraining with ~{unlabeled_count:,} unlabeled fish images\")\n",
    "print(f\"‚ö° Configuration completed in seconds (optimized for large datasets)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64267b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è CREATING ViT-BASE MAE MODEL\n",
      "==================================================\n",
      "‚úÖ Cleared previous models from memory\n",
      "üöÄ Creating ViT-Base MAE model...\n",
      "‚úÖ ViT-Base MAE model created successfully!\n",
      "üìä Model Statistics:\n",
      "   Total parameters: 111,907,840\n",
      "   Trainable parameters: 111,907,840\n",
      "   Model size: ~426.9 MB\n",
      "\n",
      "üß™ Testing model forward pass...\n",
      "‚úÖ ViT-Base MAE model created successfully!\n",
      "üìä Model Statistics:\n",
      "   Total parameters: 111,907,840\n",
      "   Trainable parameters: 111,907,840\n",
      "   Model size: ~426.9 MB\n",
      "\n",
      "üß™ Testing model forward pass...\n",
      "‚úÖ Forward pass successful!\n",
      "   Input shape: torch.Size([2, 3, 224, 224])\n",
      "   Loss: 1.3527\n",
      "   Prediction shape: torch.Size([2, 196, 768])\n",
      "   Latent shape: torch.Size([2, 50, 768])\n",
      "\n",
      "‚öôÔ∏è Setting up ViT-Base optimizer and scheduler...\n",
      "‚úÖ Optimizer and scheduler created!\n",
      "   Learning rate: 0.0001\n",
      "   Weight decay: 0.05\n",
      "   Betas: (0.9, 0.95)\n",
      "\n",
      "üéâ ViT-BASE MAE SETUP COMPLETE!\n",
      "==================================================\n",
      "‚úÖ Model: ViT-Base MAE (768-dim embeddings)\n",
      "‚úÖ Parameters: 111,907,840\n",
      "‚úÖ Batch size: 12\n",
      "‚úÖ Device: mps\n",
      "üöÄ Ready for ViT-Base MAE pretraining!\n",
      "\n",
      "üíæ Estimated Memory Usage:\n",
      "   Model: ~426.9 MB\n",
      "   Batch (12 images): ~6.9 MB\n",
      "   Gradients: ~426.9 MB\n",
      "   Total estimated: ~860.7 MB\n",
      "\n",
      "üçé MPS Optimization Tips:\n",
      "   - Using batch size 12 (reduced from 16)\n",
      "   - num_workers=0 for compatibility\n",
      "   - Automatic memory management enabled\n",
      "‚úÖ Forward pass successful!\n",
      "   Input shape: torch.Size([2, 3, 224, 224])\n",
      "   Loss: 1.3527\n",
      "   Prediction shape: torch.Size([2, 196, 768])\n",
      "   Latent shape: torch.Size([2, 50, 768])\n",
      "\n",
      "‚öôÔ∏è Setting up ViT-Base optimizer and scheduler...\n",
      "‚úÖ Optimizer and scheduler created!\n",
      "   Learning rate: 0.0001\n",
      "   Weight decay: 0.05\n",
      "   Betas: (0.9, 0.95)\n",
      "\n",
      "üéâ ViT-BASE MAE SETUP COMPLETE!\n",
      "==================================================\n",
      "‚úÖ Model: ViT-Base MAE (768-dim embeddings)\n",
      "‚úÖ Parameters: 111,907,840\n",
      "‚úÖ Batch size: 12\n",
      "‚úÖ Device: mps\n",
      "üöÄ Ready for ViT-Base MAE pretraining!\n",
      "\n",
      "üíæ Estimated Memory Usage:\n",
      "   Model: ~426.9 MB\n",
      "   Batch (12 images): ~6.9 MB\n",
      "   Gradients: ~426.9 MB\n",
      "   Total estimated: ~860.7 MB\n",
      "\n",
      "üçé MPS Optimization Tips:\n",
      "   - Using batch size 12 (reduced from 16)\n",
      "   - num_workers=0 for compatibility\n",
      "   - Automatic memory management enabled\n"
     ]
    }
   ],
   "source": [
    "# üèóÔ∏è Create ViT-Base MAE Model (768-dimensional embeddings)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"üèóÔ∏è CREATING ViT-BASE MAE MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Clear previous models from memory if they exist\n",
    "if 'mae_model' in globals():\n",
    "    del mae_model\n",
    "if 'mae_optimizer' in globals():\n",
    "    del mae_optimizer\n",
    "if 'mae_scheduler' in globals():\n",
    "    del mae_scheduler\n",
    "\n",
    "# Clear GPU memory\n",
    "if DEVICE.type == 'mps':\n",
    "    torch.mps.empty_cache()\n",
    "elif DEVICE.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "print(\"‚úÖ Cleared previous models from memory\")\n",
    "\n",
    "# Create ViT-Base MAE model with 768-dimensional embeddings\n",
    "print(\"üöÄ Creating ViT-Base MAE model...\")\n",
    "\n",
    "try:\n",
    "    mae_model_base = MaskedAutoEncoder(\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        encoder_embed_dim=768,  # ViT-Base: 768-dimensional embeddings\n",
    "        encoder_depth=12,       # ViT-Base: 12 transformer layers\n",
    "        encoder_num_heads=12,   # ViT-Base: 12 attention heads\n",
    "        decoder_embed_dim=512,  # Larger decoder for ViT-Base\n",
    "        decoder_depth=8,        # Deeper decoder for ViT-Base\n",
    "        decoder_num_heads=16,   # More decoder heads\n",
    "        mlp_ratio=4.0,\n",
    "        norm_pix_loss=True\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(\"‚úÖ ViT-Base MAE model created successfully!\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in mae_model_base.parameters())\n",
    "    trainable_params = sum(p.numel() for p in mae_model_base.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"üìä Model Statistics:\")\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   Model size: ~{total_params * 4 / (1024**2):.1f} MB\")\n",
    "    \n",
    "    # Test forward pass to verify model works\n",
    "    print(\"\\nüß™ Testing model forward pass...\")\n",
    "    test_input = torch.randn(2, 3, 224, 224).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss, pred, mask, latent = mae_model_base(test_input, mask_ratio=0.75)\n",
    "    \n",
    "    print(f\"‚úÖ Forward pass successful!\")\n",
    "    print(f\"   Input shape: {test_input.shape}\")\n",
    "    print(f\"   Loss: {loss.item():.4f}\")\n",
    "    print(f\"   Prediction shape: {pred.shape}\")\n",
    "    print(f\"   Latent shape: {latent.shape}\")\n",
    "    \n",
    "    # Clean up test tensor\n",
    "    del test_input, loss, pred, mask, latent\n",
    "    if DEVICE.type == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating ViT-Base MAE model: {e}\")\n",
    "    print(\"üí° This might be due to insufficient memory\")\n",
    "    print(\"   Try reducing batch size or using ViT-Small instead\")\n",
    "    raise\n",
    "\n",
    "# Create optimizer for ViT-Base model\n",
    "print(\"\\n‚öôÔ∏è Setting up ViT-Base optimizer and scheduler...\")\n",
    "\n",
    "try:\n",
    "    mae_optimizer_base = torch.optim.AdamW(\n",
    "        mae_model_base.parameters(),\n",
    "        lr=MAE_CONFIG['learning_rate'],\n",
    "        weight_decay=MAE_CONFIG['weight_decay'],\n",
    "        betas=(MAE_CONFIG['beta1'], MAE_CONFIG['beta2'])\n",
    "    )\n",
    "    \n",
    "    mae_scheduler_base = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        mae_optimizer_base,\n",
    "        T_max=MAE_CONFIG['epochs'],\n",
    "        eta_min=MAE_CONFIG['learning_rate'] * 0.01\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Optimizer and scheduler created!\")\n",
    "    print(f\"   Learning rate: {MAE_CONFIG['learning_rate']}\")\n",
    "    print(f\"   Weight decay: {MAE_CONFIG['weight_decay']}\")\n",
    "    print(f\"   Betas: ({MAE_CONFIG['beta1']}, {MAE_CONFIG['beta2']})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating optimizer: {e}\")\n",
    "    raise\n",
    "\n",
    "# Set global variables for training\n",
    "mae_model = mae_model_base\n",
    "mae_optimizer = mae_optimizer_base\n",
    "mae_scheduler = mae_scheduler_base\n",
    "\n",
    "print(\"\\nüéâ ViT-BASE MAE SETUP COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"‚úÖ Model: ViT-Base MAE (768-dim embeddings)\")\n",
    "print(f\"‚úÖ Parameters: {total_params:,}\")\n",
    "print(f\"‚úÖ Batch size: {MAE_CONFIG['batch_size']}\")\n",
    "print(f\"‚úÖ Device: {DEVICE}\")\n",
    "print(\"üöÄ Ready for ViT-Base MAE pretraining!\")\n",
    "\n",
    "# Memory usage estimation\n",
    "print(f\"\\nüíæ Estimated Memory Usage:\")\n",
    "print(f\"   Model: ~{total_params * 4 / (1024**2):.1f} MB\")\n",
    "print(f\"   Batch ({MAE_CONFIG['batch_size']} images): ~{MAE_CONFIG['batch_size'] * 3 * 224 * 224 * 4 / (1024**2):.1f} MB\")\n",
    "print(f\"   Gradients: ~{total_params * 4 / (1024**2):.1f} MB\") \n",
    "print(f\"   Total estimated: ~{(total_params * 8 + MAE_CONFIG['batch_size'] * 3 * 224 * 224 * 4) / (1024**2):.1f} MB\")\n",
    "\n",
    "if DEVICE.type == 'mps':\n",
    "    print(\"\\nüçé MPS Optimization Tips:\")\n",
    "    print(\"   - Using batch size 12 (reduced from 16)\")\n",
    "    print(\"   - num_workers=0 for compatibility\")\n",
    "    print(\"   - Automatic memory management enabled\")\n",
    "elif DEVICE.type == 'cuda':\n",
    "    print(f\"\\nüî• GPU Memory Available: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b1c8d1",
   "metadata": {},
   "source": [
    "## üíæ **MAE Checkpointing & Recovery Options**\n",
    "\n",
    "### üîÑ **Automatic Checkpoint Features:**\n",
    "- ‚úÖ **Saves every 5 epochs** (instead of 10) to `/content/drive/MyDrive/ViT-FishID/mae_checkpoints/`\n",
    "- ‚úÖ **Saves every 100 batches** for ultra-frequent recovery points\n",
    "- ‚úÖ **Dual backup system** - saves to both main and backup directories\n",
    "- ‚úÖ **Auto-resume** - automatically detects and resumes from latest checkpoint\n",
    "- ‚úÖ **Best model tracking** - always keeps the best performing model\n",
    "- ‚úÖ **Training metadata** - JSON file tracks progress and timestamps\n",
    "\n",
    "### üìÅ **Checkpoint Files Created:**\n",
    "```\n",
    "/content/drive/MyDrive/ViT-FishID/mae_checkpoints/\n",
    "‚îú‚îÄ‚îÄ mae_checkpoint_epoch_5.pth      # Every 5 epochs\n",
    "‚îú‚îÄ‚îÄ mae_checkpoint_epoch_10.pth\n",
    "‚îú‚îÄ‚îÄ mae_checkpoint_batch_100.pth    # Every 100 batches  \n",
    "‚îú‚îÄ‚îÄ mae_checkpoint_batch_200.pth\n",
    "‚îú‚îÄ‚îÄ mae_best_model.pth              # Best loss so far\n",
    "‚îú‚îÄ‚îÄ mae_final_model.pth             # Final trained model\n",
    "‚îî‚îÄ‚îÄ training_metadata.json          # Progress tracking\n",
    "```\n",
    "\n",
    "### üö® **If Training Gets Interrupted:**\n",
    "\n",
    "**Option 1: Automatic Resume (Recommended)**\n",
    "- Just re-run the MAE training cell\n",
    "- It automatically detects the latest checkpoint\n",
    "- Continues from where it left off\n",
    "\n",
    "**Option 2: Manual Resume**\n",
    "```python\n",
    "# Set specific checkpoint to resume from\n",
    "MAE_CONFIG['resume_checkpoint'] = '/content/drive/MyDrive/ViT-FishID/mae_checkpoints/mae_checkpoint_epoch_15.pth'\n",
    "MAE_CONFIG['auto_resume'] = True\n",
    "# Then run the training cell\n",
    "```\n",
    "\n",
    "**Option 3: Check Training Status**\n",
    "```python\n",
    "# Check what checkpoints exist\n",
    "import os\n",
    "checkpoint_dir = '/content/drive/MyDrive/ViT-FishID/mae_checkpoints/'\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    files = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')])\n",
    "    for f in files:\n",
    "        print(f\"üìÅ {f}\")\n",
    "```\n",
    "\n",
    "### ‚è±Ô∏è **Recovery Time Estimates:**\n",
    "- **From epoch checkpoint**: ~30 seconds to resume\n",
    "- **From batch checkpoint**: ~30 seconds to resume  \n",
    "- **Maximum lost progress**: ~100 batches (5-10 minutes of training)\n",
    "\n",
    "### üéØ **Success Indicators:**\n",
    "Look for these messages during training:\n",
    "- `üíæ Checkpoint saved: mae_checkpoint_epoch_X.pth`\n",
    "- `üèÜ New best model saved! Loss: X.XXXX`\n",
    "- `‚úÖ Successfully resumed from epoch X`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a8ebe",
   "metadata": {},
   "source": [
    "## üöÄ Section 6: Execute MAE Pretraining Phase\n",
    "\n",
    "Running self-supervised pretraining to learn robust visual representations from unlabeled fish images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89459b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ CREATING FAST MAE DATASET AND DATALOADER\n",
      "==================================================\n",
      "üñºÔ∏è Fixed MAE Transform Pipeline:\n",
      "  1. Resize to 224x224 (exact size)\n",
      "  2. Convert to tensor\n",
      "  3. ImageNet normalization\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  3. ImageNet normalization\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Create dataset with fast loading\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìÇ Creating fast MAE dataset from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mMAE_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata_dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# Limit to reasonable number for faster testing (remove limit for full training)\u001b[39;00m\n\u001b[32m     90\u001b[39m     mae_dataset = FastMAEDataset(\n\u001b[32m     91\u001b[39m         data_dir=MAE_CONFIG[\u001b[33m'\u001b[39m\u001b[33mdata_dir\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     92\u001b[39m         transform=mae_transform,\n\u001b[32m     93\u001b[39m         max_files=\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Set to 5000 for testing, None for full dataset\u001b[39;00m\n\u001b[32m     94\u001b[39m     )\n",
      "\u001b[31mKeyError\u001b[39m: 'data_dir'"
     ]
    }
   ],
   "source": [
    "# Create Fast MAE Dataset and DataLoader for Pretraining\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üéØ CREATING FAST MAE DATASET AND DATALOADER\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "class FastMAEDataset(Dataset):\n",
    "    \"\"\"Ultra-fast dataset class for MAE pretraining using lazy loading\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, transform=None, max_files=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Use glob for faster file discovery (don't load all files at once)\n",
    "        print(f\"üîç Fast scanning directory: {data_dir}\")\n",
    "        \n",
    "        # Pattern matching for common image extensions\n",
    "        patterns = [\n",
    "            os.path.join(data_dir, '*.jpg'),\n",
    "            os.path.join(data_dir, '*.jpeg'), \n",
    "            os.path.join(data_dir, '*.png'),\n",
    "            os.path.join(data_dir, '*.JPG'),\n",
    "            os.path.join(data_dir, '*.JPEG'),\n",
    "            os.path.join(data_dir, '*.PNG')\n",
    "        ]\n",
    "        \n",
    "        self.image_files = []\n",
    "        for pattern in patterns:\n",
    "            self.image_files.extend(glob.glob(pattern))\n",
    "            if max_files and len(self.image_files) >= max_files:\n",
    "                self.image_files = self.image_files[:max_files]\n",
    "                break\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(self.image_files)} images for MAE pretraining\")\n",
    "        \n",
    "        if len(self.image_files) == 0:\n",
    "            raise ValueError(f\"No valid image files found in {data_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # Apply transforms\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return image\n",
    "        \n",
    "        except Exception as e:\n",
    "            # If image loading fails, return a black image\n",
    "            print(f\"‚ö†Ô∏è Warning: Failed to load image {idx}: {e}\")\n",
    "            if self.transform:\n",
    "                return self.transform(Image.new('RGB', (224, 224), color='black'))\n",
    "            else:\n",
    "                return torch.zeros(3, 224, 224)\n",
    "\n",
    "# Define MAE transforms (fixed size for batch compatibility)\n",
    "mae_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Force exact 224x224 size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "print(f\"üñºÔ∏è Fixed MAE Transform Pipeline:\")\n",
    "print(f\"  1. Resize to 224x224 (exact size)\")\n",
    "print(f\"  2. Convert to tensor\") \n",
    "print(f\"  3. ImageNet normalization\")\n",
    "\n",
    "# Create dataset with fast loading\n",
    "print(f\"\\nüìÇ Creating fast MAE dataset from: {MAE_CONFIG['data_dir']}\")\n",
    "\n",
    "try:\n",
    "    # Limit to reasonable number for faster testing (remove limit for full training)\n",
    "    mae_dataset = FastMAEDataset(\n",
    "        data_dir=MAE_CONFIG['data_dir'],\n",
    "        transform=mae_transform,\n",
    "        max_files=None  # Set to 5000 for testing, None for full dataset\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Fast MAE dataset created!\")\n",
    "    print(f\"üìä Dataset size: {len(mae_dataset):,} unlabeled images\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create MAE dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create DataLoader with MPS optimizations\n",
    "print(f\"\\nüîÑ Creating optimized MAE dataloader...\")\n",
    "\n",
    "dataloader_kwargs = {\n",
    "    'batch_size': MAE_CONFIG['batch_size'],\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0 if DEVICE.type == 'mps' else MAE_CONFIG['num_workers'],  # MPS works better with 0 workers\n",
    "    'pin_memory': False,  # Disable for compatibility\n",
    "    'drop_last': True,\n",
    "    'persistent_workers': False\n",
    "}\n",
    "\n",
    "print(f\"üçé Dataloader optimizations:\")\n",
    "print(f\"   Workers: {dataloader_kwargs['num_workers']} ({'MPS optimized' if DEVICE.type == 'mps' else 'Standard'})\")\n",
    "print(f\"   Pin memory: {dataloader_kwargs['pin_memory']}\")\n",
    "\n",
    "try:\n",
    "    mae_dataloader = DataLoader(mae_dataset, **dataloader_kwargs)\n",
    "    \n",
    "    print(f\"‚úÖ MAE dataloader created!\")\n",
    "    print(f\"üìä Batches per epoch: {len(mae_dataloader)}\")\n",
    "    print(f\"üéØ Batch size: {MAE_CONFIG['batch_size']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create MAE dataloader: {e}\")\n",
    "    raise\n",
    "\n",
    "# Quick test of dataloader\n",
    "print(f\"\\nüß™ Testing MAE dataloader (quick test)...\")\n",
    "\n",
    "try:\n",
    "    # Test loading one batch\n",
    "    test_iter = iter(mae_dataloader)\n",
    "    test_batch = next(test_iter)\n",
    "    \n",
    "    print(f\"‚úÖ Dataloader test successful!\")\n",
    "    print(f\"üìä Batch shape: {test_batch.shape}\")\n",
    "    print(f\"üìä Data type: {test_batch.dtype}\")\n",
    "    print(f\"üìä Value range: [{test_batch.min():.3f}, {test_batch.max():.3f}]\")\n",
    "    \n",
    "    # Test on device\n",
    "    test_batch = test_batch.to(DEVICE)\n",
    "    print(f\"‚úÖ Successfully moved batch to {DEVICE}\")\n",
    "    \n",
    "    # Quick cleanup\n",
    "    del test_batch, test_iter\n",
    "    if DEVICE.type == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Dataloader test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Set up MAE training components (only if model exists)\n",
    "print(f\"\\n‚öôÔ∏è Setting up MAE training components...\")\n",
    "\n",
    "# Check if MAE model exists before creating optimizer\n",
    "if 'mae_model' in globals() and mae_model is not None:\n",
    "    # AdamW optimizer\n",
    "    mae_optimizer = torch.optim.AdamW(\n",
    "        mae_model.parameters(),\n",
    "        lr=MAE_CONFIG['learning_rate'],\n",
    "        weight_decay=MAE_CONFIG['weight_decay'],\n",
    "        betas=(MAE_CONFIG['beta1'], MAE_CONFIG['beta2'])\n",
    "    )\n",
    "\n",
    "    # Simple learning rate scheduler\n",
    "    mae_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        mae_optimizer,\n",
    "        T_max=MAE_CONFIG['epochs'],\n",
    "        eta_min=MAE_CONFIG['learning_rate'] * 0.01\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Training components ready!\")\n",
    "    print(f\"üìä Total batches: {len(mae_dataloader):,}\")\n",
    "    print(f\"üìä Initial LR: {MAE_CONFIG['learning_rate']}\")\n",
    "    print(f\"üìä Weight decay: {MAE_CONFIG['weight_decay']}\")\n",
    "    print(f\"‚úÖ Model: {sum(p.numel() for p in mae_model.parameters()):,} parameters\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  MAE model not found!\")\n",
    "    print(f\"üí° Please run the MAE model creation cell first:\")\n",
    "    print(f\"   - Look for 'Create ViT-Base MAE Model' cell\")\n",
    "    print(f\"   - Or run the ViT-Base MAE configuration cells\")\n",
    "    print(f\"   - Then re-run this cell\")\n",
    "    \n",
    "    # Set placeholders\n",
    "    mae_optimizer = None\n",
    "    mae_scheduler = None\n",
    "\n",
    "# Disable wandb for local execution\n",
    "MAE_CONFIG['use_wandb'] = False\n",
    "print(f\"üìù Wandb logging: disabled (local execution)\")\n",
    "\n",
    "print(f\"\\nüöÄ FAST MAE SETUP COMPLETE!\")\n",
    "print(f\"‚úÖ Dataset: {len(mae_dataset):,} images (fast loading)\")\n",
    "print(f\"‚úÖ Batches: {len(mae_dataloader):,} per epoch\")\n",
    "print(f\"‚úÖ Device: {DEVICE}\")\n",
    "if 'mae_model' in globals() and mae_model is not None:\n",
    "    print(f\"‚úÖ Ready for training!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Model setup incomplete - run model creation cell first!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383172de",
   "metadata": {},
   "source": [
    "## üîÑ Section 7: Load MAE Pretrained Weights for ViT\n",
    "\n",
    "Extracting pretrained encoder weights from MAE and loading them into the ViT classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c131233",
   "metadata": {},
   "source": [
    "## üé≠ Section 6: Execute MAE Pretraining (This Will Take Several Hours)\n",
    "\n",
    "**Warning:** This section will train for 50 epochs on 24,015 unlabeled fish images. Expected runtime: 3-6 hours depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e74b815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≠ RESUMING MAE PRETRAINING FROM EPOCH 29\n",
      "============================================================\n",
      "‚ö†Ô∏è  This will take 3-6 hours to complete!\n",
      "üìä Training 24,015 images for 50 epochs\n",
      "üîÑ Resuming from epoch 29, will train epochs 30-50\n",
      "üéØ Expected time: ~4.4 hours\n",
      "üìÇ Checking checkpoint architecture from epoch 29...\n",
      "‚úÖ Checkpoint loaded for architecture inspection\n",
      "üéØ Detected ViT-Base architecture (embed_dim=768)\n",
      "\n",
      "üèóÔ∏è  CREATING MAE MODEL\n",
      "========================================\n",
      "üéØ Creating ViT-Base MAE model (768-dim, 12 encoder layers, 8 decoder layers)\n",
      "‚úÖ Checkpoint loaded for architecture inspection\n",
      "üéØ Detected ViT-Base architecture (embed_dim=768)\n",
      "\n",
      "üèóÔ∏è  CREATING MAE MODEL\n",
      "========================================\n",
      "üéØ Creating ViT-Base MAE model (768-dim, 12 encoder layers, 8 decoder layers)\n",
      "‚úÖ ViT-Base MAE model created and moved to device\n",
      "‚úÖ MAE optimizer created\n",
      "‚úÖ MAE scheduler created\n",
      "\n",
      "üìÇ Loading checkpoint from epoch 29...\n",
      "‚úÖ ViT-Base MAE model created and moved to device\n",
      "‚úÖ MAE optimizer created\n",
      "‚úÖ MAE scheduler created\n",
      "\n",
      "üìÇ Loading checkpoint from epoch 29...\n",
      "‚úÖ Checkpoint loaded with weights_only=False (trusted source)\n",
      "‚úÖ Successfully loaded checkpoint from epoch 29\n",
      "üìä Checkpoint loss: 0.0225\n",
      "üìÖ Checkpoint timestamp: 2025-08-18T23:10:50.392516\n",
      "üìä Loaded training history: 1 epochs\n",
      "\n",
      "üöÄ INITIALIZING MAE TRAINING\n",
      "========================================\n",
      "‚úÖ Training setup complete!\n",
      "üìä Model parameters: 111,907,840\n",
      "üìä Architecture: ViT-Base\n",
      "üìä Batches per epoch: 1,500\n",
      "üìä Total training steps: 31,500\n",
      "üìä Starting from epoch: 30\n",
      "‚è∞ Start time: 2025-08-19 08:59:27\n",
      "\n",
      "üéØ EPOCH 30/50\n",
      "--------------------------------------------------\n",
      "‚úÖ Checkpoint loaded with weights_only=False (trusted source)\n",
      "‚úÖ Successfully loaded checkpoint from epoch 29\n",
      "üìä Checkpoint loss: 0.0225\n",
      "üìÖ Checkpoint timestamp: 2025-08-18T23:10:50.392516\n",
      "üìä Loaded training history: 1 epochs\n",
      "\n",
      "üöÄ INITIALIZING MAE TRAINING\n",
      "========================================\n",
      "‚úÖ Training setup complete!\n",
      "üìä Model parameters: 111,907,840\n",
      "üìä Architecture: ViT-Base\n",
      "üìä Batches per epoch: 1,500\n",
      "üìä Total training steps: 31,500\n",
      "üìä Starting from epoch: 30\n",
      "‚è∞ Start time: 2025-08-19 08:59:27\n",
      "\n",
      "üéØ EPOCH 30/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:   3%|‚ñè     | 50/1500 [01:04<18:06,  1.33it/s, Loss=0.0110, Avg=0.0239, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 50: Loss=0.0110, Steps/sec=0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:   7%|‚ñé    | 100/1500 [01:41<16:08,  1.45it/s, Loss=0.0409, Avg=0.0242, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 100: Loss=0.0409, Steps/sec=0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  10%|‚ñå    | 150/1500 [02:18<19:05,  1.18it/s, Loss=0.0259, Avg=0.0241, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 150: Loss=0.0259, Steps/sec=1.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  13%|‚ñã    | 200/1500 [02:55<15:07,  1.43it/s, Loss=0.0274, Avg=0.0236, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 200: Loss=0.0274, Steps/sec=1.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  17%|‚ñä    | 250/1500 [03:31<14:08,  1.47it/s, Loss=0.0180, Avg=0.0231, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 250: Loss=0.0180, Steps/sec=1.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  20%|‚ñà    | 300/1500 [04:07<14:55,  1.34it/s, Loss=0.0319, Avg=0.0221, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 300: Loss=0.0319, Steps/sec=1.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  23%|‚ñà‚ñè   | 350/1500 [04:45<15:26,  1.24it/s, Loss=0.0189, Avg=0.0225, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 350: Loss=0.0189, Steps/sec=1.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  27%|‚ñà‚ñé   | 400/1500 [05:23<14:15,  1.29it/s, Loss=0.0490, Avg=0.0222, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 400: Loss=0.0490, Steps/sec=1.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  30%|‚ñà‚ñå   | 450/1500 [06:02<12:43,  1.38it/s, Loss=0.0223, Avg=0.0222, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 450: Loss=0.0223, Steps/sec=1.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  33%|‚ñà‚ñã   | 500/1500 [06:38<11:26,  1.46it/s, Loss=0.0055, Avg=0.0220, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 500: Loss=0.0055, Steps/sec=1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  37%|‚ñà‚ñä   | 550/1500 [07:15<11:07,  1.42it/s, Loss=0.0344, Avg=0.0224, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 550: Loss=0.0344, Steps/sec=1.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  40%|‚ñà‚ñà   | 600/1500 [07:56<14:15,  1.05it/s, Loss=0.0103, Avg=0.0224, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 600: Loss=0.0103, Steps/sec=1.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  43%|‚ñà‚ñà‚ñè  | 650/1500 [08:32<10:56,  1.29it/s, Loss=0.0104, Avg=0.0224, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 650: Loss=0.0104, Steps/sec=1.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  47%|‚ñà‚ñà‚ñé  | 700/1500 [09:09<09:17,  1.43it/s, Loss=0.0111, Avg=0.0227, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 700: Loss=0.0111, Steps/sec=1.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  50%|‚ñà‚ñà‚ñå  | 750/1500 [09:48<11:12,  1.12it/s, Loss=0.0110, Avg=0.0226, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 750: Loss=0.0110, Steps/sec=1.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  53%|‚ñà‚ñà‚ñã  | 800/1500 [10:29<09:08,  1.28it/s, Loss=0.0152, Avg=0.0227, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 800: Loss=0.0152, Steps/sec=1.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  57%|‚ñà‚ñà‚ñä  | 850/1500 [11:08<09:19,  1.16it/s, Loss=0.0240, Avg=0.0227, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 850: Loss=0.0240, Steps/sec=1.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  60%|‚ñà‚ñà‚ñà  | 900/1500 [11:53<09:35,  1.04it/s, Loss=0.0406, Avg=0.0228, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 900: Loss=0.0406, Steps/sec=1.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  63%|‚ñà‚ñà‚ñà‚ñè | 950/1500 [12:36<08:53,  1.03it/s, Loss=0.0392, Avg=0.0228, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 950: Loss=0.0392, Steps/sec=1.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  67%|‚ñà‚ñà‚ñã | 1000/1500 [13:21<07:35,  1.10it/s, Loss=0.0156, Avg=0.0226, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,000: Loss=0.0156, Steps/sec=1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  70%|‚ñà‚ñà‚ñä | 1050/1500 [14:05<06:31,  1.15it/s, Loss=0.0141, Avg=0.0225, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,050: Loss=0.0141, Steps/sec=1.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  73%|‚ñà‚ñà‚ñâ | 1100/1500 [14:44<06:05,  1.09it/s, Loss=0.0164, Avg=0.0226, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,100: Loss=0.0164, Steps/sec=1.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  77%|‚ñà‚ñà‚ñà | 1150/1500 [15:25<06:39,  1.14s/it, Loss=0.0213, Avg=0.0225, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,150: Loss=0.0213, Steps/sec=1.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  80%|‚ñà‚ñà‚ñà‚ñè| 1200/1500 [16:07<04:04,  1.23it/s, Loss=0.0997, Avg=0.0226, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,200: Loss=0.0997, Steps/sec=1.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  83%|‚ñà‚ñà‚ñà‚ñé| 1250/1500 [16:45<03:03,  1.36it/s, Loss=0.0170, Avg=0.0226, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,250: Loss=0.0170, Steps/sec=1.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  87%|‚ñà‚ñà‚ñà‚ñç| 1300/1500 [17:23<03:17,  1.01it/s, Loss=0.0119, Avg=0.0226, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,300: Loss=0.0119, Steps/sec=1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  90%|‚ñà‚ñà‚ñà‚ñå| 1350/1500 [18:03<01:53,  1.32it/s, Loss=0.0131, Avg=0.0226, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,350: Loss=0.0131, Steps/sec=1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  93%|‚ñà‚ñà‚ñà‚ñã| 1400/1500 [18:42<01:10,  1.43it/s, Loss=0.0199, Avg=0.0225, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,400: Loss=0.0199, Steps/sec=1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50:  97%|‚ñà‚ñà‚ñà‚ñä| 1450/1500 [19:22<00:56,  1.13s/it, Loss=0.0172, Avg=0.0225, LR=9.65e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,450: Loss=0.0172, Steps/sec=1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 30/50: 100%|‚ñà‚ñà‚ñà‚ñà| 1500/1500 [20:07<00:00,  1.24it/s, Loss=0.0337, Avg=0.0224, LR=9.65e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,500: Loss=0.0337, Steps/sec=1.24\n",
      "\n",
      "üìä EPOCH 30 COMPLETED\n",
      "üìâ Average Loss: 0.0224\n",
      "‚è±Ô∏è  Epoch Time: 20.1 minutes\n",
      "üìà Learning Rate: 9.53e-05\n",
      "‚è≥ Estimated remaining time: 5.8 hours\n",
      "üñºÔ∏è  Generating reconstruction visualization...\n",
      "üìä Epoch 30 - Reconstruction loss: 0.0091\n",
      "üìä Masked patches: 147/196\n",
      "üìä Masked patches: 147/196\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_30.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_30.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0224\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 31/50\n",
      "--------------------------------------------------\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0224\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 31/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:   3%|‚ñè     | 50/1500 [00:40<19:21,  1.25it/s, Loss=0.0148, Avg=0.0248, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,550: Loss=0.0148, Steps/sec=1.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:   7%|‚ñé    | 100/1500 [01:15<15:55,  1.46it/s, Loss=0.0223, Avg=0.0231, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,600: Loss=0.0223, Steps/sec=1.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:   9%|‚ñç    | 132/1500 [01:38<16:32,  1.38it/s, Loss=0.0175, Avg=0.0232, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,750: Loss=0.0127, Steps/sec=1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  20%|‚ñà    | 300/1500 [04:12<14:13,  1.41it/s, Loss=0.0168, Avg=0.0225, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,800: Loss=0.0168, Steps/sec=1.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  23%|‚ñà‚ñè   | 350/1500 [04:48<12:56,  1.48it/s, Loss=0.0195, Avg=0.0230, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,850: Loss=0.0195, Steps/sec=1.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  27%|‚ñà‚ñé   | 400/1500 [05:22<12:27,  1.47it/s, Loss=0.0276, Avg=0.0232, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,900: Loss=0.0276, Steps/sec=1.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  30%|‚ñà‚ñå   | 450/1500 [05:56<11:56,  1.46it/s, Loss=0.0172, Avg=0.0232, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 1,950: Loss=0.0172, Steps/sec=1.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  33%|‚ñà‚ñã   | 500/1500 [06:29<11:01,  1.51it/s, Loss=0.0134, Avg=0.0229, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,000: Loss=0.0134, Steps/sec=1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  37%|‚ñà‚ñä   | 550/1500 [07:02<10:48,  1.46it/s, Loss=0.0102, Avg=0.0229, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,050: Loss=0.0102, Steps/sec=1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  40%|‚ñà‚ñà   | 600/1500 [07:34<09:20,  1.61it/s, Loss=0.0212, Avg=0.0227, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,100: Loss=0.0212, Steps/sec=1.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  43%|‚ñà‚ñà‚ñè  | 650/1500 [08:07<09:16,  1.53it/s, Loss=0.0097, Avg=0.0229, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,150: Loss=0.0097, Steps/sec=1.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  47%|‚ñà‚ñà‚ñé  | 700/1500 [08:40<10:17,  1.30it/s, Loss=0.0069, Avg=0.0228, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,200: Loss=0.0069, Steps/sec=1.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  50%|‚ñà‚ñà‚ñå  | 750/1500 [09:13<08:10,  1.53it/s, Loss=0.0171, Avg=0.0228, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,250: Loss=0.0171, Steps/sec=1.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  53%|‚ñà‚ñà‚ñã  | 800/1500 [09:48<08:12,  1.42it/s, Loss=0.0360, Avg=0.0227, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,300: Loss=0.0360, Steps/sec=1.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  57%|‚ñà‚ñà‚ñä  | 850/1500 [10:21<07:07,  1.52it/s, Loss=0.0230, Avg=0.0227, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,350: Loss=0.0230, Steps/sec=1.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  60%|‚ñà‚ñà‚ñà  | 900/1500 [10:54<06:52,  1.45it/s, Loss=0.0139, Avg=0.0229, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,400: Loss=0.0139, Steps/sec=1.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  63%|‚ñà‚ñà‚ñà‚ñè | 950/1500 [11:28<07:22,  1.24it/s, Loss=0.0213, Avg=0.0229, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,450: Loss=0.0213, Steps/sec=1.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  67%|‚ñà‚ñà‚ñã | 1000/1500 [12:02<05:21,  1.55it/s, Loss=0.0131, Avg=0.0229, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,500: Loss=0.0131, Steps/sec=1.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  70%|‚ñà‚ñà‚ñä | 1050/1500 [12:35<04:50,  1.55it/s, Loss=0.0162, Avg=0.0227, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,550: Loss=0.0162, Steps/sec=1.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  73%|‚ñà‚ñà‚ñâ | 1100/1500 [13:08<04:13,  1.58it/s, Loss=0.0191, Avg=0.0226, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,600: Loss=0.0191, Steps/sec=1.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  77%|‚ñà‚ñà‚ñà | 1150/1500 [13:41<04:00,  1.46it/s, Loss=0.0083, Avg=0.0226, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,650: Loss=0.0083, Steps/sec=1.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  80%|‚ñà‚ñà‚ñà‚ñè| 1200/1500 [14:14<03:27,  1.45it/s, Loss=0.0232, Avg=0.0225, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,700: Loss=0.0232, Steps/sec=1.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  83%|‚ñà‚ñà‚ñà‚ñé| 1250/1500 [14:48<02:42,  1.54it/s, Loss=0.0234, Avg=0.0225, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,750: Loss=0.0234, Steps/sec=1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  87%|‚ñà‚ñà‚ñà‚ñç| 1300/1500 [15:20<02:10,  1.53it/s, Loss=0.0286, Avg=0.0225, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,800: Loss=0.0286, Steps/sec=1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  90%|‚ñà‚ñà‚ñà‚ñå| 1350/1500 [15:54<01:45,  1.43it/s, Loss=0.0921, Avg=0.0225, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,850: Loss=0.0921, Steps/sec=1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  93%|‚ñà‚ñà‚ñà‚ñã| 1400/1500 [16:28<01:02,  1.59it/s, Loss=0.0128, Avg=0.0223, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,900: Loss=0.0128, Steps/sec=1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50:  97%|‚ñà‚ñà‚ñà‚ñä| 1450/1500 [17:01<00:31,  1.57it/s, Loss=0.0315, Avg=0.0223, LR=9.53e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 2,950: Loss=0.0315, Steps/sec=1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 31/50: 100%|‚ñà‚ñà‚ñà‚ñà| 1500/1500 [17:34<00:00,  1.42it/s, Loss=0.0107, Avg=0.0222, LR=9.53e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,000: Loss=0.0107, Steps/sec=1.32\n",
      "\n",
      "üìä EPOCH 31 COMPLETED\n",
      "üìâ Average Loss: 0.0222\n",
      "‚è±Ô∏è  Epoch Time: 17.6 minutes\n",
      "üìà Learning Rate: 9.39e-05\n",
      "‚è≥ Estimated remaining time: 5.5 hours\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_31.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_31.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0222\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 32/50\n",
      "--------------------------------------------------\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0222\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 32/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:   3%|‚ñè     | 50/1500 [00:33<16:41,  1.45it/s, Loss=0.0057, Avg=0.0217, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,050: Loss=0.0057, Steps/sec=1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:   7%|‚ñé    | 100/1500 [01:06<14:36,  1.60it/s, Loss=0.0119, Avg=0.0211, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,100: Loss=0.0119, Steps/sec=1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  10%|‚ñå    | 150/1500 [01:38<13:47,  1.63it/s, Loss=0.0648, Avg=0.0209, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,150: Loss=0.0648, Steps/sec=1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  13%|‚ñã    | 200/1500 [02:13<13:33,  1.60it/s, Loss=0.0122, Avg=0.0222, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,200: Loss=0.0122, Steps/sec=1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  17%|‚ñä    | 250/1500 [02:46<14:36,  1.43it/s, Loss=0.0274, Avg=0.0223, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,250: Loss=0.0274, Steps/sec=1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  20%|‚ñà    | 300/1500 [03:19<12:24,  1.61it/s, Loss=0.0396, Avg=0.0226, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,300: Loss=0.0396, Steps/sec=1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  23%|‚ñà‚ñè   | 350/1500 [03:52<13:50,  1.39it/s, Loss=0.0253, Avg=0.0225, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,350: Loss=0.0253, Steps/sec=1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  27%|‚ñà‚ñé   | 400/1500 [04:26<12:05,  1.52it/s, Loss=0.0123, Avg=0.0223, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,400: Loss=0.0123, Steps/sec=1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  30%|‚ñà‚ñå   | 450/1500 [04:58<11:09,  1.57it/s, Loss=0.0112, Avg=0.0224, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,450: Loss=0.0112, Steps/sec=1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  33%|‚ñà‚ñã   | 500/1500 [05:30<10:33,  1.58it/s, Loss=0.0187, Avg=0.0230, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,500: Loss=0.0187, Steps/sec=1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  37%|‚ñà‚ñä   | 550/1500 [06:05<10:40,  1.48it/s, Loss=0.0140, Avg=0.0228, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,550: Loss=0.0140, Steps/sec=1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  40%|‚ñà‚ñà   | 600/1500 [06:37<10:08,  1.48it/s, Loss=0.0247, Avg=0.0228, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,600: Loss=0.0247, Steps/sec=1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  43%|‚ñà‚ñà‚ñè  | 650/1500 [07:11<08:49,  1.61it/s, Loss=0.0227, Avg=0.0226, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,650: Loss=0.0227, Steps/sec=1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  47%|‚ñà‚ñà‚ñé  | 700/1500 [07:43<08:26,  1.58it/s, Loss=0.0243, Avg=0.0227, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,700: Loss=0.0243, Steps/sec=1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  50%|‚ñà‚ñà‚ñå  | 750/1500 [08:16<07:55,  1.58it/s, Loss=0.0123, Avg=0.0227, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,750: Loss=0.0123, Steps/sec=1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  53%|‚ñà‚ñà‚ñã  | 800/1500 [08:49<07:43,  1.51it/s, Loss=0.0146, Avg=0.0227, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,800: Loss=0.0146, Steps/sec=1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  57%|‚ñà‚ñà‚ñä  | 850/1500 [09:24<07:16,  1.49it/s, Loss=0.0483, Avg=0.0226, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,850: Loss=0.0483, Steps/sec=1.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  60%|‚ñà‚ñà‚ñà  | 900/1500 [09:57<07:12,  1.39it/s, Loss=0.0237, Avg=0.0225, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,900: Loss=0.0237, Steps/sec=1.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  63%|‚ñà‚ñà‚ñà‚ñè | 950/1500 [10:29<05:37,  1.63it/s, Loss=0.0213, Avg=0.0224, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 3,950: Loss=0.0213, Steps/sec=1.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  67%|‚ñà‚ñà‚ñã | 1000/1500 [11:04<06:24,  1.30it/s, Loss=0.0132, Avg=0.0225, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,000: Loss=0.0132, Steps/sec=1.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  70%|‚ñà‚ñà‚ñä | 1050/1500 [11:36<04:42,  1.59it/s, Loss=0.0108, Avg=0.0224, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,050: Loss=0.0108, Steps/sec=1.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  73%|‚ñà‚ñà‚ñâ | 1100/1500 [12:10<04:15,  1.57it/s, Loss=0.0108, Avg=0.0223, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,100: Loss=0.0108, Steps/sec=1.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  77%|‚ñà‚ñà‚ñà | 1150/1500 [12:43<03:46,  1.55it/s, Loss=0.0272, Avg=0.0222, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,150: Loss=0.0272, Steps/sec=1.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  80%|‚ñà‚ñà‚ñà‚ñè| 1200/1500 [13:16<03:11,  1.57it/s, Loss=0.0211, Avg=0.0223, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,200: Loss=0.0211, Steps/sec=1.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  83%|‚ñà‚ñà‚ñà‚ñé| 1250/1500 [13:49<02:38,  1.58it/s, Loss=0.0072, Avg=0.0222, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,250: Loss=0.0072, Steps/sec=1.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  87%|‚ñà‚ñà‚ñà‚ñç| 1300/1500 [14:23<02:05,  1.59it/s, Loss=0.0246, Avg=0.0222, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,300: Loss=0.0246, Steps/sec=1.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  90%|‚ñà‚ñà‚ñà‚ñå| 1350/1500 [14:56<01:39,  1.50it/s, Loss=0.0122, Avg=0.0223, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,350: Loss=0.0122, Steps/sec=1.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  93%|‚ñà‚ñà‚ñà‚ñã| 1400/1500 [15:28<01:01,  1.62it/s, Loss=0.0094, Avg=0.0222, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,400: Loss=0.0094, Steps/sec=1.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50:  97%|‚ñà‚ñà‚ñà‚ñä| 1450/1500 [16:03<00:40,  1.23it/s, Loss=0.0062, Avg=0.0222, LR=9.39e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,450: Loss=0.0062, Steps/sec=1.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 32/50: 100%|‚ñà‚ñà‚ñà‚ñà| 1500/1500 [16:35<00:00,  1.51it/s, Loss=0.0116, Avg=0.0222, LR=9.39e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,500: Loss=0.0116, Steps/sec=1.38\n",
      "\n",
      "üìä EPOCH 32 COMPLETED\n",
      "üìâ Average Loss: 0.0222\n",
      "‚è±Ô∏è  Epoch Time: 16.6 minutes\n",
      "üìà Learning Rate: 9.23e-05\n",
      "‚è≥ Estimated remaining time: 5.4 hours\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_32.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_32.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0222\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 33/50\n",
      "--------------------------------------------------\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0222\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 33/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:   3%|‚ñè     | 50/1500 [00:32<15:30,  1.56it/s, Loss=0.0103, Avg=0.0220, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,550: Loss=0.0103, Steps/sec=1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:   7%|‚ñé    | 100/1500 [01:07<15:39,  1.49it/s, Loss=0.0177, Avg=0.0211, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,600: Loss=0.0177, Steps/sec=1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  10%|‚ñå    | 150/1500 [01:41<13:55,  1.62it/s, Loss=0.0064, Avg=0.0208, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,650: Loss=0.0064, Steps/sec=1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  13%|‚ñã    | 200/1500 [02:14<13:58,  1.55it/s, Loss=0.0198, Avg=0.0208, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,700: Loss=0.0198, Steps/sec=1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  17%|‚ñä    | 250/1500 [02:48<13:34,  1.54it/s, Loss=0.0116, Avg=0.0209, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,750: Loss=0.0116, Steps/sec=1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  20%|‚ñà    | 300/1500 [03:23<15:27,  1.29it/s, Loss=0.0272, Avg=0.0207, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,800: Loss=0.0272, Steps/sec=1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  23%|‚ñà‚ñè   | 350/1500 [03:55<12:03,  1.59it/s, Loss=0.0152, Avg=0.0202, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,850: Loss=0.0152, Steps/sec=1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  27%|‚ñà‚ñé   | 400/1500 [04:29<11:37,  1.58it/s, Loss=0.0140, Avg=0.0207, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,900: Loss=0.0140, Steps/sec=1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  30%|‚ñà‚ñå   | 450/1500 [05:03<11:25,  1.53it/s, Loss=0.0607, Avg=0.0210, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 4,950: Loss=0.0607, Steps/sec=1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  33%|‚ñà‚ñã   | 500/1500 [05:37<10:35,  1.57it/s, Loss=0.0192, Avg=0.0211, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,000: Loss=0.0192, Steps/sec=1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  37%|‚ñà‚ñä   | 550/1500 [06:11<10:51,  1.46it/s, Loss=0.0123, Avg=0.0215, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,050: Loss=0.0123, Steps/sec=1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  40%|‚ñà‚ñà   | 600/1500 [06:44<09:19,  1.61it/s, Loss=0.0158, Avg=0.0217, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,100: Loss=0.0158, Steps/sec=1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  43%|‚ñà‚ñà‚ñè  | 650/1500 [07:17<09:40,  1.46it/s, Loss=0.0530, Avg=0.0219, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,150: Loss=0.0530, Steps/sec=1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  47%|‚ñà‚ñà‚ñé  | 700/1500 [07:51<08:44,  1.52it/s, Loss=0.0158, Avg=0.0218, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,200: Loss=0.0158, Steps/sec=1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  50%|‚ñà‚ñà‚ñå  | 750/1500 [08:25<09:18,  1.34it/s, Loss=0.0140, Avg=0.0217, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,250: Loss=0.0140, Steps/sec=1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  53%|‚ñà‚ñà‚ñã  | 800/1500 [08:57<08:42,  1.34it/s, Loss=0.0192, Avg=0.0217, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,300: Loss=0.0192, Steps/sec=1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  57%|‚ñà‚ñà‚ñä  | 850/1500 [09:31<06:50,  1.58it/s, Loss=0.0226, Avg=0.0218, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,350: Loss=0.0226, Steps/sec=1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  60%|‚ñà‚ñà‚ñà  | 900/1500 [10:03<06:29,  1.54it/s, Loss=0.0462, Avg=0.0219, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,400: Loss=0.0462, Steps/sec=1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  63%|‚ñà‚ñà‚ñà‚ñè | 950/1500 [10:38<05:53,  1.56it/s, Loss=0.0206, Avg=0.0219, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,450: Loss=0.0206, Steps/sec=1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  67%|‚ñà‚ñà‚ñã | 1000/1500 [11:12<05:13,  1.60it/s, Loss=0.0193, Avg=0.0220, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,500: Loss=0.0193, Steps/sec=1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  70%|‚ñà‚ñà‚ñä | 1050/1500 [11:46<04:57,  1.51it/s, Loss=0.0141, Avg=0.0222, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,550: Loss=0.0141, Steps/sec=1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  73%|‚ñà‚ñà‚ñâ | 1100/1500 [12:19<04:26,  1.50it/s, Loss=0.0098, Avg=0.0222, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,600: Loss=0.0098, Steps/sec=1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  77%|‚ñà‚ñà‚ñà | 1150/1500 [12:52<03:46,  1.55it/s, Loss=0.0383, Avg=0.0222, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,650: Loss=0.0383, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  80%|‚ñà‚ñà‚ñà‚ñè| 1200/1500 [13:26<03:36,  1.39it/s, Loss=0.0047, Avg=0.0222, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,700: Loss=0.0047, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  83%|‚ñà‚ñà‚ñà‚ñé| 1250/1500 [13:58<02:36,  1.60it/s, Loss=0.0155, Avg=0.0221, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,750: Loss=0.0155, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  87%|‚ñà‚ñà‚ñà‚ñç| 1300/1500 [14:33<02:10,  1.53it/s, Loss=0.0224, Avg=0.0221, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,800: Loss=0.0224, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  90%|‚ñà‚ñà‚ñà‚ñå| 1350/1500 [15:07<01:42,  1.47it/s, Loss=0.0270, Avg=0.0221, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,850: Loss=0.0270, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  93%|‚ñà‚ñà‚ñà‚ñã| 1400/1500 [15:41<01:07,  1.47it/s, Loss=0.0185, Avg=0.0221, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,900: Loss=0.0185, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50:  97%|‚ñà‚ñà‚ñà‚ñä| 1450/1500 [16:16<00:33,  1.47it/s, Loss=0.0164, Avg=0.0220, LR=9.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5,950: Loss=0.0164, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 33/50: 100%|‚ñà‚ñà‚ñà‚ñà| 1500/1500 [16:49<00:00,  1.49it/s, Loss=0.0117, Avg=0.0220, LR=9.23e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,000: Loss=0.0117, Steps/sec=1.40\n",
      "\n",
      "üìä EPOCH 33 COMPLETED\n",
      "üìâ Average Loss: 0.0220\n",
      "‚è±Ô∏è  Epoch Time: 16.8 minutes\n",
      "üìà Learning Rate: 9.05e-05\n",
      "‚è≥ Estimated remaining time: 4.8 hours\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_33.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_33.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0220\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 34/50\n",
      "--------------------------------------------------\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0220\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 34/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:   3%|‚ñè     | 50/1500 [00:35<18:18,  1.32it/s, Loss=0.0096, Avg=0.0195, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,050: Loss=0.0096, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:   7%|‚ñé    | 100/1500 [01:09<15:57,  1.46it/s, Loss=0.0182, Avg=0.0207, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,100: Loss=0.0182, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  10%|‚ñå    | 150/1500 [01:43<14:43,  1.53it/s, Loss=0.0275, Avg=0.0216, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,150: Loss=0.0275, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  13%|‚ñã    | 200/1500 [02:18<14:38,  1.48it/s, Loss=0.0167, Avg=0.0212, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,200: Loss=0.0167, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  17%|‚ñä    | 250/1500 [02:53<13:44,  1.52it/s, Loss=0.0175, Avg=0.0214, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,250: Loss=0.0175, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  20%|‚ñà    | 300/1500 [03:27<13:29,  1.48it/s, Loss=0.0204, Avg=0.0214, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,300: Loss=0.0204, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  23%|‚ñà‚ñè   | 350/1500 [04:02<13:26,  1.43it/s, Loss=0.0207, Avg=0.0216, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,350: Loss=0.0207, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  27%|‚ñà‚ñé   | 400/1500 [04:37<12:08,  1.51it/s, Loss=0.0166, Avg=0.0219, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,400: Loss=0.0166, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  30%|‚ñà‚ñå   | 450/1500 [05:11<11:55,  1.47it/s, Loss=0.0088, Avg=0.0219, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,450: Loss=0.0088, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  33%|‚ñà‚ñã   | 500/1500 [05:47<11:04,  1.50it/s, Loss=0.0115, Avg=0.0219, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,500: Loss=0.0115, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  37%|‚ñà‚ñä   | 550/1500 [06:21<10:39,  1.48it/s, Loss=0.0229, Avg=0.0219, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,550: Loss=0.0229, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  40%|‚ñà‚ñà   | 600/1500 [06:56<10:18,  1.45it/s, Loss=0.0200, Avg=0.0222, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,600: Loss=0.0200, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  43%|‚ñà‚ñà‚ñè  | 650/1500 [07:31<12:08,  1.17it/s, Loss=0.0075, Avg=0.0222, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,650: Loss=0.0075, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  47%|‚ñà‚ñà‚ñé  | 700/1500 [08:07<09:11,  1.45it/s, Loss=0.0281, Avg=0.0221, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,700: Loss=0.0281, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  50%|‚ñà‚ñà‚ñå  | 750/1500 [08:43<08:46,  1.42it/s, Loss=0.0512, Avg=0.0221, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,750: Loss=0.0512, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  53%|‚ñà‚ñà‚ñã  | 800/1500 [09:17<08:00,  1.46it/s, Loss=0.0678, Avg=0.0221, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,800: Loss=0.0678, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  57%|‚ñà‚ñà‚ñä  | 850/1500 [09:53<07:12,  1.50it/s, Loss=0.0106, Avg=0.0220, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,850: Loss=0.0106, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  60%|‚ñà‚ñà‚ñà  | 900/1500 [10:27<06:30,  1.54it/s, Loss=0.0667, Avg=0.0221, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,900: Loss=0.0667, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  63%|‚ñà‚ñà‚ñà‚ñè | 950/1500 [11:02<06:15,  1.46it/s, Loss=0.0232, Avg=0.0220, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 6,950: Loss=0.0232, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  67%|‚ñà‚ñà‚ñã | 1000/1500 [11:38<05:45,  1.45it/s, Loss=0.0087, Avg=0.0219, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,000: Loss=0.0087, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  70%|‚ñà‚ñà‚ñä | 1050/1500 [12:12<05:52,  1.28it/s, Loss=0.0108, Avg=0.0218, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,050: Loss=0.0108, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  73%|‚ñà‚ñà‚ñâ | 1100/1500 [12:47<04:17,  1.55it/s, Loss=0.0050, Avg=0.0218, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,100: Loss=0.0050, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  77%|‚ñà‚ñà‚ñà | 1150/1500 [13:23<04:08,  1.41it/s, Loss=0.0366, Avg=0.0218, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,150: Loss=0.0366, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  80%|‚ñà‚ñà‚ñà‚ñè| 1200/1500 [13:58<03:17,  1.52it/s, Loss=0.0292, Avg=0.0218, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,200: Loss=0.0292, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  83%|‚ñà‚ñà‚ñà‚ñé| 1250/1500 [14:33<03:28,  1.20it/s, Loss=0.0280, Avg=0.0220, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,250: Loss=0.0280, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  87%|‚ñà‚ñà‚ñà‚ñç| 1300/1500 [15:08<02:17,  1.45it/s, Loss=0.0236, Avg=0.0221, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,300: Loss=0.0236, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  90%|‚ñà‚ñà‚ñà‚ñå| 1350/1500 [15:43<01:40,  1.49it/s, Loss=0.0098, Avg=0.0220, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,350: Loss=0.0098, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  93%|‚ñà‚ñà‚ñà‚ñã| 1400/1500 [16:17<01:08,  1.46it/s, Loss=0.0160, Avg=0.0219, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,400: Loss=0.0160, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50:  97%|‚ñà‚ñà‚ñà‚ñä| 1450/1500 [16:52<00:33,  1.48it/s, Loss=0.0150, Avg=0.0219, LR=9.05e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,450: Loss=0.0150, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 34/50: 100%|‚ñà‚ñà‚ñà‚ñà| 1500/1500 [17:26<00:00,  1.43it/s, Loss=0.0109, Avg=0.0219, LR=9.05e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,500: Loss=0.0109, Steps/sec=1.40\n",
      "\n",
      "üìä EPOCH 34 COMPLETED\n",
      "üìâ Average Loss: 0.0219\n",
      "‚è±Ô∏è  Epoch Time: 17.4 minutes\n",
      "üìà Learning Rate: 8.86e-05\n",
      "‚è≥ Estimated remaining time: 4.5 hours\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_34.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_34.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0219\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 35/50\n",
      "--------------------------------------------------\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0219\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 35/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:   3%|‚ñè     | 50/1500 [00:35<15:30,  1.56it/s, Loss=0.0277, Avg=0.0223, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,550: Loss=0.0277, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:   7%|‚ñé    | 100/1500 [01:09<15:08,  1.54it/s, Loss=0.0166, Avg=0.0204, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,600: Loss=0.0166, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  10%|‚ñå    | 150/1500 [01:41<14:04,  1.60it/s, Loss=0.0087, Avg=0.0212, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,650: Loss=0.0087, Steps/sec=1.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  13%|‚ñã    | 200/1500 [02:16<14:15,  1.52it/s, Loss=0.0228, Avg=0.0214, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,700: Loss=0.0228, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  17%|‚ñä    | 250/1500 [02:49<14:25,  1.44it/s, Loss=0.0353, Avg=0.0218, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,750: Loss=0.0353, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  20%|‚ñà    | 300/1500 [03:23<13:21,  1.50it/s, Loss=0.0110, Avg=0.0215, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,800: Loss=0.0110, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  23%|‚ñà‚ñè   | 350/1500 [03:57<12:49,  1.49it/s, Loss=0.0323, Avg=0.0210, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,850: Loss=0.0323, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  27%|‚ñà‚ñé   | 400/1500 [04:30<11:17,  1.62it/s, Loss=0.0205, Avg=0.0211, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,900: Loss=0.0205, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  30%|‚ñà‚ñå   | 450/1500 [05:03<13:07,  1.33it/s, Loss=0.0244, Avg=0.0212, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 7,950: Loss=0.0244, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  33%|‚ñà‚ñã   | 500/1500 [05:38<11:17,  1.48it/s, Loss=0.0138, Avg=0.0216, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,000: Loss=0.0138, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  37%|‚ñà‚ñä   | 550/1500 [06:12<10:55,  1.45it/s, Loss=0.0189, Avg=0.0217, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,050: Loss=0.0189, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  40%|‚ñà‚ñà   | 600/1500 [06:45<10:03,  1.49it/s, Loss=0.0091, Avg=0.0215, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,100: Loss=0.0091, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  43%|‚ñà‚ñà‚ñè  | 650/1500 [07:20<08:56,  1.58it/s, Loss=0.0133, Avg=0.0216, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,150: Loss=0.0133, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  47%|‚ñà‚ñà‚ñé  | 700/1500 [07:53<08:52,  1.50it/s, Loss=0.0117, Avg=0.0216, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,200: Loss=0.0117, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  50%|‚ñà‚ñà‚ñå  | 750/1500 [08:28<08:28,  1.47it/s, Loss=0.0293, Avg=0.0218, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,250: Loss=0.0293, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  53%|‚ñà‚ñà‚ñã  | 800/1500 [09:02<07:16,  1.60it/s, Loss=0.0157, Avg=0.0216, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,300: Loss=0.0157, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  57%|‚ñà‚ñà‚ñä  | 850/1500 [09:35<06:57,  1.56it/s, Loss=0.0236, Avg=0.0218, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,350: Loss=0.0236, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  60%|‚ñà‚ñà‚ñà  | 900/1500 [10:10<07:08,  1.40it/s, Loss=0.0442, Avg=0.0219, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,400: Loss=0.0442, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  63%|‚ñà‚ñà‚ñà‚ñè | 950/1500 [10:43<06:16,  1.46it/s, Loss=0.0182, Avg=0.0219, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,450: Loss=0.0182, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  67%|‚ñà‚ñà‚ñã | 1000/1500 [11:18<05:28,  1.52it/s, Loss=0.0335, Avg=0.0219, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,500: Loss=0.0335, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  70%|‚ñà‚ñà‚ñä | 1050/1500 [11:52<05:11,  1.44it/s, Loss=0.0075, Avg=0.0219, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,550: Loss=0.0075, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  73%|‚ñà‚ñà‚ñâ | 1100/1500 [12:26<04:09,  1.60it/s, Loss=0.0170, Avg=0.0218, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,600: Loss=0.0170, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  77%|‚ñà‚ñà‚ñà | 1150/1500 [12:59<03:51,  1.51it/s, Loss=0.0281, Avg=0.0219, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,650: Loss=0.0281, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  80%|‚ñà‚ñà‚ñà‚ñè| 1200/1500 [13:33<03:08,  1.60it/s, Loss=0.0096, Avg=0.0218, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,700: Loss=0.0096, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  83%|‚ñà‚ñà‚ñà‚ñé| 1250/1500 [14:07<02:53,  1.44it/s, Loss=0.0484, Avg=0.0219, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,750: Loss=0.0484, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  87%|‚ñà‚ñà‚ñà‚ñç| 1300/1500 [14:41<02:10,  1.53it/s, Loss=0.0301, Avg=0.0219, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,800: Loss=0.0301, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  90%|‚ñà‚ñà‚ñà‚ñå| 1350/1500 [15:16<01:37,  1.53it/s, Loss=0.0195, Avg=0.0219, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,850: Loss=0.0195, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  93%|‚ñà‚ñà‚ñà‚ñã| 1400/1500 [15:51<01:08,  1.46it/s, Loss=0.0161, Avg=0.0219, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,900: Loss=0.0161, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50:  97%|‚ñà‚ñà‚ñà‚ñä| 1450/1500 [16:27<00:36,  1.36it/s, Loss=0.0124, Avg=0.0218, LR=8.86e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 8,950: Loss=0.0124, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 35/50: 100%|‚ñà‚ñà‚ñà‚ñà| 1500/1500 [17:01<00:00,  1.47it/s, Loss=0.0301, Avg=0.0218, LR=8.86e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,000: Loss=0.0301, Steps/sec=1.41\n",
      "\n",
      "üìä EPOCH 35 COMPLETED\n",
      "üìâ Average Loss: 0.0218\n",
      "‚è±Ô∏è  Epoch Time: 17.0 minutes\n",
      "üìà Learning Rate: 8.66e-05\n",
      "‚è≥ Estimated remaining time: 4.3 hours\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_35.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_35.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0218\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 36/50\n",
      "--------------------------------------------------\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0218\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 36/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:   3%|‚ñè     | 50/1500 [00:35<15:57,  1.52it/s, Loss=0.0121, Avg=0.0198, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,050: Loss=0.0121, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:   7%|‚ñé    | 100/1500 [01:11<15:06,  1.54it/s, Loss=0.0207, Avg=0.0214, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,100: Loss=0.0207, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  10%|‚ñå    | 150/1500 [01:46<17:15,  1.30it/s, Loss=0.0279, Avg=0.0208, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,150: Loss=0.0279, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  13%|‚ñã    | 200/1500 [02:22<15:06,  1.43it/s, Loss=0.0185, Avg=0.0202, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,200: Loss=0.0185, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  17%|‚ñä    | 250/1500 [02:56<14:21,  1.45it/s, Loss=0.0164, Avg=0.0208, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,250: Loss=0.0164, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  20%|‚ñà    | 300/1500 [03:32<13:43,  1.46it/s, Loss=0.0386, Avg=0.0209, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,300: Loss=0.0386, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  23%|‚ñà‚ñè   | 350/1500 [04:09<12:49,  1.49it/s, Loss=0.0296, Avg=0.0211, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,350: Loss=0.0296, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  27%|‚ñà‚ñé   | 400/1500 [04:44<13:24,  1.37it/s, Loss=0.0155, Avg=0.0208, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,400: Loss=0.0155, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  30%|‚ñà‚ñå   | 450/1500 [05:18<11:17,  1.55it/s, Loss=0.0262, Avg=0.0210, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,450: Loss=0.0262, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  33%|‚ñà‚ñã   | 500/1500 [05:53<11:16,  1.48it/s, Loss=0.0647, Avg=0.0210, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,500: Loss=0.0647, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  37%|‚ñà‚ñä   | 550/1500 [06:28<10:47,  1.47it/s, Loss=0.0114, Avg=0.0215, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,550: Loss=0.0114, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  40%|‚ñà‚ñà   | 600/1500 [07:02<10:33,  1.42it/s, Loss=0.0211, Avg=0.0219, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,600: Loss=0.0211, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  43%|‚ñà‚ñà‚ñè  | 650/1500 [07:38<09:01,  1.57it/s, Loss=0.0056, Avg=0.0217, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,650: Loss=0.0056, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  47%|‚ñà‚ñà‚ñé  | 700/1500 [08:14<09:46,  1.37it/s, Loss=0.0276, Avg=0.0218, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,700: Loss=0.0276, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  50%|‚ñà‚ñà‚ñå  | 750/1500 [08:50<08:36,  1.45it/s, Loss=0.0068, Avg=0.0221, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,750: Loss=0.0068, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  53%|‚ñà‚ñà‚ñã  | 800/1500 [09:25<08:03,  1.45it/s, Loss=0.0260, Avg=0.0220, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,800: Loss=0.0260, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  57%|‚ñà‚ñà‚ñä  | 850/1500 [10:01<07:32,  1.44it/s, Loss=0.0627, Avg=0.0219, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,850: Loss=0.0627, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  60%|‚ñà‚ñà‚ñà  | 900/1500 [10:37<06:28,  1.55it/s, Loss=0.0115, Avg=0.0219, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,900: Loss=0.0115, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  63%|‚ñà‚ñà‚ñà‚ñè | 950/1500 [11:12<06:26,  1.42it/s, Loss=0.0565, Avg=0.0218, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 9,950: Loss=0.0565, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  67%|‚ñà‚ñà‚ñã | 1000/1500 [11:47<06:15,  1.33it/s, Loss=0.0484, Avg=0.0218, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,000: Loss=0.0484, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  70%|‚ñà‚ñà‚ñä | 1050/1500 [12:22<04:53,  1.53it/s, Loss=0.0161, Avg=0.0220, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,050: Loss=0.0161, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  73%|‚ñà‚ñà‚ñâ | 1100/1500 [12:57<04:30,  1.48it/s, Loss=0.0286, Avg=0.0218, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,100: Loss=0.0286, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  77%|‚ñà‚ñà‚ñà | 1150/1500 [13:33<03:55,  1.49it/s, Loss=0.0128, Avg=0.0218, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,150: Loss=0.0128, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  80%|‚ñà‚ñà‚ñà‚ñè| 1200/1500 [14:08<03:37,  1.38it/s, Loss=0.0122, Avg=0.0218, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,200: Loss=0.0122, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  83%|‚ñà‚ñà‚ñà‚ñé| 1250/1500 [14:45<03:04,  1.36it/s, Loss=0.0163, Avg=0.0218, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,250: Loss=0.0163, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  87%|‚ñà‚ñà‚ñà‚ñç| 1300/1500 [15:21<02:23,  1.39it/s, Loss=0.0172, Avg=0.0217, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,300: Loss=0.0172, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  90%|‚ñà‚ñà‚ñà‚ñå| 1350/1500 [15:56<01:41,  1.48it/s, Loss=0.0214, Avg=0.0219, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,350: Loss=0.0214, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  93%|‚ñà‚ñà‚ñà‚ñã| 1400/1500 [16:31<01:06,  1.51it/s, Loss=0.0112, Avg=0.0218, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,400: Loss=0.0112, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50:  97%|‚ñà‚ñà‚ñà‚ñä| 1450/1500 [17:06<00:33,  1.50it/s, Loss=0.0188, Avg=0.0218, LR=8.66e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,450: Loss=0.0188, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 36/50: 100%|‚ñà‚ñà‚ñà‚ñà| 1500/1500 [17:42<00:00,  1.41it/s, Loss=0.0191, Avg=0.0218, LR=8.66e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,500: Loss=0.0191, Steps/sec=1.41\n",
      "\n",
      "üìä EPOCH 36 COMPLETED\n",
      "üìâ Average Loss: 0.0218\n",
      "‚è±Ô∏è  Epoch Time: 17.7 minutes\n",
      "üìà Learning Rate: 8.44e-05\n",
      "‚è≥ Estimated remaining time: 4.1 hours\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_36.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_36.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0218\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 37/50\n",
      "--------------------------------------------------\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0218\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 37/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:   3%|‚ñè     | 50/1500 [00:37<17:02,  1.42it/s, Loss=0.0157, Avg=0.0221, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,550: Loss=0.0157, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:   7%|‚ñé    | 100/1500 [01:13<16:33,  1.41it/s, Loss=0.0145, Avg=0.0219, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,600: Loss=0.0145, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  10%|‚ñå    | 150/1500 [01:49<18:55,  1.19it/s, Loss=0.0098, Avg=0.0207, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,650: Loss=0.0098, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  13%|‚ñã    | 200/1500 [02:24<13:10,  1.64it/s, Loss=0.0378, Avg=0.0209, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,700: Loss=0.0378, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  17%|‚ñä    | 250/1500 [02:55<12:42,  1.64it/s, Loss=0.0266, Avg=0.0207, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,750: Loss=0.0266, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  20%|‚ñà    | 300/1500 [03:26<13:53,  1.44it/s, Loss=0.0644, Avg=0.0209, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,800: Loss=0.0644, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  23%|‚ñà‚ñè   | 350/1500 [03:57<11:43,  1.63it/s, Loss=0.0090, Avg=0.0211, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,850: Loss=0.0090, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  27%|‚ñà‚ñé   | 400/1500 [04:29<17:43,  1.03it/s, Loss=0.0305, Avg=0.0207, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,900: Loss=0.0305, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  30%|‚ñà‚ñå   | 450/1500 [05:00<10:57,  1.60it/s, Loss=0.0463, Avg=0.0211, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 10,950: Loss=0.0463, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  33%|‚ñà‚ñã   | 500/1500 [05:32<10:20,  1.61it/s, Loss=0.0192, Avg=0.0211, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,000: Loss=0.0192, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  37%|‚ñà‚ñä   | 550/1500 [06:03<09:22,  1.69it/s, Loss=0.0073, Avg=0.0213, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,050: Loss=0.0073, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  40%|‚ñà‚ñà   | 600/1500 [06:34<09:10,  1.63it/s, Loss=0.0263, Avg=0.0214, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,100: Loss=0.0263, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  43%|‚ñà‚ñà‚ñè  | 650/1500 [07:05<09:01,  1.57it/s, Loss=0.0305, Avg=0.0215, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,150: Loss=0.0305, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  47%|‚ñà‚ñà‚ñé  | 700/1500 [07:38<08:12,  1.62it/s, Loss=0.0173, Avg=0.0213, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,200: Loss=0.0173, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  50%|‚ñà‚ñà‚ñå  | 750/1500 [08:08<07:26,  1.68it/s, Loss=0.0164, Avg=0.0214, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,250: Loss=0.0164, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  53%|‚ñà‚ñà‚ñã  | 800/1500 [08:40<07:06,  1.64it/s, Loss=0.0421, Avg=0.0216, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,300: Loss=0.0421, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  57%|‚ñà‚ñà‚ñä  | 850/1500 [09:11<06:29,  1.67it/s, Loss=0.0065, Avg=0.0216, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,350: Loss=0.0065, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  60%|‚ñà‚ñà‚ñà  | 900/1500 [09:43<06:03,  1.65it/s, Loss=0.0131, Avg=0.0215, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,400: Loss=0.0131, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  63%|‚ñà‚ñà‚ñà‚ñè | 950/1500 [10:13<05:43,  1.60it/s, Loss=0.0166, Avg=0.0214, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,450: Loss=0.0166, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  67%|‚ñà‚ñà‚ñã | 1000/1500 [10:46<04:58,  1.68it/s, Loss=0.0064, Avg=0.0214, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,500: Loss=0.0064, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  70%|‚ñà‚ñà‚ñä | 1050/1500 [11:16<04:30,  1.66it/s, Loss=0.0091, Avg=0.0214, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,550: Loss=0.0091, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  73%|‚ñà‚ñà‚ñâ | 1100/1500 [11:48<03:56,  1.69it/s, Loss=0.0078, Avg=0.0213, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,600: Loss=0.0078, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  77%|‚ñà‚ñà‚ñà | 1150/1500 [12:19<03:37,  1.61it/s, Loss=0.0099, Avg=0.0214, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,650: Loss=0.0099, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  80%|‚ñà‚ñà‚ñà‚ñè| 1200/1500 [12:49<02:56,  1.70it/s, Loss=0.0177, Avg=0.0215, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,700: Loss=0.0177, Steps/sec=1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  83%|‚ñà‚ñà‚ñà‚ñé| 1250/1500 [13:20<02:26,  1.71it/s, Loss=0.0064, Avg=0.0213, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,750: Loss=0.0064, Steps/sec=1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  87%|‚ñà‚ñà‚ñà‚ñç| 1300/1500 [13:50<01:57,  1.70it/s, Loss=0.0119, Avg=0.0214, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,800: Loss=0.0119, Steps/sec=1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  90%|‚ñà‚ñà‚ñà‚ñå| 1350/1500 [14:20<01:28,  1.70it/s, Loss=0.0215, Avg=0.0215, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,850: Loss=0.0215, Steps/sec=1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  93%|‚ñà‚ñà‚ñà‚ñã| 1400/1500 [14:51<00:58,  1.71it/s, Loss=0.0247, Avg=0.0215, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,900: Loss=0.0247, Steps/sec=1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50:  97%|‚ñà‚ñà‚ñà‚ñä| 1450/1500 [15:22<00:30,  1.62it/s, Loss=0.0199, Avg=0.0214, LR=8.44e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 11,950: Loss=0.0199, Steps/sec=1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 37/50: 100%|‚ñà‚ñà‚ñà‚ñà| 1500/1500 [15:53<00:00,  1.57it/s, Loss=0.0145, Avg=0.0215, LR=8.44e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,000: Loss=0.0145, Steps/sec=1.43\n",
      "\n",
      "üìä EPOCH 37 COMPLETED\n",
      "üìâ Average Loss: 0.0215\n",
      "‚è±Ô∏è  Epoch Time: 15.9 minutes\n",
      "üìà Learning Rate: 8.21e-05\n",
      "‚è≥ Estimated remaining time: 3.7 hours\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_37.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_37.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0215\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 38/50\n",
      "--------------------------------------------------\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0215\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 38/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:   3%|‚ñè     | 50/1500 [00:33<18:58,  1.27it/s, Loss=0.0169, Avg=0.0207, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,050: Loss=0.0169, Steps/sec=1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:   7%|‚ñé    | 100/1500 [01:16<19:22,  1.20it/s, Loss=0.0123, Avg=0.0205, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,100: Loss=0.0123, Steps/sec=1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  10%|‚ñå    | 150/1500 [01:58<18:28,  1.22it/s, Loss=0.0076, Avg=0.0204, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,150: Loss=0.0076, Steps/sec=1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  13%|‚ñã    | 200/1500 [02:39<18:12,  1.19it/s, Loss=0.0108, Avg=0.0196, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,200: Loss=0.0108, Steps/sec=1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  17%|‚ñä    | 250/1500 [03:21<17:30,  1.19it/s, Loss=0.0116, Avg=0.0194, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,250: Loss=0.0116, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  20%|‚ñà    | 300/1500 [04:04<16:44,  1.20it/s, Loss=0.0181, Avg=0.0202, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,300: Loss=0.0181, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  23%|‚ñà‚ñè   | 350/1500 [04:59<22:17,  1.16s/it, Loss=0.0235, Avg=0.0203, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,350: Loss=0.0235, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  27%|‚ñà‚ñé   | 400/1500 [05:52<16:44,  1.09it/s, Loss=0.0093, Avg=0.0201, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,400: Loss=0.0093, Steps/sec=1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  30%|‚ñà‚ñå   | 450/1500 [06:45<15:51,  1.10it/s, Loss=0.0131, Avg=0.0200, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,450: Loss=0.0131, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  33%|‚ñà‚ñã   | 500/1500 [07:44<18:46,  1.13s/it, Loss=0.0264, Avg=0.0203, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,500: Loss=0.0264, Steps/sec=1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  37%|‚ñà‚ñä   | 550/1500 [11:00<37:28,  2.37s/it, Loss=0.0241, Avg=0.0205, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,550: Loss=0.0241, Steps/sec=1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  40%|‚ñà‚ñà   | 600/1500 [13:22<15:19,  1.02s/it, Loss=0.0561, Avg=0.0205, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,600: Loss=0.0561, Steps/sec=1.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  43%|‚ñà‚ñà‚ñè  | 650/1500 [14:51<13:08,  1.08it/s, Loss=0.0359, Avg=0.0206, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,650: Loss=0.0359, Steps/sec=1.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  47%|‚ñà‚ñà‚ñé  | 700/1500 [15:46<14:13,  1.07s/it, Loss=0.0137, Avg=0.0207, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,700: Loss=0.0137, Steps/sec=1.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  50%|‚ñà‚ñà‚ñå  | 750/1500 [16:41<12:40,  1.01s/it, Loss=0.0663, Avg=0.0208, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,750: Loss=0.0663, Steps/sec=1.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  53%|‚ñà‚ñà‚ñã  | 800/1500 [17:31<11:28,  1.02it/s, Loss=0.0179, Avg=0.0209, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,800: Loss=0.0179, Steps/sec=1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  57%|‚ñà‚ñà‚ñä  | 850/1500 [18:32<10:05,  1.07it/s, Loss=0.0105, Avg=0.0207, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,850: Loss=0.0105, Steps/sec=1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  60%|‚ñà‚ñà‚ñà  | 900/1500 [19:40<23:57,  2.40s/it, Loss=0.0114, Avg=0.0206, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,900: Loss=0.0114, Steps/sec=1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  63%|‚ñà‚ñà‚ñà‚ñè | 950/1500 [21:37<13:45,  1.50s/it, Loss=0.0157, Avg=0.0205, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 12,950: Loss=0.0157, Steps/sec=1.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  67%|‚ñà‚ñà‚ñã | 1000/1500 [22:33<06:06,  1.37it/s, Loss=0.0214, Avg=0.0207, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 13,000: Loss=0.0214, Steps/sec=1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  70%|‚ñà‚ñà‚ñä | 1050/1500 [23:22<05:49,  1.29it/s, Loss=0.0370, Avg=0.0208, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 13,050: Loss=0.0370, Steps/sec=1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  73%|‚ñà‚ñà‚ñâ | 1100/1500 [24:09<05:16,  1.26it/s, Loss=0.0233, Avg=0.0211, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 13,100: Loss=0.0233, Steps/sec=1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  77%|‚ñà‚ñà‚ñà | 1150/1500 [25:23<03:58,  1.47it/s, Loss=0.0519, Avg=0.0213, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 13,150: Loss=0.0519, Steps/sec=1.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  80%|‚ñà‚ñà‚ñà‚ñè| 1200/1500 [26:03<04:03,  1.23it/s, Loss=0.0146, Avg=0.0212, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 13,200: Loss=0.0146, Steps/sec=1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  83%|‚ñà‚ñà‚ñà‚ñé| 1250/1500 [26:43<03:18,  1.26it/s, Loss=0.0105, Avg=0.0213, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 13,250: Loss=0.0105, Steps/sec=1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  87%|‚ñà‚ñà‚ñà‚ñç| 1300/1500 [27:32<02:31,  1.32it/s, Loss=0.0236, Avg=0.0214, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 13,300: Loss=0.0236, Steps/sec=1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  90%|‚ñà‚ñà‚ñà‚ñå| 1350/1500 [28:09<01:41,  1.48it/s, Loss=0.0169, Avg=0.0213, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 13,350: Loss=0.0169, Steps/sec=1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  93%|‚ñà‚ñà‚ñà‚ñã| 1400/1500 [28:46<01:10,  1.42it/s, Loss=0.0370, Avg=0.0213, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 13,400: Loss=0.0370, Steps/sec=1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50:  97%|‚ñà‚ñà‚ñà‚ñä| 1450/1500 [29:27<00:33,  1.50it/s, Loss=0.0096, Avg=0.0213, LR=8.21e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 13,450: Loss=0.0096, Steps/sec=1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 38/50: 100%|‚ñà‚ñà‚ñà‚ñà| 1500/1500 [30:04<00:00,  1.20s/it, Loss=0.0217, Avg=0.0213, LR=8.21e-05]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 13,500: Loss=0.0217, Steps/sec=1.32\n",
      "\n",
      "üìä EPOCH 38 COMPLETED\n",
      "üìâ Average Loss: 0.0213\n",
      "‚è±Ô∏è  Epoch Time: 30.1 minutes\n",
      "üìà Learning Rate: 7.96e-05\n",
      "‚è≥ Estimated remaining time: 4.2 hours\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_38.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_checkpoint_epoch_38.pth\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0213\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 39/50\n",
      "--------------------------------------------------\n",
      "üíæ Checkpoint saved: mae_base_checkpoints/mae_best_model.pth\n",
      "üèÜ NEW BEST MODEL! Loss: 0.0213\n",
      "üíæ Memory cleaned up\n",
      "\n",
      "üéØ EPOCH 39/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 39/50:   3%|‚ñè     | 50/1500 [00:58<21:08,  1.14it/s, Loss=0.0636, Avg=0.0207, LR=7.96e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 13,550: Loss=0.0636, Steps/sec=1.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAE Epoch 39/50:   5%|‚ñè   | 78/1500 [02:34<3:02:25,  7.70s/it, Loss=0.0158, Avg=0.0223, LR=7.96e-05]"
     ]
    }
   ],
   "source": [
    "# Execute Complete MAE Pretraining on 24,015 Unlabeled Fish Images - Resume from Epoch 6\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"üé≠ RESUMING MAE PRETRAINING FROM EPOCH 29\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚ö†Ô∏è  This will take 3-6 hours to complete!\")\n",
    "print(f\"üìä Training {len(mae_dataset):,} images for {MAE_CONFIG['epochs']} epochs\")\n",
    "print(f\"üîÑ Resuming from epoch 29, will train epochs 30-{MAE_CONFIG['epochs']}\")\n",
    "print(f\"üéØ Expected time: ~{len(mae_dataloader) * (MAE_CONFIG['epochs'] - 29) * 0.5 / 3600:.1f} hours\")\n",
    "\n",
    "# Load checkpoint from epoch 29 first to determine model architecture\n",
    "start_epoch = 29\n",
    "checkpoint_path_epoch_29 = os.path.join(MAE_CONFIG['checkpoint_dir'], 'mae_checkpoint_epoch_29.pth')\n",
    "\n",
    "# Check if we need to use ViT-Base architecture based on checkpoint\n",
    "use_vit_base = False\n",
    "if os.path.exists(checkpoint_path_epoch_29):\n",
    "    print(f\"üìÇ Checking checkpoint architecture from epoch {start_epoch}...\")\n",
    "    \n",
    "    # Load checkpoint to check architecture\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path_epoch_29, map_location='cpu', weights_only=False)\n",
    "        print(\"‚úÖ Checkpoint loaded for architecture inspection\")\n",
    "        \n",
    "        # Check encoder embedding dimension from checkpoint\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            \n",
    "            # Check cls_token shape to determine architecture\n",
    "            if 'encoder.cls_token' in state_dict:\n",
    "                cls_token_shape = state_dict['encoder.cls_token'].shape\n",
    "                embed_dim = cls_token_shape[-1]  # Last dimension is embed_dim\n",
    "                \n",
    "                if embed_dim == 768:\n",
    "                    use_vit_base = True\n",
    "                    print(f\"üéØ Detected ViT-Base architecture (embed_dim={embed_dim})\")\n",
    "                elif embed_dim == 384:\n",
    "                    use_vit_base = False\n",
    "                    print(f\"üéØ Detected ViT-Small architecture (embed_dim={embed_dim})\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è  Unknown architecture (embed_dim={embed_dim}), defaulting to ViT-Base\")\n",
    "                    use_vit_base = True\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Could not determine architecture from checkpoint, defaulting to ViT-Base\")\n",
    "                use_vit_base = True\n",
    "        \n",
    "        # Clear checkpoint from memory temporarily\n",
    "        del checkpoint\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error checking checkpoint: {e}\")\n",
    "        print(\"üîß Defaulting to ViT-Base architecture\")\n",
    "        use_vit_base = True\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Checkpoint for epoch {start_epoch} not found at {checkpoint_path_epoch_29}\")\n",
    "    print(\"üîß Will create ViT-Base model and start from scratch...\")\n",
    "    use_vit_base = True\n",
    "    start_epoch = 0\n",
    "\n",
    "# Create MAE model with correct architecture\n",
    "print(f\"\\nüèóÔ∏è  CREATING MAE MODEL\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if use_vit_base:\n",
    "    print(\"üéØ Creating ViT-Base MAE model (768-dim, 12 encoder layers, 8 decoder layers)\")\n",
    "    mae_model = MaskedAutoEncoder(\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        encoder_embed_dim=768,      # ViT-Base dimensions\n",
    "        encoder_depth=12,           # ViT-Base depth\n",
    "        encoder_num_heads=12,       # ViT-Base heads\n",
    "        decoder_embed_dim=512,      # ViT-Base decoder dimensions\n",
    "        decoder_depth=8,            # ViT-Base decoder depth\n",
    "        decoder_num_heads=16,       # ViT-Base decoder heads\n",
    "        mlp_ratio=4.0,\n",
    "        norm_pix_loss=True\n",
    "    ).to(DEVICE)\n",
    "    print(\"‚úÖ ViT-Base MAE model created and moved to device\")\n",
    "else:\n",
    "    print(\"üéØ Creating ViT-Small MAE model (384-dim, 6 encoder layers, 4 decoder layers)\")\n",
    "    mae_model = MaskedAutoEncoder(\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        encoder_embed_dim=384,      # ViT-Small dimensions\n",
    "        encoder_depth=6,            # ViT-Small depth\n",
    "        encoder_num_heads=6,        # ViT-Small heads\n",
    "        decoder_embed_dim=192,      # ViT-Small decoder dimensions\n",
    "        decoder_depth=4,            # ViT-Small decoder depth\n",
    "        decoder_num_heads=6,        # ViT-Small decoder heads\n",
    "        mlp_ratio=4.0,\n",
    "        norm_pix_loss=True\n",
    "    ).to(DEVICE)\n",
    "    print(\"‚úÖ ViT-Small MAE model created and moved to device\")\n",
    "\n",
    "# Create optimizer for the correct model\n",
    "mae_optimizer = torch.optim.AdamW(\n",
    "    mae_model.parameters(),\n",
    "    lr=MAE_CONFIG['learning_rate'],\n",
    "    weight_decay=MAE_CONFIG['weight_decay'],\n",
    "    betas=(MAE_CONFIG['beta1'], MAE_CONFIG['beta2'])\n",
    ")\n",
    "print(\"‚úÖ MAE optimizer created\")\n",
    "\n",
    "# Create scheduler\n",
    "mae_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    mae_optimizer,\n",
    "    T_max=MAE_CONFIG['epochs'],\n",
    "    eta_min=MAE_CONFIG['learning_rate'] * 0.01\n",
    ")\n",
    "print(\"‚úÖ MAE scheduler created\")\n",
    "\n",
    "# Now load the checkpoint if it exists\n",
    "training_losses = []\n",
    "epoch_times = []\n",
    "\n",
    "if os.path.exists(checkpoint_path_epoch_6):\n",
    "    print(f\"\\nüìÇ Loading checkpoint from epoch {start_epoch}...\")\n",
    "    \n",
    "    # Load checkpoint with proper PyTorch 2.6+ compatibility\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path_epoch_6, map_location=DEVICE, weights_only=False)\n",
    "        print(\"‚úÖ Checkpoint loaded with weights_only=False (trusted source)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading checkpoint: {e}\")\n",
    "        print(\"üîß Attempting alternative loading method...\")\n",
    "        \n",
    "        # Fallback method with explicit safe globals for numpy\n",
    "        import numpy as np\n",
    "        with torch.serialization.safe_globals([np.core.multiarray.scalar, np.ndarray]):\n",
    "            checkpoint = torch.load(checkpoint_path_epoch_6, map_location=DEVICE)\n",
    "        print(\"‚úÖ Checkpoint loaded with safe globals\")\n",
    "    \n",
    "    # Load model state\n",
    "    try:\n",
    "        mae_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        mae_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if mae_scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:\n",
    "            mae_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded checkpoint from epoch {start_epoch}\")\n",
    "        print(f\"üìä Checkpoint loss: {checkpoint['loss']:.4f}\")\n",
    "        print(f\"üìÖ Checkpoint timestamp: {checkpoint.get('timestamp', 'N/A')}\")\n",
    "        \n",
    "        # Load training history if available\n",
    "        if 'training_losses' in checkpoint:\n",
    "            training_losses = checkpoint.get('training_losses', [])\n",
    "            epoch_times = checkpoint.get('epoch_times', [])\n",
    "            print(f\"üìä Loaded training history: {len(training_losses)} epochs\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model state: {e}\")\n",
    "        print(\"üîß Starting training from scratch...\")\n",
    "        start_epoch = 0\n",
    "        training_losses = []\n",
    "        epoch_times = []\n",
    "\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è  Starting training from scratch (no checkpoint found)\")\n",
    "    start_epoch = 0\n",
    "\n",
    "def save_mae_checkpoint(model, optimizer, scheduler, epoch, loss, config, filename, training_losses=None, epoch_times=None):\n",
    "    \"\"\"Save MAE checkpoint with full state including training history\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'loss': loss,\n",
    "        'config': config,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'training_losses': training_losses or [],\n",
    "        'epoch_times': epoch_times or []\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"üíæ Checkpoint saved: {filename}\")\n",
    "\n",
    "def visualize_mae_reconstruction(model, dataloader, device, epoch, save_dir=None):\n",
    "    \"\"\"Visualize and save MAE reconstruction results\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Get a batch of images\n",
    "            images = next(iter(dataloader))[:4].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            loss, pred, mask, latent = model(images, mask_ratio=0.75)\n",
    "            \n",
    "            print(f\"üìä Epoch {epoch} - Reconstruction loss: {loss.item():.4f}\")\n",
    "            print(f\"üìä Masked patches: {mask.sum(dim=1).float().mean().item():.0f}/{mask.shape[1]}\")\n",
    "            \n",
    "            # Save visualization if directory provided\n",
    "            if save_dir and os.path.exists(save_dir):\n",
    "                try:\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    \n",
    "                    # Show original images\n",
    "                    for i in range(min(4, images.shape[0])):\n",
    "                        plt.subplot(2, 4, i + 1)\n",
    "                        img = images[i].cpu()\n",
    "                        \n",
    "                        # Denormalize\n",
    "                        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "                        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "                        img = img * std + mean\n",
    "                        img = torch.clamp(img, 0, 1)\n",
    "                        \n",
    "                        plt.imshow(img.permute(1, 2, 0))\n",
    "                        plt.title(f'Original {i+1}')\n",
    "                        plt.axis('off')\n",
    "                    \n",
    "                    plt.suptitle(f'MAE Reconstruction - Epoch {epoch}')\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(os.path.join(save_dir, f'mae_reconstruction_epoch_{epoch}.png'), \n",
    "                               dpi=150, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Visualization save failed: {e}\")\n",
    "            \n",
    "        model.train()\n",
    "        return loss.item()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Reconstruction visualization failed: {e}\")\n",
    "        model.train()\n",
    "        return 0.0\n",
    "\n",
    "# Initialize training variables\n",
    "print(f\"\\nüöÄ INITIALIZING MAE TRAINING\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "mae_model.train()\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Create visualization directory\n",
    "viz_dir = os.path.join(MAE_CONFIG['logs_dir'], 'visualizations')\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Training setup complete!\")\n",
    "print(f\"üìä Model parameters: {sum(p.numel() for p in mae_model.parameters()):,}\")\n",
    "print(f\"üìä Architecture: {'ViT-Base' if use_vit_base else 'ViT-Small'}\")\n",
    "print(f\"üìä Batches per epoch: {len(mae_dataloader):,}\")\n",
    "print(f\"üìä Total training steps: {len(mae_dataloader) * (MAE_CONFIG['epochs'] - start_epoch):,}\")\n",
    "print(f\"üìä Starting from epoch: {start_epoch + 1}\")\n",
    "print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Main training loop - starting from start_epoch\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, MAE_CONFIG['epochs']):\n",
    "    epoch_start_time = time.time()\n",
    "    epoch_losses = []\n",
    "    \n",
    "    print(f\"\\nüéØ EPOCH {epoch+1}/{MAE_CONFIG['epochs']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Progress bar for epoch\n",
    "    pbar = tqdm(mae_dataloader, \n",
    "                desc=f\"MAE Epoch {epoch+1:2d}/{MAE_CONFIG['epochs']}\", \n",
    "                leave=True,\n",
    "                ncols=100)\n",
    "    \n",
    "    for batch_idx, images in enumerate(pbar):\n",
    "        try:\n",
    "            # Move to device\n",
    "            images = images.to(DEVICE, non_blocking=True)\n",
    "            \n",
    "            # Zero gradients\n",
    "            mae_optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with 75% masking\n",
    "            loss, pred, mask, latent = mae_model(images, mask_ratio=MAE_CONFIG['mask_ratio'])\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if MAE_CONFIG['clip_grad'] > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(mae_model.parameters(), MAE_CONFIG['clip_grad'])\n",
    "            \n",
    "            # Optimizer step\n",
    "            mae_optimizer.step()\n",
    "            \n",
    "            # Record loss\n",
    "            batch_loss = loss.item()\n",
    "            epoch_losses.append(batch_loss)\n",
    "            \n",
    "            # Update progress bar\n",
    "            current_lr = mae_optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f\"{batch_loss:.4f}\",\n",
    "                'Avg': f\"{np.mean(epoch_losses):.4f}\",\n",
    "                'LR': f\"{current_lr:.2e}\"\n",
    "            })\n",
    "            \n",
    "            # Periodic logging only (no batch checkpoint saving - only epoch-level saves)\n",
    "            if (batch_idx + 1) % MAE_CONFIG['log_frequency'] == 0:\n",
    "                elapsed = time.time() - training_start_time\n",
    "                steps_per_sec = ((epoch - start_epoch) * len(mae_dataloader) + batch_idx + 1) / elapsed\n",
    "                \n",
    "                print(f\"\\nüìä Step {(epoch - start_epoch) * len(mae_dataloader) + batch_idx + 1:,}: \"\n",
    "                      f\"Loss={batch_loss:.4f}, \"\n",
    "                      f\"Steps/sec={steps_per_sec:.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error in batch {batch_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Scheduler step\n",
    "    if mae_scheduler:\n",
    "        mae_scheduler.step()\n",
    "    \n",
    "    # Epoch statistics\n",
    "    epoch_loss = np.mean(epoch_losses)\n",
    "    training_losses.append(epoch_loss)\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    epoch_times.append(epoch_time)\n",
    "    \n",
    "    print(f\"\\nüìä EPOCH {epoch+1} COMPLETED\")\n",
    "    print(f\"üìâ Average Loss: {epoch_loss:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Epoch Time: {epoch_time/60:.1f} minutes\")\n",
    "    print(f\"üìà Learning Rate: {mae_optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Estimate remaining time\n",
    "    if len(epoch_times) >= 2:\n",
    "        avg_epoch_time = np.mean(epoch_times[-3:])  # Use last 3 epochs for estimate\n",
    "        remaining_epochs = MAE_CONFIG['epochs'] - (epoch + 1)\n",
    "        estimated_remaining = avg_epoch_time * remaining_epochs / 3600\n",
    "        print(f\"‚è≥ Estimated remaining time: {estimated_remaining:.1f} hours\")\n",
    "    \n",
    "    # Visualize reconstruction every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == start_epoch:\n",
    "        print(f\"üñºÔ∏è  Generating reconstruction visualization...\")\n",
    "        recon_loss = visualize_mae_reconstruction(mae_model, mae_dataloader, DEVICE, epoch + 1, viz_dir)\n",
    "    \n",
    "    # Save checkpoint every epoch with training history\n",
    "    checkpoint_path = os.path.join(\n",
    "        MAE_CONFIG['checkpoint_dir'], \n",
    "        f'mae_checkpoint_epoch_{epoch+1}.pth'\n",
    "    )\n",
    "    save_mae_checkpoint(mae_model, mae_optimizer, mae_scheduler, \n",
    "                      epoch + 1, epoch_loss, MAE_CONFIG, checkpoint_path, \n",
    "                      training_losses, epoch_times)\n",
    "    \n",
    "    # Save best model\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        best_checkpoint_path = os.path.join(\n",
    "            MAE_CONFIG['checkpoint_dir'], \n",
    "            'mae_best_model.pth'\n",
    "        )\n",
    "        save_mae_checkpoint(mae_model, mae_optimizer, mae_scheduler, \n",
    "                          epoch + 1, epoch_loss, MAE_CONFIG, best_checkpoint_path,\n",
    "                          training_losses, epoch_times)\n",
    "        print(f\"üèÜ NEW BEST MODEL! Loss: {best_loss:.4f}\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    if DEVICE.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    elif DEVICE.type == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    print(f\"üíæ Memory cleaned up\")\n",
    "\n",
    "# Training completed!\n",
    "total_training_time = time.time() - training_start_time\n",
    "print(f\"\\nüéâ MAE PRETRAINING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è∞ Total training time: {total_training_time/3600:.2f} hours\")\n",
    "print(f\"üèÜ Best reconstruction loss: {best_loss:.4f}\")\n",
    "print(f\"üìà Final learning rate: {mae_optimizer.param_groups[0]['lr']:.2e}\")\n",
    "print(f\"üìä Total epochs completed: {MAE_CONFIG['epochs'] - start_epoch}\")\n",
    "\n",
    "# Save final checkpoint\n",
    "final_checkpoint_path = os.path.join(\n",
    "    MAE_CONFIG['checkpoint_dir'], \n",
    "    'mae_final_model.pth'\n",
    ")\n",
    "save_mae_checkpoint(mae_model, mae_optimizer, mae_scheduler, \n",
    "                  MAE_CONFIG['epochs'], training_losses[-1], MAE_CONFIG, final_checkpoint_path,\n",
    "                  training_losses, epoch_times)\n",
    "\n",
    "print(f\"üíæ Final checkpoint saved: {final_checkpoint_path}\")\n",
    "\n",
    "# Create and save training curve\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(range(start_epoch + 1, start_epoch + 1 + len(training_losses)), training_losses, 'b-', linewidth=2)\n",
    "plt.title('MAE Training Loss', fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Reconstruction Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(range(start_epoch + 1, start_epoch + 1 + len(epoch_times)), [t/60 for t in epoch_times], 'r-', linewidth=2)\n",
    "plt.title('Epoch Training Time', fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Time (minutes)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MAE_CONFIG['checkpoint_dir'], 'mae_training_summary.png'), \n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Training summary\n",
    "print(f\"\\nüìã TRAINING SUMMARY\")\n",
    "print(\"=\"*40)\n",
    "print(f\"‚úÖ Dataset: {len(mae_dataset):,} unlabeled fish images\")\n",
    "print(f\"‚úÖ Architecture: {'ViT-Base' if use_vit_base else 'ViT-Small'}\")\n",
    "print(f\"‚úÖ Epochs completed: {MAE_CONFIG['epochs'] - start_epoch}\")\n",
    "print(f\"‚úÖ Total batches processed: {len(mae_dataloader) * (MAE_CONFIG['epochs'] - start_epoch):,}\")\n",
    "print(f\"‚úÖ Best loss achieved: {best_loss:.4f}\")\n",
    "print(f\"‚úÖ Average epoch time: {np.mean(epoch_times)/60:.1f} minutes\" if epoch_times else \"‚úÖ Average epoch time: N/A\")\n",
    "print(f\"‚úÖ Checkpoints saved to: {MAE_CONFIG['checkpoint_dir']}\")\n",
    "print(f\"‚úÖ Visualizations saved to: {viz_dir}\")\n",
    "\n",
    "# Store paths for next section\n",
    "MAE_PRETRAINED_PATH = best_checkpoint_path\n",
    "print(f\"\\nüéØ MAE PRETRAINING COMPLETE!\")\n",
    "print(f\"üîë Best model path: {MAE_PRETRAINED_PATH}\")\n",
    "print(f\"üöÄ Ready to load pretrained weights for classification!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67c2f74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ LOADING MAE PRETRAINED WEIGHTS\n",
      "==================================================\n",
      "üèóÔ∏è  Creating ViT classification model...\n",
      "‚ö†Ô∏è  MAE checkpoint not found, using ImageNet pretrained ViT\n",
      "‚ö†Ô∏è  Using ImageNet pretrained ViT with 768 features\n",
      "‚ö†Ô∏è  Using ImageNet pretrained ViT with 768 features\n",
      "\n",
      "üìä ViT Model Statistics:\n",
      "  Total parameters: 85,828,645\n",
      "  Backbone parameters: 85,798,656\n",
      "  Classifier parameters: 29,989\n",
      "  Number of classes: 37\n",
      "\n",
      "üß™ Testing ViT model...\n",
      "‚úÖ Classification forward pass successful!\n",
      "üìä Logits shape: torch.Size([2, 37])\n",
      "‚úÖ Feature extraction successful!\n",
      "üìä Features shape: torch.Size([2, 768])\n",
      "\n",
      "üéØ ViT MODEL READY FOR EMA TRAINING\n",
      "==================================================\n",
      "‚úÖ Model architecture: Vision Transformer\n",
      "‚úÖ Pretraining: ImageNet supervised\n",
      "‚úÖ Classification head: Initialized for 37 fish species\n",
      "üöÄ Ready for EMA student-teacher semi-supervised training!\n",
      "\n",
      "üìä ViT Model Statistics:\n",
      "  Total parameters: 85,828,645\n",
      "  Backbone parameters: 85,798,656\n",
      "  Classifier parameters: 29,989\n",
      "  Number of classes: 37\n",
      "\n",
      "üß™ Testing ViT model...\n",
      "‚úÖ Classification forward pass successful!\n",
      "üìä Logits shape: torch.Size([2, 37])\n",
      "‚úÖ Feature extraction successful!\n",
      "üìä Features shape: torch.Size([2, 768])\n",
      "\n",
      "üéØ ViT MODEL READY FOR EMA TRAINING\n",
      "==================================================\n",
      "‚úÖ Model architecture: Vision Transformer\n",
      "‚úÖ Pretraining: ImageNet supervised\n",
      "‚úÖ Classification head: Initialized for 37 fish species\n",
      "üöÄ Ready for EMA student-teacher semi-supervised training!\n"
     ]
    }
   ],
   "source": [
    "# Load MAE Pretrained Weights into ViT Classification Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "print(\"üîÑ LOADING MAE PRETRAINED WEIGHTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Enhanced ViT model with MAE pretraining support\n",
    "class MAEPretrainedViT(nn.Module):\n",
    "    \"\"\"ViT model that can load MAE pretrained encoder weights\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, mae_encoder=None, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        if mae_encoder is not None:\n",
    "            # Use pretrained MAE encoder\n",
    "            self.backbone = mae_encoder\n",
    "            self.feature_dim = mae_encoder.blocks[0].norm1.normalized_shape[0]  # Get embed_dim\n",
    "            print(f\"‚úÖ Using MAE pretrained encoder with {self.feature_dim} features\")\n",
    "        else:\n",
    "            # Fallback to timm ViT\n",
    "            import timm\n",
    "            self.backbone = timm.create_model(\n",
    "                'vit_base_patch16_224',\n",
    "                pretrained=True,\n",
    "                num_classes=0,\n",
    "                global_pool='token'\n",
    "            )\n",
    "            self.feature_dim = self.backbone.num_features\n",
    "            print(f\"‚ö†Ô∏è  Using ImageNet pretrained ViT with {self.feature_dim} features\")\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(self.feature_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(self.feature_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize classification head\n",
    "        self._init_classifier()\n",
    "    \n",
    "    def _init_classifier(self):\n",
    "        \"\"\"Initialize the classification head weights\"\"\"\n",
    "        for module in self.classifier.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.trunc_normal_(module.weight, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for classification\"\"\"\n",
    "        if hasattr(self.backbone, 'patch_embed'):\n",
    "            # MAE encoder forward pass\n",
    "            x = self.backbone.patch_embed(x)\n",
    "            x = x + self.backbone.pos_embed[:, 1:, :]  # Add pos embed without cls\n",
    "            \n",
    "            # Add cls token\n",
    "            cls_token = self.backbone.cls_token + self.backbone.pos_embed[:, :1, :]\n",
    "            cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "            \n",
    "            # Apply transformer blocks\n",
    "            for blk in self.backbone.blocks:\n",
    "                x = blk(x)\n",
    "            x = self.backbone.norm(x)\n",
    "            \n",
    "            # Take cls token\n",
    "            x = x[:, 0]\n",
    "        else:\n",
    "            # Standard timm ViT forward\n",
    "            x = self.backbone(x)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        \"\"\"Extract features without classification\"\"\"\n",
    "        if hasattr(self.backbone, 'patch_embed'):\n",
    "            # MAE encoder feature extraction\n",
    "            x = self.backbone.patch_embed(x)\n",
    "            x = x + self.backbone.pos_embed[:, 1:, :]\n",
    "            \n",
    "            cls_token = self.backbone.cls_token + self.backbone.pos_embed[:, :1, :]\n",
    "            cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "            \n",
    "            for blk in self.backbone.blocks:\n",
    "                x = blk(x)\n",
    "            x = self.backbone.norm(x)\n",
    "            \n",
    "            return x[:, 0]  # Return cls token features\n",
    "        else:\n",
    "            return self.backbone(x)\n",
    "\n",
    "print(f\"üèóÔ∏è  Creating ViT classification model...\")\n",
    "\n",
    "# Load MAE checkpoint\n",
    "if 'MAE_PRETRAINED_PATH' in globals() and os.path.exists(MAE_PRETRAINED_PATH):\n",
    "    print(f\"üìÇ Loading MAE checkpoint: {MAE_PRETRAINED_PATH}\")\n",
    "    \n",
    "    mae_checkpoint = torch.load(MAE_PRETRAINED_PATH, map_location='cpu')\n",
    "    print(f\"‚úÖ MAE checkpoint loaded (epoch {mae_checkpoint['epoch']})\")\n",
    "    print(f\"üìä MAE training loss: {mae_checkpoint['loss']:.4f}\")\n",
    "    \n",
    "    # Create new MAE model and load weights\n",
    "    mae_pretrained = MaskedAutoEncoder(\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        encoder_embed_dim=768,\n",
    "        encoder_depth=12,\n",
    "        encoder_num_heads=12,\n",
    "        decoder_embed_dim=512,\n",
    "        decoder_depth=8,\n",
    "        decoder_num_heads=16,\n",
    "        mlp_ratio=4.0,\n",
    "        norm_pix_loss=True\n",
    "    )\n",
    "    \n",
    "    mae_pretrained.load_state_dict(mae_checkpoint['model_state_dict'])\n",
    "    mae_encoder = mae_pretrained.encoder\n",
    "    \n",
    "    # Create ViT with MAE pretrained encoder\n",
    "    vit_model = MAEPretrainedViT(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        mae_encoder=mae_encoder,\n",
    "        dropout_rate=0.1\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(f\"‚úÖ ViT model created with MAE pretrained encoder!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  MAE checkpoint not found, using ImageNet pretrained ViT\")\n",
    "    vit_model = MAEPretrainedViT(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        mae_encoder=None,\n",
    "        dropout_rate=0.1\n",
    "    ).to(DEVICE)\n",
    "\n",
    "# Model statistics\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params = count_parameters(vit_model)\n",
    "backbone_params = count_parameters(vit_model.backbone)\n",
    "classifier_params = count_parameters(vit_model.classifier)\n",
    "\n",
    "print(f\"\\nüìä ViT Model Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Backbone parameters: {backbone_params:,}\")\n",
    "print(f\"  Classifier parameters: {classifier_params:,}\")\n",
    "print(f\"  Number of classes: {NUM_CLASSES}\")\n",
    "\n",
    "# Test model forward pass\n",
    "print(f\"\\nüß™ Testing ViT model...\")\n",
    "test_input = torch.randn(2, 3, 224, 224).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test classification\n",
    "    logits = vit_model(test_input)\n",
    "    print(f\"‚úÖ Classification forward pass successful!\")\n",
    "    print(f\"üìä Logits shape: {logits.shape}\")\n",
    "    \n",
    "    # Test feature extraction\n",
    "    features = vit_model.get_features(test_input)\n",
    "    print(f\"‚úÖ Feature extraction successful!\")\n",
    "    print(f\"üìä Features shape: {features.shape}\")\n",
    "\n",
    "# Enhanced ViT model is now ready with MAE pretrained weights\n",
    "print(f\"\\nüéØ ViT MODEL READY FOR EMA TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úÖ Model architecture: Vision Transformer\")\n",
    "print(f\"‚úÖ Pretraining: {'MAE self-supervised' if 'MAE_PRETRAINED_PATH' in globals() else 'ImageNet supervised'}\")\n",
    "print(f\"‚úÖ Classification head: Initialized for {NUM_CLASSES} fish species\")\n",
    "print(f\"üöÄ Ready for EMA student-teacher semi-supervised training!\")\n",
    "\n",
    "# Clean up MAE model to free memory\n",
    "if 'mae_model' in globals():\n",
    "    del mae_model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"üóëÔ∏è  Cleaned up MAE model to free GPU memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64effe8a",
   "metadata": {},
   "source": [
    "## üéì Section 8: Configure EMA Student-Teacher Framework\n",
    "\n",
    "Setting up the EMA teacher model and semi-supervised learning pipeline with the MAE-pretrained backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5f80a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì CONFIGURING EMA STUDENT-TEACHER FRAMEWORK\n",
      "==================================================\n",
      "üìä EMA Configuration:\n",
      "  num_classes: 37\n",
      "  img_size: 224\n",
      "  epochs: 100\n",
      "  batch_size: 32\n",
      "  learning_rate: 0.0001\n",
      "  weight_decay: 0.05\n",
      "  warmup_epochs: 10\n",
      "  ema_decay: 0.995\n",
      "  consistency_weight: 2.0\n",
      "  pseudo_label_threshold: 0.7\n",
      "  temperature: 4.0\n",
      "  ramp_up_epochs: 20\n",
      "  clip_grad: 1.0\n",
      "  label_smoothing: 0.1\n",
      "  save_frequency: 10\n",
      "  checkpoint_dir: /content/drive/MyDrive/ViT-FishID/ema_checkpoints\n",
      "  use_wandb: True\n",
      "  wandb_project: ViT-FishID-EMA-Training\n",
      "  wandb_run_name: ema-mae-pretrained-20250818-224306\n",
      "  labeled_dir: /Users/catalinathomson/Desktop/Fish/ViT-FishID/fish_cutouts/labeled\n",
      "  unlabeled_dir: /Users/catalinathomson/Desktop/Fish/ViT-FishID/fish_cutouts/unlabeled\n",
      "  train_split: 0.8\n",
      "  num_workers: 4\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 237\u001b[39m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    236\u001b[39m \u001b[38;5;66;03m# Create checkpoint directory\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEMA_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcheckpoint_dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müíæ EMA Checkpoint directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEMA_CONFIG[\u001b[33m'\u001b[39m\u001b[33mcheckpoint_dir\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# Create datasets and dataloaders\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:218\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:218\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "    \u001b[31m[... skipping similar frames: makedirs at line 218 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:218\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:228\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 30] Read-only file system: '/content'"
     ]
    }
   ],
   "source": [
    "# Configure EMA Student-Teacher Framework for Semi-Supervised Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import copy\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üéì CONFIGURING EMA STUDENT-TEACHER FRAMEWORK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# EMA Teacher Implementation\n",
    "class EMATeacher(nn.Module):\n",
    "    \"\"\"Exponential Moving Average Teacher for Semi-Supervised Learning\"\"\"\n",
    "    \n",
    "    def __init__(self, student_model, ema_decay=0.995):\n",
    "        super().__init__()\n",
    "        self.ema_decay = ema_decay\n",
    "        self.student_model = student_model\n",
    "        \n",
    "        # Create teacher as copy of student\n",
    "        self.teacher_model = copy.deepcopy(student_model)\n",
    "        \n",
    "        # Disable gradients for teacher\n",
    "        for param in self.teacher_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        print(f\"‚úÖ EMA Teacher created with decay: {ema_decay}\")\n",
    "    \n",
    "    def update_teacher(self):\n",
    "        \"\"\"Update teacher weights using EMA\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for teacher_param, student_param in zip(\n",
    "                self.teacher_model.parameters(), \n",
    "                self.student_model.parameters()\n",
    "            ):\n",
    "                teacher_param.data = (\n",
    "                    self.ema_decay * teacher_param.data + \n",
    "                    (1 - self.ema_decay) * student_param.data\n",
    "                )\n",
    "    \n",
    "    def forward(self, x, use_teacher=False):\n",
    "        \"\"\"Forward pass through student or teacher\"\"\"\n",
    "        if use_teacher:\n",
    "            return self.teacher_model(x)\n",
    "        else:\n",
    "            return self.student_model(x)\n",
    "    \n",
    "    def get_teacher_predictions(self, x):\n",
    "        \"\"\"Get teacher predictions for pseudo-labeling\"\"\"\n",
    "        self.teacher_model.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.teacher_model(x)\n",
    "\n",
    "# Consistency Loss for Semi-Supervised Learning\n",
    "class ConsistencyLoss(nn.Module):\n",
    "    \"\"\"Consistency loss between student and teacher predictions\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=4.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    def forward(self, student_logits, teacher_logits):\n",
    "        \"\"\"Compute consistency loss using KL divergence\"\"\"\n",
    "        # Apply temperature scaling\n",
    "        student_probs = F.log_softmax(student_logits / self.temperature, dim=1)\n",
    "        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n",
    "        \n",
    "        # KL divergence loss\n",
    "        consistency_loss = self.kl_div(student_probs, teacher_probs)\n",
    "        consistency_loss *= (self.temperature ** 2)\n",
    "        \n",
    "        return consistency_loss\n",
    "\n",
    "# Semi-Supervised Dataset\n",
    "class FishSemiSupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset combining labeled and unlabeled fish images\"\"\"\n",
    "    \n",
    "    def __init__(self, labeled_dir, unlabeled_dir, img_size=224, mode='train'):\n",
    "        self.img_size = img_size\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Load labeled data\n",
    "        self.labeled_data = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "        \n",
    "        if os.path.exists(labeled_dir):\n",
    "            species_dirs = [d for d in os.listdir(labeled_dir) \n",
    "                          if os.path.isdir(os.path.join(labeled_dir, d)) and not d.startswith('.')]\n",
    "            species_dirs.sort()\n",
    "            \n",
    "            for idx, species in enumerate(species_dirs):\n",
    "                self.class_to_idx[species] = idx\n",
    "                self.idx_to_class[idx] = species\n",
    "                \n",
    "                species_path = os.path.join(labeled_dir, species)\n",
    "                for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
    "                    for img_path in glob.glob(os.path.join(species_path, ext)):\n",
    "                        self.labeled_data.append((img_path, idx))\n",
    "        \n",
    "        # Load unlabeled data\n",
    "        self.unlabeled_data = []\n",
    "        if os.path.exists(unlabeled_dir):\n",
    "            for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
    "                for img_path in glob.glob(os.path.join(unlabeled_dir, ext)):\n",
    "                    self.unlabeled_data.append(img_path)\n",
    "        \n",
    "        print(f\"üìä Loaded {len(self.labeled_data)} labeled images from {len(self.class_to_idx)} species\")\n",
    "        print(f\"üìä Loaded {len(self.unlabeled_data)} unlabeled images\")\n",
    "        \n",
    "        # Data augmentation transforms\n",
    "        if mode == 'train':\n",
    "            self.labeled_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=15),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            self.unlabeled_transform_weak = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            self.unlabeled_transform_strong = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=20),\n",
    "                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.labeled_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labeled_data) + len(self.unlabeled_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.labeled_data):\n",
    "            # Labeled data\n",
    "            img_path, label = self.labeled_data[idx]\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                image = self.labeled_transform(image)\n",
    "                return image, label, True  # True indicates labeled data\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load {img_path}: {e}\")\n",
    "                return torch.zeros(3, self.img_size, self.img_size), 0, True\n",
    "        else:\n",
    "            # Unlabeled data\n",
    "            unlabeled_idx = idx - len(self.labeled_data)\n",
    "            img_path = self.unlabeled_data[unlabeled_idx]\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                \n",
    "                if self.mode == 'train':\n",
    "                    # Return both weak and strong augmentations for consistency training\n",
    "                    image_weak = self.unlabeled_transform_weak(image)\n",
    "                    image_strong = self.unlabeled_transform_strong(image)\n",
    "                    return (image_weak, image_strong), -1, False  # -1 indicates no label, False indicates unlabeled\n",
    "                else:\n",
    "                    image = self.labeled_transform(image)\n",
    "                    return image, -1, False\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load {img_path}: {e}\")\n",
    "                if self.mode == 'train':\n",
    "                    zero_img = torch.zeros(3, self.img_size, self.img_size)\n",
    "                    return (zero_img, zero_img), -1, False\n",
    "                else:\n",
    "                    return torch.zeros(3, self.img_size, self.img_size), -1, False\n",
    "\n",
    "# EMA Training Configuration\n",
    "EMA_CONFIG = {\n",
    "    # Model settings\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'img_size': 224,\n",
    "    \n",
    "    # Training settings\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32,  # Balanced for labeled + unlabeled data\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.05,\n",
    "    'warmup_epochs': 10,\n",
    "    \n",
    "    # EMA settings\n",
    "    'ema_decay': 0.995,\n",
    "    'consistency_weight': 2.0,\n",
    "    'pseudo_label_threshold': 0.7,\n",
    "    'temperature': 4.0,\n",
    "    'ramp_up_epochs': 20,\n",
    "    \n",
    "    # Optimization\n",
    "    'clip_grad': 1.0,\n",
    "    'label_smoothing': 0.1,\n",
    "    \n",
    "    # Saving\n",
    "    'save_frequency': 10,\n",
    "    'checkpoint_dir': '/content/drive/MyDrive/ViT-FishID/ema_checkpoints',\n",
    "    \n",
    "    # Logging\n",
    "    'use_wandb': True,\n",
    "    'wandb_project': 'ViT-FishID-EMA-Training',\n",
    "    'wandb_run_name': f'ema-mae-pretrained-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}',\n",
    "    \n",
    "    # Data\n",
    "    'labeled_dir': LABELED_DIR,\n",
    "    'unlabeled_dir': UNLABELED_DIR,\n",
    "    'train_split': 0.8,\n",
    "    'num_workers': 4,\n",
    "}\n",
    "\n",
    "print(\"üìä EMA Configuration:\")\n",
    "for key, value in EMA_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(EMA_CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "print(f\"\\nüíæ EMA Checkpoint directory: {EMA_CONFIG['checkpoint_dir']}\")\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "print(f\"\\nüì¶ Creating semi-supervised datasets...\")\n",
    "\n",
    "# Full dataset\n",
    "full_dataset = FishSemiSupervisedDataset(\n",
    "    EMA_CONFIG['labeled_dir'], \n",
    "    EMA_CONFIG['unlabeled_dir'], \n",
    "    EMA_CONFIG['img_size'], \n",
    "    mode='train'\n",
    ")\n",
    "\n",
    "# Split labeled data into train/validation\n",
    "labeled_size = len(full_dataset.labeled_data)\n",
    "train_size = int(EMA_CONFIG['train_split'] * labeled_size)\n",
    "val_size = labeled_size - train_size\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_labeled_data = full_dataset.labeled_data[:train_size]\n",
    "val_labeled_data = full_dataset.labeled_data[train_size:]\n",
    "\n",
    "# Training dataset (includes all unlabeled data)\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, labeled_data, unlabeled_data, class_to_idx, img_size):\n",
    "        self.labeled_data = labeled_data\n",
    "        self.unlabeled_data = unlabeled_data\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Same transforms as before\n",
    "        self.labeled_transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.unlabeled_transform_weak = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.unlabeled_transform_strong = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=20),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labeled_data) + len(self.unlabeled_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.labeled_data):\n",
    "            # Labeled data\n",
    "            img_path, label = self.labeled_data[idx]\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                image = self.labeled_transform(image)\n",
    "                return image, label, True\n",
    "            except:\n",
    "                return torch.zeros(3, self.img_size, self.img_size), 0, True\n",
    "        else:\n",
    "            # Unlabeled data\n",
    "            unlabeled_idx = idx - len(self.labeled_data)\n",
    "            img_path = self.unlabeled_data[unlabeled_idx]\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                image_weak = self.unlabeled_transform_weak(image)\n",
    "                image_strong = self.unlabeled_transform_strong(image)\n",
    "                return (image_weak, image_strong), -1, False\n",
    "            except:\n",
    "                zero_img = torch.zeros(3, self.img_size, self.img_size)\n",
    "                return (zero_img, zero_img), -1, False\n",
    "\n",
    "# Validation dataset\n",
    "class ValDataset(Dataset):\n",
    "    def __init__(self, labeled_data, class_to_idx, img_size):\n",
    "        self.labeled_data = labeled_data\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labeled_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.labeled_data[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "            return image, label\n",
    "        except:\n",
    "            return torch.zeros(3, self.img_size, self.img_size), 0\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TrainDataset(\n",
    "    train_labeled_data, \n",
    "    full_dataset.unlabeled_data, \n",
    "    full_dataset.class_to_idx, \n",
    "    EMA_CONFIG['img_size']\n",
    ")\n",
    "\n",
    "val_dataset = ValDataset(\n",
    "    val_labeled_data, \n",
    "    full_dataset.class_to_idx, \n",
    "    EMA_CONFIG['img_size']\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=EMA_CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=EMA_CONFIG['num_workers'],\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=EMA_CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=EMA_CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Datasets created:\")\n",
    "print(f\"  üìä Training: {len(train_dataset)} samples ({len(train_labeled_data)} labeled + {len(full_dataset.unlabeled_data)} unlabeled)\")\n",
    "print(f\"  üìä Validation: {len(val_dataset)} samples (labeled)\")\n",
    "print(f\"  üìä Classes: {len(full_dataset.class_to_idx)}\")\n",
    "\n",
    "# Create EMA teacher\n",
    "ema_teacher = EMATeacher(vit_model, ema_decay=EMA_CONFIG['ema_decay']).to(DEVICE)\n",
    "consistency_loss_fn = ConsistencyLoss(temperature=EMA_CONFIG['temperature']).to(DEVICE)\n",
    "\n",
    "print(f\"\\n‚úÖ EMA Framework Ready:\")\n",
    "print(f\"  üéì Student model: MAE-pretrained ViT\")\n",
    "print(f\"  üë®‚Äçüè´ Teacher model: EMA with decay {EMA_CONFIG['ema_decay']}\")\n",
    "print(f\"  üîÑ Consistency loss: KL divergence with temperature {EMA_CONFIG['temperature']}\")\n",
    "print(f\"üöÄ Ready for semi-supervised training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c5a67",
   "metadata": {},
   "source": [
    "## üöÄ Section 9: Execute Semi-Supervised Training with EMA\n",
    "\n",
    "Running the complete semi-supervised training pipeline combining labeled supervision with unlabeled consistency learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b01226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Semi-Supervised EMA Training with MAE Pretrained Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üéì STARTING EMA SEMI-SUPERVISED TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "ema_optimizer = optim.AdamW(\n",
    "    vit_model.parameters(),\n",
    "    lr=EMA_CONFIG['learning_rate'],\n",
    "    weight_decay=EMA_CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Cosine annealing scheduler with warmup\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))\n",
    "    \n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "total_steps = len(train_dataloader) * EMA_CONFIG['epochs']\n",
    "warmup_steps = len(train_dataloader) * EMA_CONFIG['warmup_epochs']\n",
    "\n",
    "ema_scheduler = get_cosine_schedule_with_warmup(\n",
    "    ema_optimizer, \n",
    "    warmup_steps, \n",
    "    total_steps\n",
    ")\n",
    "\n",
    "# Loss functions\n",
    "supervised_loss_fn = nn.CrossEntropyLoss(label_smoothing=EMA_CONFIG['label_smoothing'])\n",
    "\n",
    "# Consistency weight ramp-up function\n",
    "def get_consistency_weight(epoch, ramp_up_epochs):\n",
    "    \"\"\"Gradually ramp up consistency weight\"\"\"\n",
    "    if epoch < ramp_up_epochs:\n",
    "        return EMA_CONFIG['consistency_weight'] * (epoch / ramp_up_epochs)\n",
    "    return EMA_CONFIG['consistency_weight']\n",
    "\n",
    "# Pseudo-labeling function\n",
    "def get_pseudo_labels(teacher_logits, threshold):\n",
    "    \"\"\"Generate pseudo-labels from teacher predictions\"\"\"\n",
    "    teacher_probs = F.softmax(teacher_logits, dim=1)\n",
    "    max_probs, pseudo_labels = torch.max(teacher_probs, dim=1)\n",
    "    \n",
    "    # Create mask for confident predictions\n",
    "    confident_mask = max_probs >= threshold\n",
    "    \n",
    "    return pseudo_labels, confident_mask, max_probs\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, val_dataloader, device):\n",
    "    \"\"\"Validate model on labeled validation set\"\"\"\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    accuracy = 100.0 * total_correct / total_samples\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "    \n",
    "    return accuracy, avg_loss\n",
    "\n",
    "# Save checkpoint function\n",
    "def save_ema_checkpoint(student_model, teacher_model, optimizer, scheduler, epoch, \n",
    "                       best_acc, config, filename):\n",
    "    \"\"\"Save EMA training checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'student_state_dict': student_model.state_dict(),\n",
    "        'teacher_state_dict': teacher_model.teacher_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_accuracy': best_acc,\n",
    "        'config': config,\n",
    "        'class_to_idx': full_dataset.class_to_idx,\n",
    "        'num_classes': config['num_classes']\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"üíæ Saved checkpoint: {filename}\")\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "if EMA_CONFIG['use_wandb']:\n",
    "    print(\"üìà Initializing Weights & Biases...\")\n",
    "    try:\n",
    "        wandb.init(\n",
    "            project=EMA_CONFIG['wandb_project'],\n",
    "            name=EMA_CONFIG['wandb_run_name'],\n",
    "            config=EMA_CONFIG,\n",
    "            tags=['ema', 'semi-supervised', 'fish', 'mae-pretrained']\n",
    "        )\n",
    "        print(f\"‚úÖ W&B initialized: {wandb.run.url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  W&B initialization failed: {e}\")\n",
    "        EMA_CONFIG['use_wandb'] = False\n",
    "\n",
    "# Training loop\n",
    "print(f\"üé¨ Starting EMA training for {EMA_CONFIG['epochs']} epochs...\")\n",
    "print(f\"üìä Training data: {len(train_dataset)} samples\")\n",
    "print(f\"üìä Validation data: {len(val_dataset)} samples\")\n",
    "print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "vit_model.train()\n",
    "ema_teacher.student_model.train()\n",
    "ema_teacher.teacher_model.eval()\n",
    "\n",
    "best_accuracy = 0.0\n",
    "training_history = {\n",
    "    'supervised_loss': [],\n",
    "    'consistency_loss': [],\n",
    "    'total_loss': [],\n",
    "    'val_accuracy': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EMA_CONFIG['epochs']):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    vit_model.train()\n",
    "    epoch_supervised_loss = 0.0\n",
    "    epoch_consistency_loss = 0.0\n",
    "    epoch_total_loss = 0.0\n",
    "    num_labeled_samples = 0\n",
    "    num_unlabeled_samples = 0\n",
    "    \n",
    "    # Get current consistency weight\n",
    "    current_consistency_weight = get_consistency_weight(epoch, EMA_CONFIG['ramp_up_epochs'])\n",
    "    \n",
    "    pbar = tqdm(train_dataloader, desc=f\"EMA Epoch {epoch+1}/{EMA_CONFIG['epochs']}\")\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(pbar):\n",
    "        # Separate labeled and unlabeled data\n",
    "        labeled_data = []\n",
    "        unlabeled_data = []\n",
    "        \n",
    "        for data, label, is_labeled in zip(*batch_data):\n",
    "            if is_labeled:\n",
    "                labeled_data.append((data, label))\n",
    "            else:\n",
    "                unlabeled_data.append(data)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        supervised_loss = torch.tensor(0.0).to(DEVICE)\n",
    "        consistency_loss = torch.tensor(0.0).to(DEVICE)\n",
    "        \n",
    "        # Process labeled data\n",
    "        if labeled_data:\n",
    "            labeled_images = torch.stack([data for data, _ in labeled_data]).to(DEVICE)\n",
    "            labeled_targets = torch.tensor([label for _, label in labeled_data]).to(DEVICE)\n",
    "            \n",
    "            # Student forward pass\n",
    "            student_outputs = vit_model(labeled_images)\n",
    "            supervised_loss = supervised_loss_fn(student_outputs, labeled_targets)\n",
    "            \n",
    "            num_labeled_samples += len(labeled_data)\n",
    "        \n",
    "        # Process unlabeled data\n",
    "        if unlabeled_data and current_consistency_weight > 0:\n",
    "            # Unlabeled data comes as (weak_aug, strong_aug) tuples\n",
    "            weak_images = torch.stack([data[0] for data in unlabeled_data]).to(DEVICE)\n",
    "            strong_images = torch.stack([data[1] for data in unlabeled_data]).to(DEVICE)\n",
    "            \n",
    "            # Teacher predictions on weakly augmented images\n",
    "            teacher_outputs = ema_teacher.get_teacher_predictions(weak_images)\n",
    "            \n",
    "            # Generate pseudo-labels\n",
    "            pseudo_labels, confident_mask, max_probs = get_pseudo_labels(\n",
    "                teacher_outputs, EMA_CONFIG['pseudo_label_threshold']\n",
    "            )\n",
    "            \n",
    "            if confident_mask.sum() > 0:\n",
    "                # Student predictions on strongly augmented images\n",
    "                student_outputs_unlabeled = vit_model(strong_images)\n",
    "                \n",
    "                # Consistency loss only for confident predictions\n",
    "                if confident_mask.sum() > 0:\n",
    "                    student_confident = student_outputs_unlabeled[confident_mask]\n",
    "                    teacher_confident = teacher_outputs[confident_mask]\n",
    "                    \n",
    "                    consistency_loss = consistency_loss_fn(student_confident, teacher_confident)\n",
    "                    consistency_loss *= current_consistency_weight\n",
    "            \n",
    "            num_unlabeled_samples += len(unlabeled_data)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = supervised_loss + consistency_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        ema_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if EMA_CONFIG['clip_grad'] > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(vit_model.parameters(), EMA_CONFIG['clip_grad'])\n",
    "        \n",
    "        ema_optimizer.step()\n",
    "        ema_scheduler.step()\n",
    "        \n",
    "        # Update teacher with EMA\n",
    "        ema_teacher.update_teacher()\n",
    "        \n",
    "        # Record losses\n",
    "        epoch_supervised_loss += supervised_loss.item()\n",
    "        epoch_consistency_loss += consistency_loss.item()\n",
    "        epoch_total_loss += total_loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'Sup_Loss': f\"{supervised_loss.item():.4f}\",\n",
    "            'Con_Loss': f\"{consistency_loss.item():.4f}\",\n",
    "            'Con_Weight': f\"{current_consistency_weight:.3f}\",\n",
    "            'LR': f\"{ema_optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        })\n",
    "        \n",
    "        # Log to wandb\n",
    "        if EMA_CONFIG['use_wandb'] and wandb.run:\n",
    "            try:\n",
    "                wandb.log({\n",
    "                    'batch_supervised_loss': supervised_loss.item(),\n",
    "                    'batch_consistency_loss': consistency_loss.item(),\n",
    "                    'batch_total_loss': total_loss.item(),\n",
    "                    'consistency_weight': current_consistency_weight,\n",
    "                    'learning_rate': ema_optimizer.param_groups[0]['lr'],\n",
    "                    'step': epoch * len(train_dataloader) + batch_idx\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Validation phase\n",
    "    val_accuracy, val_loss = validate_model(vit_model, val_dataloader, DEVICE)\n",
    "    \n",
    "    # Epoch statistics\n",
    "    avg_supervised_loss = epoch_supervised_loss / len(train_dataloader)\n",
    "    avg_consistency_loss = epoch_consistency_loss / len(train_dataloader)\n",
    "    avg_total_loss = epoch_total_loss / len(train_dataloader)\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    training_history['supervised_loss'].append(avg_supervised_loss)\n",
    "    training_history['consistency_loss'].append(avg_consistency_loss)\n",
    "    training_history['total_loss'].append(avg_total_loss)\n",
    "    training_history['val_accuracy'].append(val_accuracy)\n",
    "    training_history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f\"\\nüìä Epoch {epoch+1} Summary:\")\n",
    "    print(f\"  üìâ Supervised Loss: {avg_supervised_loss:.4f}\")\n",
    "    print(f\"  üìâ Consistency Loss: {avg_consistency_loss:.4f}\")\n",
    "    print(f\"  üìâ Total Loss: {avg_total_loss:.4f}\")\n",
    "    print(f\"  üìà Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "    print(f\"  üìâ Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"  ‚è±Ô∏è  Time: {epoch_time:.1f}s\")\n",
    "    print(f\"  üéì Labeled samples: {num_labeled_samples}\")\n",
    "    print(f\"  üîÑ Unlabeled samples: {num_unlabeled_samples}\")\n",
    "    \n",
    "    # Save checkpoint periodically\n",
    "    if (epoch + 1) % EMA_CONFIG['save_frequency'] == 0:\n",
    "        checkpoint_path = os.path.join(\n",
    "            EMA_CONFIG['checkpoint_dir'], \n",
    "            f'ema_checkpoint_epoch_{epoch+1}.pth'\n",
    "        )\n",
    "        save_ema_checkpoint(\n",
    "            vit_model, ema_teacher, ema_optimizer, ema_scheduler,\n",
    "            epoch + 1, val_accuracy, EMA_CONFIG, checkpoint_path\n",
    "        )\n",
    "    \n",
    "    # Save best model\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_checkpoint_path = os.path.join(\n",
    "            EMA_CONFIG['checkpoint_dir'], \n",
    "            'ema_best_model.pth'\n",
    "        )\n",
    "        save_ema_checkpoint(\n",
    "            vit_model, ema_teacher, ema_optimizer, ema_scheduler,\n",
    "            epoch + 1, val_accuracy, EMA_CONFIG, best_checkpoint_path\n",
    "        )\n",
    "        print(f\"üèÜ New best model saved! Accuracy: {best_accuracy:.2f}%\")\n",
    "    \n",
    "    # Log epoch results to wandb\n",
    "    if EMA_CONFIG['use_wandb'] and wandb.run:\n",
    "        try:\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'epoch_supervised_loss': avg_supervised_loss,\n",
    "                'epoch_consistency_loss': avg_consistency_loss,\n",
    "                'epoch_total_loss': avg_total_loss,\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'val_loss': val_loss,\n",
    "                'best_accuracy': best_accuracy,\n",
    "                'epoch_time': epoch_time\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Memory cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Training completed\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nüéâ EMA TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è∞ Total training time: {total_time/3600:.2f} hours\")\n",
    "print(f\"üèÜ Best validation accuracy: {best_accuracy:.2f}%\")\n",
    "print(f\"üìà Final learning rate: {ema_optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "# Save final checkpoint\n",
    "final_checkpoint_path = os.path.join(\n",
    "    EMA_CONFIG['checkpoint_dir'], \n",
    "    'ema_final_model.pth'\n",
    ")\n",
    "save_ema_checkpoint(\n",
    "    vit_model, ema_teacher, ema_optimizer, ema_scheduler,\n",
    "    EMA_CONFIG['epochs'], training_history['val_accuracy'][-1], \n",
    "    EMA_CONFIG, final_checkpoint_path\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss curves\n",
    "epochs = range(1, len(training_history['total_loss']) + 1)\n",
    "ax1.plot(epochs, training_history['supervised_loss'], 'b-', label='Supervised Loss', linewidth=2)\n",
    "ax1.plot(epochs, training_history['consistency_loss'], 'r-', label='Consistency Loss', linewidth=2)\n",
    "ax1.plot(epochs, training_history['total_loss'], 'g-', label='Total Loss', linewidth=2)\n",
    "ax1.set_title('Training Loss Curves', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy\n",
    "ax2.plot(epochs, training_history['val_accuracy'], 'purple', linewidth=2)\n",
    "ax2.set_title('Validation Accuracy', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "ax3.plot(epochs, training_history['val_loss'], 'orange', linewidth=2)\n",
    "ax3.set_title('Validation Loss', fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss ratio\n",
    "consistency_ratio = [c/(s+1e-8) for s, c in zip(training_history['supervised_loss'], \n",
    "                                                training_history['consistency_loss'])]\n",
    "ax4.plot(epochs, consistency_ratio, 'brown', linewidth=2)\n",
    "ax4.set_title('Consistency/Supervised Loss Ratio', fontweight='bold')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Ratio')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(EMA_CONFIG['checkpoint_dir'], 'ema_training_curves.png'), \n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final wandb logging\n",
    "if EMA_CONFIG['use_wandb'] and wandb.run:\n",
    "    try:\n",
    "        wandb.log({\n",
    "            'final_best_accuracy': best_accuracy,\n",
    "            'final_val_accuracy': training_history['val_accuracy'][-1],\n",
    "            'total_training_time_hours': total_time/3600,\n",
    "            'total_epochs': EMA_CONFIG['epochs']\n",
    "        })\n",
    "        wandb.finish()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\n‚úÖ Semi-supervised EMA training with MAE pretraining completed!\")\n",
    "print(f\"üíæ Checkpoints saved to: {EMA_CONFIG['checkpoint_dir']}\")\n",
    "print(f\"üéØ Best model achieved {best_accuracy:.2f}% accuracy!\")\n",
    "print(f\"üöÄ Model ready for evaluation and deployment!\")\n",
    "\n",
    "# Store paths for evaluation\n",
    "EMA_BEST_MODEL_PATH = best_checkpoint_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3789d94",
   "metadata": {},
   "source": [
    "## üìä Section 10: Monitor Training Progress and Save Checkpoints\n",
    "\n",
    "Tracking training metrics, analyzing model performance, and managing checkpoint saves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111848cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor Training Progress and Manage Checkpoints\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "print(\"üìä TRAINING PROGRESS MONITORING & CHECKPOINT MANAGEMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def analyze_training_checkpoints(checkpoint_dir):\n",
    "    \"\"\"Analyze all training checkpoints and extract metrics\"\"\"\n",
    "    \n",
    "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'ema_checkpoint_epoch_*.pth'))\n",
    "    checkpoint_files.sort(key=lambda x: int(x.split('epoch_')[1].split('.')[0]))\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        print(\"‚ùå No checkpoint files found\")\n",
    "        return None\n",
    "    \n",
    "    checkpoint_data = []\n",
    "    \n",
    "    print(f\"üìÇ Found {len(checkpoint_files)} checkpoint files\")\n",
    "    print(\"\\nüìä Checkpoint Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Epoch':<8} {'Accuracy':<12} {'File Size':<12} {'Timestamp'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for checkpoint_file in checkpoint_files:\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_file, map_location='cpu')\n",
    "            \n",
    "            epoch = checkpoint.get('epoch', 0)\n",
    "            accuracy = checkpoint.get('best_accuracy', 0)\n",
    "            file_size = os.path.getsize(checkpoint_file) / (1024 * 1024)  # MB\n",
    "            timestamp = datetime.fromtimestamp(os.path.getmtime(checkpoint_file))\n",
    "            \n",
    "            checkpoint_data.append({\n",
    "                'epoch': epoch,\n",
    "                'accuracy': accuracy,\n",
    "                'file_size': file_size,\n",
    "                'timestamp': timestamp,\n",
    "                'file_path': checkpoint_file\n",
    "            })\n",
    "            \n",
    "            print(f\"{epoch:<8} {accuracy:<12.2f} {file_size:<12.1f} {timestamp.strftime('%H:%M:%S')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not load {checkpoint_file}: {e}\")\n",
    "    \n",
    "    return checkpoint_data\n",
    "\n",
    "def visualize_training_progress(checkpoint_dir, training_history=None):\n",
    "    \"\"\"Create comprehensive training progress visualizations\"\"\"\n",
    "    \n",
    "    if training_history is None:\n",
    "        print(\"‚ö†Ô∏è  Training history not available, using checkpoint data only\")\n",
    "        return\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # 1. Loss progression\n",
    "    ax1 = plt.subplot(2, 4, 1)\n",
    "    epochs = range(1, len(training_history['total_loss']) + 1)\n",
    "    ax1.plot(epochs, training_history['supervised_loss'], 'b-', label='Supervised', linewidth=2)\n",
    "    ax1.plot(epochs, training_history['consistency_loss'], 'r-', label='Consistency', linewidth=2)\n",
    "    ax1.plot(epochs, training_history['total_loss'], 'g-', label='Total', linewidth=2)\n",
    "    ax1.set_title('Training Loss Progression', fontweight='bold', fontsize=12)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Validation accuracy\n",
    "    ax2 = plt.subplot(2, 4, 2)\n",
    "    ax2.plot(epochs, training_history['val_accuracy'], 'purple', linewidth=3)\n",
    "    ax2.set_title('Validation Accuracy', fontweight='bold', fontsize=12)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add best accuracy line\n",
    "    best_acc = max(training_history['val_accuracy'])\n",
    "    ax2.axhline(y=best_acc, color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'Best: {best_acc:.2f}%')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Loss smoothed (moving average)\n",
    "    ax3 = plt.subplot(2, 4, 3)\n",
    "    window = min(10, len(epochs) // 4)\n",
    "    if window > 1:\n",
    "        smoothed_total = np.convolve(training_history['total_loss'], \n",
    "                                   np.ones(window)/window, mode='valid')\n",
    "        smoothed_epochs = epochs[window-1:]\n",
    "        ax3.plot(smoothed_epochs, smoothed_total, 'darkgreen', linewidth=2)\n",
    "    ax3.set_title(f'Smoothed Total Loss (window={window})', fontweight='bold', fontsize=12)\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Learning dynamics\n",
    "    ax4 = plt.subplot(2, 4, 4)\n",
    "    consistency_ratio = [c/(s+1e-8) for s, c in zip(training_history['supervised_loss'], \n",
    "                                                    training_history['consistency_loss'])]\n",
    "    ax4.plot(epochs, consistency_ratio, 'brown', linewidth=2)\n",
    "    ax4.set_title('Consistency/Supervised Ratio', fontweight='bold', fontsize=12)\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Ratio')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Validation loss vs accuracy\n",
    "    ax5 = plt.subplot(2, 4, 5)\n",
    "    scatter = ax5.scatter(training_history['val_loss'], training_history['val_accuracy'], \n",
    "                         c=epochs, cmap='viridis', alpha=0.7)\n",
    "    ax5.set_title('Validation Loss vs Accuracy', fontweight='bold', fontsize=12)\n",
    "    ax5.set_xlabel('Validation Loss')\n",
    "    ax5.set_ylabel('Validation Accuracy (%)')\n",
    "    plt.colorbar(scatter, ax=ax5, label='Epoch')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Training efficiency\n",
    "    ax6 = plt.subplot(2, 4, 6)\n",
    "    total_loss_diff = np.diff(training_history['total_loss'])\n",
    "    ax6.plot(epochs[1:], total_loss_diff, 'orange', linewidth=2)\n",
    "    ax6.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax6.set_title('Loss Change Rate', fontweight='bold', fontsize=12)\n",
    "    ax6.set_xlabel('Epoch')\n",
    "    ax6.set_ylabel('Loss Œî')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Accuracy improvement\n",
    "    ax7 = plt.subplot(2, 4, 7)\n",
    "    acc_diff = np.diff(training_history['val_accuracy'])\n",
    "    ax7.plot(epochs[1:], acc_diff, 'darkblue', linewidth=2)\n",
    "    ax7.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax7.set_title('Accuracy Change Rate', fontweight='bold', fontsize=12)\n",
    "    ax7.set_xlabel('Epoch')\n",
    "    ax7.set_ylabel('Accuracy Œî (%)')\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 8. Training summary stats\n",
    "    ax8 = plt.subplot(2, 4, 8)\n",
    "    ax8.axis('off')\n",
    "    \n",
    "    # Calculate statistics\n",
    "    final_acc = training_history['val_accuracy'][-1]\n",
    "    best_acc = max(training_history['val_accuracy'])\n",
    "    best_epoch = training_history['val_accuracy'].index(best_acc) + 1\n",
    "    acc_improvement = final_acc - training_history['val_accuracy'][0]\n",
    "    \n",
    "    summary_text = f\\\"\\\"\\\"Training Summary\n",
    "    \n",
    "üìä Final Accuracy: {final_acc:.2f}%\n",
    "üèÜ Best Accuracy: {best_acc:.2f}%\n",
    "üéØ Best Epoch: {best_epoch}\n",
    "üìà Total Improvement: {acc_improvement:.2f}%\n",
    "üî• Epochs Trained: {len(epochs)}\n",
    "\n",
    "üí° Loss Components:\n",
    "üìâ Final Supervised: {training_history['supervised_loss'][-1]:.4f}\n",
    "üîÑ Final Consistency: {training_history['consistency_loss'][-1]:.4f}\n",
    "‚öñÔ∏è Final Total: {training_history['total_loss'][-1]:.4f}\n",
    "    \\\"\\\"\\\"\n",
    "    \n",
    "    ax8.text(0.1, 0.9, summary_text, transform=ax8.transAxes, fontsize=10,\n",
    "            verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the visualization\n",
    "    viz_path = os.path.join(checkpoint_dir, 'training_analysis.png')\n",
    "    plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìä Training analysis saved to: {viz_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def backup_best_models(checkpoint_dir, backup_dir=None):\n",
    "    \"\"\"Backup the best models to Google Drive\"\"\"\n",
    "    \n",
    "    if backup_dir is None:\n",
    "        backup_dir = '/content/drive/MyDrive/ViT-FishID_MAE_EMA_Backup'\n",
    "    \n",
    "    os.makedirs(backup_dir, exist_ok=True)\n",
    "    \n",
    "    # Find best model files\n",
    "    best_files = ['ema_best_model.pth', 'ema_final_model.pth']\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    for filename in best_files:\n",
    "        source_path = os.path.join(checkpoint_dir, filename)\n",
    "        if os.path.exists(source_path):\n",
    "            # Create timestamped backup\n",
    "            backup_filename = f\\\"{timestamp}_{filename}\\\"\n",
    "            backup_path = os.path.join(backup_dir, backup_filename)\\n            \\n            try:\\n                shutil.copy2(source_path, backup_path)\\n                file_size = os.path.getsize(backup_path) / (1024 * 1024)  # MB\\n                print(f\\\"‚úÖ Backed up {filename} -> {backup_filename} ({file_size:.1f} MB)\\\")\\n            except Exception as e:\\n                print(f\\\"‚ùå Failed to backup {filename}: {e}\\\")\\n        else:\\n            print(f\\\"‚ö†Ô∏è  {filename} not found in {checkpoint_dir}\\\")\\n    \\n    print(f\\\"üíæ Backups saved to: {backup_dir}\\\")\\n\\n# Run analysis if checkpoints exist\\nif 'EMA_CONFIG' in globals() and os.path.exists(EMA_CONFIG['checkpoint_dir']):\\n    print(f\\\"üìÅ Analyzing checkpoints in: {EMA_CONFIG['checkpoint_dir']}\\\")\\n    \\n    # Analyze checkpoints\\n    checkpoint_data = analyze_training_checkpoints(EMA_CONFIG['checkpoint_dir'])\\n    \\n    # Visualize training progress\\n    if 'training_history' in globals():\\n        print(\\\"\\\\nüìä Creating training progress visualization...\\\")\\n        fig = visualize_training_progress(EMA_CONFIG['checkpoint_dir'], training_history)\\n    \\n    # Backup best models\\n    print(\\\"\\\\nüíæ Backing up best models to Google Drive...\\\")\\n    backup_best_models(EMA_CONFIG['checkpoint_dir'])\\n    \\n    # Model size analysis\\n    print(\\\"\\\\nüìä MODEL SIZE ANALYSIS:\\\")\\n    print(\\\"=\\\"*40)\\n    \\n    if checkpoint_data:\\n        latest_checkpoint = max(checkpoint_data, key=lambda x: x['epoch'])\\n        print(f\\\"Latest checkpoint size: {latest_checkpoint['file_size']:.1f} MB\\\")\\n        \\n        total_size = sum(cp['file_size'] for cp in checkpoint_data)\\n        print(f\\\"Total checkpoint storage: {total_size:.1f} MB\\\")\\n        \\n        avg_size = total_size / len(checkpoint_data)\\n        print(f\\\"Average checkpoint size: {avg_size:.1f} MB\\\")\\n    \\n    # Training efficiency metrics\\n    if 'training_history' in globals() and 'total_time' in globals():\\n        print(\\\"\\\\n‚ö° TRAINING EFFICIENCY:\\\")\\n        print(\\\"=\\\"*40)\\n        \\n        total_epochs = len(training_history['val_accuracy'])\\n        time_per_epoch = total_time / total_epochs\\n        \\n        print(f\\\"Total training time: {total_time/3600:.2f} hours\\\")\\n        print(f\\\"Time per epoch: {time_per_epoch:.1f} seconds\\\")\\n        print(f\\\"Final accuracy: {training_history['val_accuracy'][-1]:.2f}%\\\")\\n        print(f\\\"Best accuracy: {max(training_history['val_accuracy']):.2f}%\\\")\\n        \\n        # Accuracy per hour\\n        acc_per_hour = max(training_history['val_accuracy']) / (total_time / 3600)\\n        print(f\\\"Accuracy gained per hour: {acc_per_hour:.2f}%/hr\\\")\\n    \\n    # Disk usage summary\\n    print(\\\"\\\\nüíæ STORAGE SUMMARY:\\\")\\n    print(\\\"=\\\"*40)\\n    \\n    checkpoint_size = sum(os.path.getsize(os.path.join(EMA_CONFIG['checkpoint_dir'], f)) \\n                         for f in os.listdir(EMA_CONFIG['checkpoint_dir']) \\n                         if f.endswith('.pth')) / (1024 * 1024)  # MB\\n    \\n    mae_checkpoint_size = 0\\n    if 'MAE_CONFIG' in globals() and os.path.exists(MAE_CONFIG['checkpoint_dir']):\\n        mae_checkpoint_size = sum(os.path.getsize(os.path.join(MAE_CONFIG['checkpoint_dir'], f)) \\n                                 for f in os.listdir(MAE_CONFIG['checkpoint_dir']) \\n                                 if f.endswith('.pth')) / (1024 * 1024)  # MB\\n    \\n    total_storage = checkpoint_size + mae_checkpoint_size\\n    \\n    print(f\\\"EMA checkpoints: {checkpoint_size:.1f} MB\\\")\\n    print(f\\\"MAE checkpoints: {mae_checkpoint_size:.1f} MB\\\")\\n    print(f\\\"Total storage used: {total_storage:.1f} MB\\\")\\n    \\nelse:\\n    print(\\\"‚ö†Ô∏è  No checkpoint directory found. Training may not have completed.\\\")\\n\\nprint(f\\\"\\\\n‚úÖ Training monitoring and checkpoint management complete!\\\")\\nprint(f\\\"üìä All analysis saved to checkpoint directories\\\")\\nprint(f\\\"üíæ Best models backed up to Google Drive\\\")\\nprint(f\\\"üéØ Ready for model evaluation and deployment!\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480fab9",
   "metadata": {},
   "source": [
    "## üß™ Section 11: Evaluate Final Model Performance\n",
    "\n",
    "Comprehensive evaluation of the trained model with performance comparisons and deployment preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Evaluation and Performance Analysis\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "\n",
    "print(\"üß™ COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def load_best_model(checkpoint_path, device):\n",
    "    \"\"\"Load the best trained model\"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"‚ùå Checkpoint not found: {checkpoint_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    print(f\"üìÇ Loading checkpoint from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"üèÜ Best accuracy: {checkpoint['best_accuracy']:.2f}%\")\n",
    "    \n",
    "    # Recreate model architecture\n",
    "    model = MAEPretrainedViT(\n",
    "        num_classes=checkpoint['num_classes'],\n",
    "        mae_encoder=None,  # Will load weights directly\n",
    "        dropout_rate=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load state dict\n",
    "    model.load_state_dict(checkpoint['student_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    class_to_idx = checkpoint.get('class_to_idx', {})\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    \n",
    "    return model, idx_to_class\n",
    "\n",
    "def create_test_dataset(labeled_dir, class_to_idx, img_size=224, test_split=0.2):\n",
    "    \\\"\\\"\\\"Create a test dataset from labeled data\\\"\\\"\\\"\\n    \\n    test_data = []\\n    \\n    for species, class_idx in class_to_idx.items():\\n        species_path = os.path.join(labeled_dir, species)\\n        if not os.path.exists(species_path):\\n            continue\\n            \\n        # Get all images for this species\\n        images = []\\n        for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\\n            images.extend(glob.glob(os.path.join(species_path, ext)))\\n        \\n        # Take last 20% as test set (assuming first 80% used for training)\\n        test_count = max(1, int(len(images) * test_split))\\n        test_images = images[-test_count:]\\n        \\n        for img_path in test_images:\\n            test_data.append((img_path, class_idx))\\n    \\n    print(f\\\"üìä Created test set with {len(test_data)} images\\\")\\n    return test_data\\n\\ndef evaluate_model_comprehensive(model, test_data, idx_to_class, device, img_size=224):\\n    \\\"\\\"\\\"Comprehensive model evaluation\\\"\\\"\\\"\\n    \\n    model.eval()\\n    \\n    # Prepare data transforms\\n    test_transform = transforms.Compose([\\n        transforms.Resize((img_size, img_size)),\\n        transforms.ToTensor(),\\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n    ])\\n    \\n    all_predictions = []\\n    all_labels = []\\n    all_probabilities = []\\n    prediction_details = []\\n    \\n    print(f\\\"üß™ Evaluating on {len(test_data)} test images...\\\")\\n    \\n    with torch.no_grad():\\n        for i, (img_path, true_label) in enumerate(tqdm(test_data, desc=\\\"Evaluating\\\")):\\n            try:\\n                # Load and preprocess image\\n                image = Image.open(img_path).convert('RGB')\\n                image_tensor = test_transform(image).unsqueeze(0).to(device)\\n                \\n                # Get model prediction\\n                outputs = model(image_tensor)\\n                probabilities = F.softmax(outputs, dim=1)\\n                predicted_class = torch.argmax(outputs, dim=1).item()\\n                confidence = probabilities[0, predicted_class].item()\\n                \\n                all_predictions.append(predicted_class)\\n                all_labels.append(true_label)\\n                all_probabilities.append(probabilities.cpu().numpy()[0])\\n                \\n                prediction_details.append({\\n                    'image_path': img_path,\\n                    'true_label': true_label,\\n                    'true_species': idx_to_class[true_label],\\n                    'predicted_label': predicted_class,\\n                    'predicted_species': idx_to_class[predicted_class],\\n                    'confidence': confidence,\\n                    'correct': predicted_class == true_label\\n                })\\n                \\n            except Exception as e:\\n                print(f\\\"‚ö†Ô∏è  Error processing {img_path}: {e}\\\")\\n                continue\\n    \\n    return all_predictions, all_labels, all_probabilities, prediction_details\\n\\ndef analyze_results(predictions, labels, probabilities, prediction_details, idx_to_class):\\n    \\\"\\\"\\\"Analyze evaluation results\\\"\\\"\\\"\\n    \\n    # Basic metrics\\n    accuracy = np.mean(np.array(predictions) == np.array(labels)) * 100\\n    \\n    # Top-k accuracy\\n    top3_acc = top_k_accuracy_score(labels, probabilities, k=3) * 100\\n    top5_acc = top_k_accuracy_score(labels, probabilities, k=5) * 100\\n    \\n    print(f\\\"\\\\nüìä EVALUATION RESULTS\\\")\\n    print(\\\"=\\\"*50)\\n    print(f\\\"üéØ Top-1 Accuracy: {accuracy:.2f}%\\\")\\n    print(f\\\"üéØ Top-3 Accuracy: {top3_acc:.2f}%\\\")\\n    print(f\\\"üéØ Top-5 Accuracy: {top5_acc:.2f}%\\\")\\n    \\n    # Per-class metrics\\n    precision, recall, f1, support = precision_recall_fscore_support(\\n        labels, predictions, average=None, zero_division=0\\n    )\\n    \\n    # Create detailed classification report\\n    class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\\n    report = classification_report(\\n        labels, predictions, \\n        target_names=class_names, \\n        output_dict=True,\\n        zero_division=0\\n    )\\n    \\n    # Convert to DataFrame for better visualization\\n    report_df = pd.DataFrame(report).transpose()\\n    \\n    print(f\\\"\\\\nüìä PER-CLASS PERFORMANCE (Top 10 by F1-Score):\\\")\\n    print(\\\"-\\\"*70)\\n    \\n    # Sort by F1-score and show top 10\\n    class_metrics = report_df.iloc[:-3].sort_values('f1-score', ascending=False)\\n    top_classes = class_metrics.head(10)\\n    \\n    for idx, (species, metrics) in enumerate(top_classes.iterrows()):\\n        print(f\\\"{idx+1:2d}. {species[:25]:<25} Precision: {metrics['precision']:.3f} \\\"\\n              f\\\"Recall: {metrics['recall']:.3f} F1: {metrics['f1-score']:.3f}\\\")\\n    \\n    # Confidence analysis\\n    confidences = [detail['confidence'] for detail in prediction_details]\\n    correct_confidences = [detail['confidence'] for detail in prediction_details if detail['correct']]\\n    incorrect_confidences = [detail['confidence'] for detail in prediction_details if not detail['correct']]\\n    \\n    print(f\\\"\\\\nüìä CONFIDENCE ANALYSIS:\\\")\\n    print(\\\"-\\\"*40)\\n    print(f\\\"Average confidence (all): {np.mean(confidences):.3f}\\\")\\n    print(f\\\"Average confidence (correct): {np.mean(correct_confidences):.3f}\\\")\\n    print(f\\\"Average confidence (incorrect): {np.mean(incorrect_confidences):.3f}\\\")\\n    \\n    return {\\n        'accuracy': accuracy,\\n        'top3_accuracy': top3_acc,\\n        'top5_accuracy': top5_acc,\\n        'report_df': report_df,\\n        'class_metrics': class_metrics,\\n        'confidences': confidences,\\n        'correct_confidences': correct_confidences,\\n        'incorrect_confidences': incorrect_confidences\\n    }\\n\\ndef visualize_evaluation_results(results, predictions, labels, idx_to_class, save_dir):\\n    \\\"\\\"\\\"Create comprehensive evaluation visualizations\\\"\\\"\\\"\\n    \\n    # Create figure with multiple subplots\\n    fig = plt.figure(figsize=(20, 15))\\n    \\n    # 1. Confusion Matrix\\n    ax1 = plt.subplot(3, 3, 1)\\n    cm = confusion_matrix(labels, predictions)\\n    \\n    # Normalize confusion matrix\\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\\n    \\n    # Only show top 15 classes for readability\\n    top_classes_idx = results['class_metrics'].head(15).index\\n    top_class_indices = [i for i, name in enumerate(idx_to_class.values()) if name in top_classes_idx]\\n    \\n    if len(top_class_indices) > 1:\\n        cm_subset = cm_normalized[np.ix_(top_class_indices, top_class_indices)]\\n        class_names_subset = [list(idx_to_class.values())[i][:10] for i in top_class_indices]\\n        \\n        sns.heatmap(cm_subset, annot=True, fmt='.2f', cmap='Blues', \\n                   xticklabels=class_names_subset, yticklabels=class_names_subset,\\n                   ax=ax1)\\n        ax1.set_title('Confusion Matrix (Top 15 Classes)', fontweight='bold')\\n        ax1.set_xlabel('Predicted')\\n        ax1.set_ylabel('Actual')\\n    \\n    # 2. Accuracy by class\\n    ax2 = plt.subplot(3, 3, 2)\\n    top_f1_classes = results['class_metrics'].head(15)\\n    ax2.barh(range(len(top_f1_classes)), top_f1_classes['f1-score'])\\n    ax2.set_yticks(range(len(top_f1_classes)))\\n    ax2.set_yticklabels([name[:15] for name in top_f1_classes.index])\\n    ax2.set_xlabel('F1-Score')\\n    ax2.set_title('F1-Score by Species (Top 15)', fontweight='bold')\\n    ax2.grid(True, alpha=0.3)\\n    \\n    # 3. Confidence distribution\\n    ax3 = plt.subplot(3, 3, 3)\\n    ax3.hist(results['correct_confidences'], bins=30, alpha=0.7, label='Correct', color='green')\\n    ax3.hist(results['incorrect_confidences'], bins=30, alpha=0.7, label='Incorrect', color='red')\\n    ax3.set_xlabel('Confidence')\\n    ax3.set_ylabel('Frequency')\\n    ax3.set_title('Confidence Distribution', fontweight='bold')\\n    ax3.legend()\\n    ax3.grid(True, alpha=0.3)\\n    \\n    # 4. Top-k accuracy\\n    ax4 = plt.subplot(3, 3, 4)\\n    k_values = [1, 3, 5]\\n    k_accuracies = [results['accuracy'], results['top3_accuracy'], results['top5_accuracy']]\\n    ax4.bar(k_values, k_accuracies, color=['blue', 'orange', 'green'])\\n    ax4.set_xlabel('K (Top-K)')\\n    ax4.set_ylabel('Accuracy (%)')\\n    ax4.set_title('Top-K Accuracy', fontweight='bold')\\n    ax4.grid(True, alpha=0.3)\\n    \\n    # 5. Precision vs Recall scatter\\n    ax5 = plt.subplot(3, 3, 5)\\n    class_metrics = results['class_metrics'].iloc[:-3]  # Exclude summary rows\\n    scatter = ax5.scatter(class_metrics['recall'], class_metrics['precision'], \\n                         c=class_metrics['f1-score'], cmap='viridis', alpha=0.7)\\n    ax5.set_xlabel('Recall')\\n    ax5.set_ylabel('Precision')\\n    ax5.set_title('Precision vs Recall by Species', fontweight='bold')\\n    plt.colorbar(scatter, ax=ax5, label='F1-Score')\\n    ax5.grid(True, alpha=0.3)\\n    \\n    # 6. Support distribution\\n    ax6 = plt.subplot(3, 3, 6)\\n    support_counts = results['class_metrics']['support'].iloc[:-3]\\n    ax6.hist(support_counts, bins=20, color='purple', alpha=0.7)\\n    ax6.set_xlabel('Number of Test Samples')\\n    ax6.set_ylabel('Number of Species')\\n    ax6.set_title('Test Sample Distribution', fontweight='bold')\\n    ax6.grid(True, alpha=0.3)\\n    \\n    # 7. Performance summary text\\n    ax7 = plt.subplot(3, 3, 7)\\n    ax7.axis('off')\\n    \\n    summary_text = f\\\"\\\"\\\"Model Performance Summary\\n    \\nüéØ Overall Accuracy: {results['accuracy']:.2f}%\\nüéØ Top-3 Accuracy: {results['top3_accuracy']:.2f}%\\nüéØ Top-5 Accuracy: {results['top5_accuracy']:.2f}%\\n\\nüìä Best Performing Species:\\n{results['class_metrics'].head(3).index.tolist()[0][:20]}...\\n{results['class_metrics'].head(3).index.tolist()[1][:20]}...\\n{results['class_metrics'].head(3).index.tolist()[2][:20]}...\\n\\nüí° Average Confidence:\\n‚úÖ Correct: {np.mean(results['correct_confidences']):.3f}\\n‚ùå Incorrect: {np.mean(results['incorrect_confidences']):.3f}\\n\\nüìà Model Quality:\\n{\\\"Excellent\\\" if results['accuracy'] > 90 else \\\"Good\\\" if results['accuracy'] > 80 else \\\"Fair\\\" if results['accuracy'] > 70 else \\\"Needs Improvement\\\"}\\n    \\\"\\\"\\\"\\n    \\n    ax7.text(0.1, 0.9, summary_text, transform=ax7.transAxes, fontsize=10,\\n            verticalalignment='top', fontfamily='monospace',\\n            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\\n    \\n    plt.tight_layout()\\n    \\n    # Save visualization\\n    viz_path = os.path.join(save_dir, 'evaluation_results.png')\\n    plt.savefig(viz_path, dpi=300, bbox_inches='tight')\\n    print(f\\\"üìä Evaluation visualization saved to: {viz_path}\\\")\\n    \\n    plt.show()\\n    \\n    return fig\\n\\n# Run comprehensive evaluation\\nif 'EMA_BEST_MODEL_PATH' in globals() and os.path.exists(EMA_BEST_MODEL_PATH):\\n    print(f\\\"üß™ Loading best EMA model: {EMA_BEST_MODEL_PATH}\\\")\\n    \\n    # Load the best model\\n    best_model, idx_to_class = load_best_model(EMA_BEST_MODEL_PATH, DEVICE)\\n    \\n    if best_model is not None:\\n        # Create test dataset\\n        test_data = create_test_dataset(LABELED_DIR, \\n                                       {v: k for k, v in idx_to_class.items()},\\n                                       img_size=224)\\n        \\n        if test_data:\\n            # Run comprehensive evaluation\\n            predictions, labels, probabilities, prediction_details = evaluate_model_comprehensive(\\n                best_model, test_data, idx_to_class, DEVICE\\n            )\\n            \\n            # Analyze results\\n            results = analyze_results(predictions, labels, probabilities, \\n                                    prediction_details, idx_to_class)\\n            \\n            # Create visualizations\\n            if 'EMA_CONFIG' in globals():\\n                fig = visualize_evaluation_results(results, predictions, labels, \\n                                                  idx_to_class, EMA_CONFIG['checkpoint_dir'])\\n            \\n            # Save detailed results\\n            if 'EMA_CONFIG' in globals():\\n                results_path = os.path.join(EMA_CONFIG['checkpoint_dir'], 'evaluation_results.json')\\n                \\n                # Prepare results for JSON serialization\\n                json_results = {\\n                    'overall_accuracy': float(results['accuracy']),\\n                    'top3_accuracy': float(results['top3_accuracy']),\\n                    'top5_accuracy': float(results['top5_accuracy']),\\n                    'average_confidence_correct': float(np.mean(results['correct_confidences'])),\\n                    'average_confidence_incorrect': float(np.mean(results['incorrect_confidences'])),\\n                    'total_test_samples': len(test_data),\\n                    'num_classes': len(idx_to_class)\\n                }\\n                \\n                import json\\n                with open(results_path, 'w') as f:\\n                    json.dump(json_results, f, indent=2)\\n                \\n                print(f\\\"üìä Detailed results saved to: {results_path}\\\")\\n            \\n            print(f\\\"\\\\nüéâ EVALUATION COMPLETED SUCCESSFULLY!\\\")\\n            print(f\\\"üèÜ Final model achieved {results['accuracy']:.2f}% accuracy\\\")\\n            print(f\\\"üöÄ Model ready for deployment!\\\")\\n        \\n        else:\\n            print(\\\"‚ùå No test data available for evaluation\\\")\\n    \\n    else:\\n        print(\\\"‚ùå Could not load the best model\\\")\\n\\nelse:\\n    print(\\\"‚ö†Ô∏è  Best model checkpoint not found. Please complete training first.\\\")\\n\\nprint(f\\\"\\\\n‚úÖ Model evaluation and analysis complete!\\\")\\nprint(f\\\"üìä All evaluation results saved to checkpoint directory\\\")\\nprint(f\\\"üéØ Ready for model deployment and production use!\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Create Updated DataLoader for ViT-Base MAE Training\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"üìä CREATING ViT-BASE MAE DATALOADER\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Clear existing dataloader if it exists\n",
    "if 'mae_dataloader' in globals():\n",
    "    del mae_dataloader\n",
    "\n",
    "print(f\"üîÑ Creating dataloader with adjusted batch size: {MAE_CONFIG['batch_size']}\")\n",
    "\n",
    "# Create DataLoader with ViT-Base optimized settings\n",
    "dataloader_kwargs = {\n",
    "    'batch_size': MAE_CONFIG['batch_size'],  # Reduced to 12 for ViT-Base\n",
    "    'shuffle': True,\n",
    "    'num_workers': MAE_CONFIG['num_workers'],  # 0 for MPS compatibility\n",
    "    'pin_memory': False,  # Disable for MPS compatibility\n",
    "    'drop_last': True,    # Ensure consistent batch sizes\n",
    "    'persistent_workers': False\n",
    "}\n",
    "\n",
    "print(f\"‚öôÔ∏è DataLoader configuration:\")\n",
    "print(f\"   Batch size: {dataloader_kwargs['batch_size']}\")\n",
    "print(f\"   Workers: {dataloader_kwargs['num_workers']} ({'MPS optimized' if DEVICE.type == 'mps' else 'Standard'})\")\n",
    "print(f\"   Pin memory: {dataloader_kwargs['pin_memory']}\")\n",
    "print(f\"   Drop last: {dataloader_kwargs['drop_last']}\")\n",
    "\n",
    "try:\n",
    "    mae_dataloader_base = DataLoader(mae_dataset, **dataloader_kwargs)\n",
    "    \n",
    "    print(f\"‚úÖ ViT-Base MAE dataloader created!\")\n",
    "    print(f\"üìä Dataset statistics:\")\n",
    "    print(f\"   Total images: {len(mae_dataset):,}\")\n",
    "    print(f\"   Batches per epoch: {len(mae_dataloader_base):,}\")\n",
    "    print(f\"   Images per batch: {MAE_CONFIG['batch_size']}\")\n",
    "    print(f\"   Total images per epoch: {len(mae_dataloader_base) * MAE_CONFIG['batch_size']:,}\")\n",
    "    \n",
    "    # Estimate training time\n",
    "    estimated_batches_total = len(mae_dataloader_base) * MAE_CONFIG['epochs']\n",
    "    estimated_time_hours = estimated_batches_total * 0.8 / 3600  # ~0.8 sec per batch for ViT-Base\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Training time estimation:\")\n",
    "    print(f\"   Batches per epoch: {len(mae_dataloader_base):,}\")\n",
    "    print(f\"   Total epochs: {MAE_CONFIG['epochs']}\")\n",
    "    print(f\"   Total batches: {estimated_batches_total:,}\")\n",
    "    print(f\"   Estimated time: {estimated_time_hours:.1f} hours\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create ViT-Base MAE dataloader: {e}\")\n",
    "    print(\"üí° Try reducing batch size further if memory issues occur\")\n",
    "    raise\n",
    "\n",
    "# Test the dataloader with ViT-Base model\n",
    "print(f\"\\nüß™ Testing ViT-Base dataloader...\")\n",
    "\n",
    "try:\n",
    "    test_iter = iter(mae_dataloader_base)\n",
    "    test_batch = next(test_iter)\n",
    "    \n",
    "    print(f\"‚úÖ Dataloader test successful!\")\n",
    "    print(f\"   Batch shape: {test_batch.shape}\")\n",
    "    print(f\"   Memory per batch: ~{test_batch.numel() * 4 / (1024**2):.1f} MB\")\n",
    "    \n",
    "    # Test on device\n",
    "    test_batch = test_batch.to(DEVICE)\n",
    "    print(f\"‚úÖ Successfully moved batch to {DEVICE}\")\n",
    "    \n",
    "    # Quick forward pass test with the ViT-Base model\n",
    "    print(f\"üß™ Testing forward pass with ViT-Base model...\")\n",
    "    with torch.no_grad():\n",
    "        test_loss, _, _, _ = mae_model(test_batch, mask_ratio=0.75)\n",
    "    \n",
    "    print(f\"‚úÖ Forward pass successful! Loss: {test_loss.item():.4f}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del test_batch, test_iter, test_loss\n",
    "    if DEVICE.type == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Dataloader test failed: {e}\")\n",
    "    print(\"üí° This might indicate memory issues with ViT-Base\")\n",
    "    raise\n",
    "\n",
    "# Set global dataloader\n",
    "mae_dataloader = mae_dataloader_base\n",
    "\n",
    "print(f\"\\nüéâ ViT-BASE DATALOADER SETUP COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"‚úÖ Dataloader created and tested\")\n",
    "print(\"‚úÖ Forward pass verified\")\n",
    "print(\"‚úÖ Memory usage optimized\")\n",
    "print(\"üöÄ Ready to start ViT-Base MAE pretraining!\")\n",
    "\n",
    "# Final memory check\n",
    "if DEVICE.type == 'mps':\n",
    "    print(f\"\\nüçé MPS memory optimizations active\")\n",
    "elif DEVICE.type == 'cuda':\n",
    "    memory_allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    memory_reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "    print(f\"\\nüî• GPU Memory Status:\")\n",
    "    print(f\"   Allocated: {memory_allocated:.2f} GB\")\n",
    "    print(f\"   Reserved: {memory_reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6fbd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ START ViT-BASE MAE PRETRAINING\n",
    "print(\"üöÄ STARTING ViT-BASE MAE PRETRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Training configuration\n",
    "SAVE_EVERY = 5  # Save checkpoint every N epochs\n",
    "LOG_EVERY = 100  # Log every N batches\n",
    "\n",
    "# Create checkpoint directory for ViT-Base\n",
    "checkpoint_dir = 'mae_checkpoints_base'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    print(f\"üìÅ Created checkpoint directory: {checkpoint_dir}\")\n",
    "\n",
    "print(f\"üéØ Training Configuration:\")\n",
    "print(f\"   Model: ViT-Base (12 layers, 12 heads, 768-dim)\")\n",
    "print(f\"   Epochs: {MAE_CONFIG['epochs']}\")\n",
    "print(f\"   Batch size: {MAE_CONFIG['batch_size']}\")\n",
    "print(f\"   Learning rate: {MAE_CONFIG['base_lr']:.6f}\")\n",
    "print(f\"   Mask ratio: {MAE_CONFIG['mask_ratio']}\")\n",
    "print(f\"   Weight decay: {MAE_CONFIG['weight_decay']}\")\n",
    "print(f\"   Checkpoint every: {SAVE_EVERY} epochs\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "\n",
    "# Training metrics\n",
    "train_losses = []\n",
    "epoch_times = []\n",
    "\n",
    "# Start training\n",
    "start_time = time.time()\n",
    "print(f\"\\n‚è∞ Training started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    for epoch in range(MAE_CONFIG['epochs']):\n",
    "        epoch_start = time.time()\n",
    "        mae_model.train()\n",
    "        \n",
    "        # Training metrics for this epoch\n",
    "        epoch_losses = []\n",
    "        batch_times = []\n",
    "        \n",
    "        print(f\"\\nüîÑ EPOCH {epoch + 1}/{MAE_CONFIG['epochs']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for batch_idx, batch in enumerate(mae_dataloader):\n",
    "            batch_start = time.time()\n",
    "            \n",
    "            # Move to device\n",
    "            batch = batch.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss, pred, mask, _ = mae_model(batch, mask_ratio=MAE_CONFIG['mask_ratio'])\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Record metrics\n",
    "            current_loss = loss.item()\n",
    "            epoch_losses.append(current_loss)\n",
    "            batch_time = time.time() - batch_start\n",
    "            batch_times.append(batch_time)\n",
    "            \n",
    "            # Log progress\n",
    "            if batch_idx % LOG_EVERY == 0 or batch_idx == len(mae_dataloader) - 1:\n",
    "                avg_batch_time = sum(batch_times[-LOG_EVERY:]) / len(batch_times[-LOG_EVERY:])\n",
    "                current_lr = scheduler.get_last_lr()[0]\n",
    "                \n",
    "                print(f\"   Batch {batch_idx:4d}/{len(mae_dataloader):4d} | \"\n",
    "                      f\"Loss: {current_loss:.4f} | \"\n",
    "                      f\"LR: {current_lr:.6f} | \"\n",
    "                      f\"Time: {batch_time:.2f}s | \"\n",
    "                      f\"Avg: {avg_batch_time:.2f}s\")\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del batch, loss, pred, mask\n",
    "            if DEVICE.type == 'mps':\n",
    "                torch.mps.empty_cache()\n",
    "            elif DEVICE.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Epoch summary\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "        epoch_times.append(epoch_time)\n",
    "        \n",
    "        print(f\"\\n‚úÖ EPOCH {epoch + 1} COMPLETE\")\n",
    "        print(f\"   Average Loss: {avg_epoch_loss:.4f}\")\n",
    "        print(f\"   Epoch Time: {epoch_time:.1f}s ({epoch_time/60:.1f} min)\")\n",
    "        print(f\"   Batches: {len(mae_dataloader)}\")\n",
    "        print(f\"   Images processed: {len(mae_dataloader) * MAE_CONFIG['batch_size']:,}\")\n",
    "        \n",
    "        # Memory status\n",
    "        if DEVICE.type == 'cuda':\n",
    "            memory_allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "            memory_reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "            print(f\"   GPU Memory: {memory_allocated:.2f}GB allocated, {memory_reserved:.2f}GB reserved\")\n",
    "        \n",
    "        # Save checkpoint every 5 epochs\n",
    "        if (epoch + 1) % SAVE_EVERY == 0:\n",
    "            checkpoint_path = f\"{checkpoint_dir}/mae_base_checkpoint_epoch_{epoch + 1}.pth\"\n",
    "            \n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': mae_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': avg_epoch_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'config': MAE_CONFIG,\n",
    "                'model_params': {\n",
    "                    'embed_dim': MAE_CONFIG['embed_dim'],\n",
    "                    'num_heads': MAE_CONFIG['num_heads'],\n",
    "                    'depth': MAE_CONFIG['depth'],\n",
    "                    'decoder_embed_dim': MAE_CONFIG['decoder_embed_dim'],\n",
    "                    'decoder_depth': MAE_CONFIG['decoder_depth'],\n",
    "                    'decoder_num_heads': MAE_CONFIG['decoder_num_heads']\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"üíæ Checkpoint saved: {checkpoint_path}\")\n",
    "        \n",
    "        # Progress update\n",
    "        elapsed_time = time.time() - start_time\n",
    "        avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "        remaining_epochs = MAE_CONFIG['epochs'] - (epoch + 1)\n",
    "        estimated_remaining = remaining_epochs * avg_epoch_time\n",
    "        \n",
    "        print(f\"üìä Progress: {((epoch + 1) / MAE_CONFIG['epochs']) * 100:.1f}% complete\")\n",
    "        print(f\"‚è±Ô∏è  Elapsed: {elapsed_time/3600:.1f}h | \"\n",
    "              f\"Remaining: {estimated_remaining/3600:.1f}h | \"\n",
    "              f\"Total Est: {(elapsed_time + estimated_remaining)/3600:.1f}h\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\n‚ö†Ô∏è  Training interrupted by user at epoch {epoch + 1}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training failed at epoch {epoch + 1}: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    # Save final checkpoint regardless of how training ended\n",
    "    final_checkpoint_path = f\"{checkpoint_dir}/mae_base_checkpoint_final.pth\"\n",
    "    \n",
    "    try:\n",
    "        final_checkpoint = {\n",
    "            'epoch': epoch + 1 if 'epoch' in locals() else 0,\n",
    "            'model_state_dict': mae_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': train_losses[-1] if train_losses else float('inf'),\n",
    "            'train_losses': train_losses,\n",
    "            'config': MAE_CONFIG,\n",
    "            'model_params': {\n",
    "                'embed_dim': MAE_CONFIG['embed_dim'],\n",
    "                'num_heads': MAE_CONFIG['num_heads'],\n",
    "                'depth': MAE_CONFIG['depth'],\n",
    "                'decoder_embed_dim': MAE_CONFIG['decoder_embed_dim'],\n",
    "                'decoder_depth': MAE_CONFIG['decoder_depth'],\n",
    "                'decoder_num_heads': MAE_CONFIG['decoder_num_heads']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        torch.save(final_checkpoint, final_checkpoint_path)\n",
    "        print(f\"üíæ Final checkpoint saved: {final_checkpoint_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save final checkpoint: {e}\")\n",
    "\n",
    "# Training summary\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nüéâ ViT-BASE MAE TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Training Summary:\")\n",
    "print(f\"   Total epochs: {len(train_losses)}\")\n",
    "print(f\"   Final loss: {train_losses[-1]:.4f}\" if train_losses else \"   No training completed\")\n",
    "print(f\"   Total time: {total_time/3600:.1f} hours\")\n",
    "print(f\"   Avg time per epoch: {(total_time/len(epoch_times)):.1f}s\" if epoch_times else \"N/A\")\n",
    "print(f\"   Checkpoints saved in: {checkpoint_dir}\")\n",
    "print(f\"   Model architecture: ViT-Base (768-dim embeddings)\")\n",
    "print(f\"   Total parameters: ~86M\")\n",
    "\n",
    "# Plot training curve if we have losses\n",
    "if train_losses:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, 'b-', linewidth=2, label='Training Loss')\n",
    "    plt.title('ViT-Base MAE Training Loss', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìà Training curve plotted!\")\n",
    "\n",
    "print(f\"\\nüöÄ ViT-Base MAE pretraining is complete!\")\n",
    "print(f\"üí° Use the saved checkpoints for downstream classification training\")\n",
    "print(f\"üìÅ Checkpoints are in: {checkpoint_dir}/\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
