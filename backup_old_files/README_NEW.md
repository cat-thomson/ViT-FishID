# ğŸŸ ViT-FishID: Semi-Supervised Fish Classification

A streamlined implementation of Vision Transformer (ViT) with EMA teacher-student framework for semi-supervised fish species classification.

## ğŸ¯ Project Overview

This project achieved **78.32% validation accuracy** on fish species classification using semi-supervised learning with 19,219 fish images (6,546 labeled + 12,673 unlabeled) across 36 species.

### Key Features
- **Semi-Supervised Learning**: EMA teacher-student framework with consistency regularization
- **Vision Transformers**: ViT-Base model with 85.8M parameters
- **Automated Data Pipeline**: Extract fish cutouts from bounding box annotations
- **Google Colab Support**: Complete notebook for cloud training
- **Production Ready**: Achieved research-quality results

## ğŸ“ Simplified Project Structure

```
ViT-FishID/
â”œâ”€â”€ README.md                     # This comprehensive guide
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ train.py                      # Main training script (both supervised & semi-supervised)
â”œâ”€â”€ model.py                      # ViT model + EMA teacher components
â”œâ”€â”€ trainer.py                    # Unified trainer for both modes
â”œâ”€â”€ data.py                       # Data loading and transformations
â”œâ”€â”€ pipeline.py                   # Data extraction and organization
â”œâ”€â”€ utils.py                      # Utility functions
â”œâ”€â”€ species_mapping.txt           # Species ID mapping (122 species)
â”œâ”€â”€ Colab_Training.ipynb          # Google Colab notebook
â””â”€â”€ data/                         # Dataset directory
    â”œâ”€â”€ labeled/                  # Labeled fish by species
    â”‚   â”œâ”€â”€ Sparidae_Chrysoblephus_laticeps/
    â”‚   â”œâ”€â”€ Serranidae_Epinephelus_marginatus/
    â”‚   â””â”€â”€ ...
    â””â”€â”€ unlabeled/                # Unlabeled fish images
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
git clone https://github.com/cat-thomson/ViT-FishID.git
cd ViT-FishID
pip install -r requirements.txt
```

### 2. Prepare Your Data

#### Option A: Process Raw Video Frames (Recommended)

If you have video frames with YOLO annotations:

```bash
# Process multiple video directories
python pipeline.py \
    --multi_video_dir /path/to/video/directories \
    --species_mapping_file species_mapping.txt \
    --output_dir ./data/organized_dataset

# Or process a single directory
python pipeline.py \
    --frames_dir /path/to/frames \
    --annotations_dir /path/to/annotations \
    --output_dir ./data/organized_dataset
```

#### Option B: Use Pre-organized Data

If you already have organized fish images, create this structure:

```
data/
â”œâ”€â”€ labeled/
â”‚   â”œâ”€â”€ species_1/
â”‚   â”‚   â”œâ”€â”€ fish_001.jpg
â”‚   â”‚   â””â”€â”€ fish_002.jpg
â”‚   â””â”€â”€ species_2/
â”‚       â””â”€â”€ ...
â””â”€â”€ unlabeled/
    â”œâ”€â”€ fish_003.jpg
    â””â”€â”€ fish_004.jpg
```

### 3. Train Your Model

#### Semi-Supervised Training (Recommended)

```bash
python train.py \
    --data_dir ./data/organized_dataset \
    --mode semi_supervised \
    --epochs 100 \
    --batch_size 32 \
    --consistency_weight 2.0 \
    --pseudo_label_threshold 0.7 \
    --use_wandb
```

#### Supervised Training (Labeled data only)

```bash
python train.py \
    --data_dir ./data/organized_dataset \
    --mode supervised \
    --epochs 100 \
    --batch_size 32 \
    --use_wandb
```

## ğŸŒ Google Colab Training

For users without local GPU access:

1. **Open Colab**: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cat-thomson/ViT-FishID/blob/main/Colab_Training.ipynb)

2. **Upload Data**: Put your fish images in Google Drive

3. **Update Paths**: Modify the data path in the notebook

4. **Run Training**: Execute all cells to train your model

**Expected Colab Performance:**
- Training Time: 2-3 hours for 50 epochs on Tesla T4
- Memory Usage: ~6-8GB GPU memory
- Accuracy: Similar to local training (~75-85%)

## ğŸ§  Model Architecture

### Vision Transformer (ViT-Base)
- **Parameters**: 85.8M parameters
- **Input Size**: 224Ã—224 RGB images
- **Patch Size**: 16Ã—16 patches
- **Implementation**: timm library for optimal performance

### EMA Teacher-Student Framework
- **Teacher Model**: Exponential moving average of student weights (momentum: 0.999)
- **Student Model**: Standard ViT trained with supervised + consistency loss
- **Consistency Loss**: MSE between teacher and student predictions with temperature scaling
- **Semi-Supervised Learning**: Leverages unlabeled data through consistency regularization

## ğŸ“Š Training Configuration

### Recommended Parameters

```python
# Semi-supervised training
python train.py \
    --mode semi_supervised \
    --epochs 100 \
    --batch_size 32 \
    --learning_rate 1e-4 \
    --consistency_weight 2.0 \
    --pseudo_label_threshold 0.7 \
    --ema_momentum 0.999 \
    --temperature 4.0 \
    --warmup_epochs 10 \
    --ramp_up_epochs 20
```

### Key Hyperparameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| `consistency_weight` | 2.0 | Weight for consistency loss |
| `pseudo_label_threshold` | 0.7 | Confidence threshold for pseudo-labels |
| `ema_momentum` | 0.999 | EMA momentum for teacher updates |
| `temperature` | 4.0 | Temperature scaling for consistency loss |
| `unlabeled_ratio` | 2.0 | Ratio of unlabeled to labeled samples |

## ğŸ“ˆ Performance Results

### Achieved Performance (100 epochs)
- **Teacher Model**: 78.32% validation accuracy, 95.11% top-5 accuracy
- **Student Model**: 76.95% validation accuracy
- **Dataset**: 19,219 total images (6,546 labeled + 12,673 unlabeled)
- **Species**: 36 fish species across 4 families

### Training Metrics
```
Epoch 100/100:
ğŸ“š Labeled Accuracy: 89.2%
ğŸ”„ Pseudo-label Accuracy: 84.7%
ğŸ“Š High-confidence Pseudo-labels: 21.2%
ğŸ¯ Teacher Validation: 78.32%
â­ Top-5 Accuracy: 95.11%
```

## ğŸ”§ Command Line Options

### Data Pipeline (`pipeline.py`)

```bash
# Required
--multi_video_dir PATH        # Directory with video subdirectories
--output_dir PATH             # Output directory for organized dataset

# Optional
--buffer_ratio 0.1            # Padding around bounding boxes
--min_cutout_size 50          # Minimum cutout size in pixels
--confidence_threshold 0.0    # Minimum detection confidence
--labeled_families LIST       # Families for labeled dataset
--max_per_species N           # Limit cutouts per species
--preview_only                # Show stats without processing
```

### Training (`train.py`)

```bash
# Required
--data_dir PATH               # Dataset directory
--mode {supervised,semi_supervised}

# Model & Training
--model_name MODEL            # ViT architecture
--epochs 100                  # Number of epochs
--batch_size 32               # Batch size
--learning_rate 1e-4          # Learning rate

# Semi-supervised
--consistency_weight 2.0      # Consistency loss weight
--pseudo_label_threshold 0.7  # Pseudo-label confidence threshold
--unlabeled_ratio 2.0         # Unlabeled to labeled ratio

# Logging
--use_wandb                   # Enable W&B logging
--wandb_project PROJECT       # W&B project name
```

## ğŸ—ï¸ Data Pipeline Details

### Supported Input Formats

1. **YOLO Annotations**: `.txt` files with format `class_id x_center y_center width height confidence`
2. **Multiple Video Directories**: Each subdirectory contains frames and annotations
3. **Species Mapping**: `species_mapping.txt` with format `"Family Species": ID`

### Processing Steps

1. **Extract Cutouts**: Extract fish from bounding box annotations
2. **Filter by Size**: Remove cutouts smaller than minimum size
3. **Apply Confidence Threshold**: Filter low-confidence detections
4. **Organize by Species**: Group cutouts by species using mapping file
5. **Create Labeled/Unlabeled Split**: Separate data based on target families

### Output Structure

```
organized_dataset/
â”œâ”€â”€ labeled/
â”‚   â”œâ”€â”€ Sparidae_Chrysoblephus_laticeps/    # Target family species
â”‚   â”œâ”€â”€ Serranidae_Epinephelus_marginatus/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ unlabeled/                               # All other species
â”‚   â”œâ”€â”€ fish_001.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ extraction_summary.json                 # Processing statistics
```

## ğŸ’¾ Checkpoints and Resuming

### Automatic Checkpointing
- Models saved every 10 epochs to `./checkpoints/`
- Best model saved as `model_best.pth`
- Includes optimizer and scheduler states

### Resume Training
```bash
python train.py \
    --data_dir ./data \
    --mode semi_supervised \
    --resume_from ./checkpoints/model_best.pth
```

## ğŸ“Š Monitoring Training

### Weights & Biases Integration

```bash
# Setup W&B (first time)
wandb login

# Train with logging
python train.py \
    --data_dir ./data \
    --mode semi_supervised \
    --use_wandb \
    --wandb_project "fish-classification"
```

### Key Metrics to Monitor

**Semi-supervised Training:**
- `Cons Loss`: Should be > 0.001 (indicates active semi-supervised learning)
- `High-conf Pseudo`: Percentage of unlabeled data with high-confidence predictions
- `Pseudo Accuracy`: Quality of teacher predictions on unlabeled data
- `Teacher vs Student`: Teacher should perform slightly better

**Warning Signs:**
- `Cons Loss = 0.0000`: Semi-supervised learning inactive (check threshold)
- Pseudo accuracy much lower than labeled accuracy
- Large gap between teacher and student performance

## ğŸ” Troubleshooting

### Common Issues

**1. Consistency Loss is 0.0000**
```bash
# Solution: Lower pseudo-label threshold
python train.py --pseudo_label_threshold 0.5  # Instead of 0.95
```

**2. GPU Memory Error**
```bash
# Solution: Reduce batch size
python train.py --batch_size 16  # Instead of 32
```

**3. Poor Semi-supervised Performance**
```bash
# Solution: Adjust consistency weight and ramp-up
python train.py --consistency_weight 1.0 --ramp_up_epochs 30
```

**4. Data Loading Errors**
```bash
# Check data structure
python train.py --data_dir ./data --preview_only
```

### Performance Tips

1. **Start with Supervised Training**: Establish baseline performance
2. **Monitor Consistency Loss**: Should be active (> 0.001) throughout training
3. **Adjust Pseudo-label Threshold**: Lower threshold = more unlabeled data usage
4. **Use Warmup**: Allow model to stabilize before applying consistency loss

## ğŸ›ï¸ Advanced Configuration

### Custom Species Mapping

Create or modify `species_mapping.txt`:

```text
"Sparidae Chrysoblephus laticeps": 1,
"Serranidae Epinephelus marginatus": 2,
"Carangidae Trachurus capensis": 3,
```

### Strong Data Augmentation

The training pipeline includes strong augmentation for semi-supervised learning:
- Random crops and flips
- Color jittering
- Gaussian blur and noise
- CLAHE enhancement

### Learning Rate Scheduling

- **Warmup**: Linear warmup for first 10 epochs
- **Cosine Annealing**: Cosine decay over total epochs
- **EMA Updates**: Consistent teacher updates throughout training

## ğŸš€ Production Deployment

### Model Inference

```python
import torch
from model import ViTForFishClassification

# Load trained model
model = ViTForFishClassification(num_classes=36)
checkpoint = torch.load('checkpoints/model_best.pth')
model.load_state_dict(checkpoint['teacher_state_dict'])  # Use teacher weights
model.eval()

# Inference
with torch.no_grad():
    outputs = model(input_tensor)
    predictions = torch.softmax(outputs, dim=1)
```

### Model Export

```python
# Export to ONNX for deployment
torch.onnx.export(
    model, 
    dummy_input, 
    "fish_classifier.onnx",
    export_params=True,
    opset_version=11
)
```

## ğŸ“š References

- [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)
- [Mean Teacher for Semi-Supervised Learning](https://arxiv.org/abs/1703.01780)
- [FixMatch: Simplifying Semi-Supervised Learning](https://arxiv.org/abs/2001.07685)
- [Exponential Moving Average (EMA) for Deep Learning](https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage)

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:
- Additional augmentation strategies
- Multi-scale training approaches
- Advanced pseudo-labeling techniques
- Performance optimizations

## ğŸ“„ License

MIT License - feel free to use for research and commercial applications.

---

## ğŸ‰ Results Summary

**This implementation achieved:**
- âœ… **78.32% accuracy** on 36 fish species
- âœ… **19,219 total images** processed successfully
- âœ… **Semi-supervised learning** working correctly
- âœ… **Production-ready** pipeline and training code
- âœ… **Google Colab** support for cloud training

**Ready to classify fish with state-of-the-art accuracy! ğŸŸ**
